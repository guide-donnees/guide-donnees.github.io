# Analyser


Derrière le terme "analyser" s'entend l'extraction de l'information des données le plus souvent par l'utilisation de puissance de calcul.
Cela recouvre de nombreux types de techniques (calcul intensif, traitement statistique, machine learning, visualisation ...), et nécessite également des plateformes adaptées.

Cette étape du cycle de vie de nombreuses données impose que ces données soient exploitables c'est-à-dire bien organisées, dans des formats adaptés à l'analyse envisagée, 
de façon à pouvoir leur appliquer des traitements automatisés.

Plusieurs évènements récurrents, annuels ou bisannuels, auxquels participent intensément les réseaux métiers, comme les 
[JCAD (Journées Calcul et Données)](http://www.france-grilles.fr/category/journees-jcad/), les [JDev (Journées du DEVeloppement logiciel)](http://devlog.cnrs.fr/manifestations/start) par exemple, 
intègrent de nombreuses interventions sur ces différentes thématiques,
allant de la description des plateformes aux outils disponibles, en
passant par l'organisation des développements et la
reproductibilité. Ils incluent aussi très souvent des retours
d'expérience particulièrement riches.

[CH- *A moins de trouver 1 angle d'approche spécifique pour traiter et analyser, peut-être envisager une fusion "traiter & analyer" ?*]

## Plateformes de traitement de données

De nombreuses ressources sont disponibles, à différentes échelles, pour analyser et traiter des données.
De façon générale, on distingue :

- les ressources de type calcul intensif ou HPC (High Performance Computing) organisée à l'échelle européenne (EuroHPC ou Tier 0), nationale (GENCI et les centres nationaux ou Tier 1) et régionale (mésocentres ou Tier 2)
- les ressources de type cloud (par exemple le cloud distribué de France Grille France Cloud)
- les ressources de type grille de calcul ou HTC (High Throughput Computing), par exemple l'infrastructure France Grille ou le Centre de Calcul de l'IN2P3.

Elles sont décrites dans une [page spécifique](98.Infrastructures.md).

## Outils pour l'analyse des données

Il existe de très nombreux outils permettant d'analyser ses donnée, allant du langage de programmation au workflow de traitement en passant par les logiciels de visualisation ...

### Langages de programmation

Certains langages de programmation sont plus particulièrement utilisé pour l'analyse de données. En dehors du langage R spécifique aux statistiques et à la science des données, l'écosystème s'enrichit très rapidement :

- Python devient de plus en plus utilisé en science des données. Une [introduction sur le sujet](https://github.com/fran6w/IRMAR/blob/master/IRMAR.pdf) a été réalisée en décembre 2017 par Francis Wolinski (Société Yotta Conseil) dans le cadre d'une journée organisée par le réseau Calcul.
- Julia est un des langages qui prend de l'importance sur ce sujet. Plusieurs présentations qui lui sont consacrées apporte un éclairage intéressant :
	- Une journée d'introduction au langage et à son utilisation en traitement de données aborde en particulier [le cadre des algorithmes Map/Reduce](https://calcul.math.cnrs.fr/attachments/spip/Documents/Journees/jan2019/julia_map_reduce.pdf) , ainsi que [les performances du langage sous forme de benchmarks](https://plmlab.math.cnrs.fr/fabreges/julia-2019/-/blob/master/bench.pdf) .

Des retours d'expérience illustrent l'utilisation de ces outils :

* Concernant les outils python, l'[utilisation de Dask à la place de job array](https://jcad2019.sciencesconf.org/data/20191009_JCAD2019TutoDaskJobArray.pdf) a été présentée lors des JCAD 2019 par Guillaume Eynard-Bontemps, CNES.
* Cette présentation a été complétée par celle de l'[outil Pangeo](https://jcad2019.sciencesconf.org/data/Briol_pangeo_swot_jcad2019_2019_10_10.pdf), basé sur Python et les notebooks et exploité par la communauté big data géosciences.

### Approches méthodologiques
 
L'analyse des données ne concerne pas uniquement les modèles statistiques. 
De nombreux domaines appliqués reposent sur l’analyse de données géométriques: médecine, neurosciences, sismique, météorologie, vision par ordinateur, apprentissage statistique. Cette variété d'applications se retrouve dans la forme, la qualité et la sémantique des données ainsi que dans la nature des problèmes mathématiques qu’elles posent. 
Une [école thématique a été consacrée à ce sujet en 2018](https://geomdata.sciencesconf.org/) , à destination des non spécialistes.
Elle l'a abordé sous plusieurs angles :

- Analyse topologique de données,
- Anatomie computationnelle,
- Méthode d’évolution de front et fast marching,
- Méthodes variationnelles pour l’imagerie
   
 Outre les présentations, de nombreux TP ont été proposés avec la mise en œuvre pratique des algorithmes, dans le langage Python. 
  
 Un des enjeux de l'analyse de de volumes de données de grandes tailles, multidimensionnelles concerne les méthodes de réduction de la dimension (classiques comme ACP, AFC, MDS, …) ou issues du « machine learning » (kernel PCA, …).
 Cette approche a été abordée lors d'une [école thématique qui a eu lieu en 2017](https://calcul.math.cnrs.fr/2017-09-anf-reduction-dimension.html).
 Cette formation, nécessitant des connaissances de base en calcul matriciel, a permis d'approfondir certaines des techniques matricielles (recherche de valeurs propres, décomposition en valeurs singulières), sur le plan à la fois théorique et pratique.
 
 On peut trouver un exemple d'[utilisation concrète de ce type de technique](https://jcad2018.sciencesconf.org/data/afranc_jcad_2018_vendredi.pdf) présenté lors des JCAD 2018 par Alain Franc, INRA, appliqué à la biologie.
 
 De façon un peu générale, toutes ces approches conduisent ou sont la base de certains pans de l'Intelligence Artificielle. De plus en plus d'évènements sont consacrés à ces technologies.

Une [introduction](https://indico.mathrice.fr/event/130/contribution/3/material/slides/0.pdf) sur cette thématique a été réalisée en 2018 dans le cadre des Journées Système du réseau ResInfo.
De même, le réseau SARI grenoblois a organisé une journée sur le sujet, avec une [présentation de Jean-Luc Parouty](https://sari.grenoble-inp.fr/lib/exe/fetch.php?media=theme:ia_deep_learning:ia_deep_learning:ia_machinelearning_pres20190606.pdf) particulièrement didactique.

Compte tenu de l'engouement engendré autour de l'IA, de nombreuses journées et conférences sont organisées sur le sujet. En particulier, il fait l'objet de sessions spéciales lors des Journées Développement (Jdev) de [2020](http://devlog.cnrs.fr/jdev2020/t8) et [2017](http://devlog.cnrs.fr/jdev2017/t7).

### Visualisation des données numériques
  
Un des outils d'analyse les plus fréquents correspond à la visualisation des données. Cette visualisation peut s'avérer particulièrement délicate dans le cadre de très gros volumes de données, et nécessite de s'appuyer sur des solutions techniques spécifiques.

 Dans le domaine des données numériques, plusieurs bibliothèques sont particulièrement adaptées aux données de grande taille, ainsi qu'à la visualisation in-situ, c'est-à-dire en cours de calcul en ce qui concerne les données de simulation : VisIt et ParaView. Plusieurs interventions sur ce sujet ont été réalisés dans le cadre d'une [journée dédiée organisée en 2017 par le réseau Calcul](https://calcul.math.cnrs.fr/2017-12-atelier-visualisation.html) .
 
 De même, une [action de formation](  https://calcul.math.cnrs.fr/2016-11-anf-visu-data.html) a été complètement consacrée à ce sujet en 2016 par le réseau Calcul. Elle a en particulier abordé les bonnes pratiques concernant la production de données : formats d'archivage, technique d'analyse, cycle de vie ... ainsi que les outils de visualisation avancés (visualisation in-situ, temps réel, web).

***

La participation à l’analyse des données se réalise au travers d’un accompagnement sur la mise en place de procédures qualité, par l’optimisation des calculs ou des temps d’accès, 
ainsi que dans le choix et la mise en œuvre des techniques et outils d’analyse et éventuellement dans leur conception,en fonction des besoins 
(formats, interopérabilité, performance, ergonomie, visualisation, travail collaboratif...)

   * Traitement de grosses masses de données (BigData), regroupement de données, filtrage et extraction des données pertinentes (ANF 2016)
   * <https://indico.mathrice.fr/event/5/contribution/19/material/slides/0.pdf>
Nouveaux outils d'analyse des masses de données (IA) (JoSy 2018)

   * <https://indico.mathrice.fr/event/130/contribution/3/material/slides/0.pdf>
  
## Mettre en place un contrôle Qualité des données

Par nature, la recherche n’est pas répétitive mais riche en incertitudes contrairement à un processus industriel. 
La confiance dans la qualité d’une recherche consiste donc à établir et vérifier que les différentes étapes d’une étude peuvent être répétées en obtenant le même résultat par des chercheurs différents à des moments différents.
Ainsi, une donnée est fiable, si dans des conditions données, aucune déviation n’est constatée en fonction du temps, durant un laps de temps donné. 
Il est donc essentiel de s’assurer que l’ensemble des activités de recherche soit maitrisée, cela est le cas de toute la chaine fonctionnelle d’une analyse (par exemple des pipettes, des balances jusqu’aux équipements d’analyse). 

De ce fait, il convient d'avoir une totale maitrise des équipements :
* <https://qualsimp.sciencesconf.org/data/program/9_Trac_abilite_des_donne_es_de_la_recherche_Virginie_JAN_LOGASSI.pdf>

  

   * Plusieurs présentations et ateliers sur ce thème ont eu lieu lors de l'ANF « Sciences des données : un nouveau challenge pour les métiers liés aux bases de données »(<http://rbdd.cnrs.fr/spip.php?article288)> - du 5 au 7 novembre 2018 à Sète. En particulier l'Atelier qualité des données dont les travaux portaient sur les questions suivantes : 
       * Quelles sont les différentes notions de **qualité** des données ?
       * Comment contrôler la **qualité** des données dans la BDD : avant ou pendant l’insertion de données
       * Faut-il automatiser le contrôle de la qualité dans les bases ?
       * Quels sont les outils disponibles et comment les utiliser ?    L'Introduction méthodologique et terminologique (<http://rbdd.cnrs.fr/IMG/pdf/qualite\_des\_donnees\_plumejeaud\_2018\_04112018.pdf?517/365a13edab604bd0700b045bfac29a3607acb649)> a été suivie d'un cours et de TP portant sur "OpenRefine pour traiter son fichier d’entrée" (<http://rbdd.cnrs.fr/IMG/pdf/openrefinecours.pdf?518/a69ce451abd02003a0e96957e39828e0f2e9f2ee> , <http://rbdd.cnrs.fr/IMG/pdf/openrefinedoc.pdf?519/a6de5321fdbedeec29da6cc8b82250d02937ddeb> , <http://rbdd.cnrs.fr/IMG/zip/exos.zip?520/e51f82826431b71f767e4347fd57716fa9175664> )
    On pourra aussi de référer à quelques présentations de l''ANF « Interfacer les outils mobiles avec son système d’information » citée au chapitre 3 (Collecter) :

       *  Outils nomades : validation des données (C. Plumejeaud) (<http://rbdd.cnrs.fr/IMG/pdf/anf\_rbbd\_2019\_outils\_mobiles\_tp\_qualite.pdf?573/e1425561fd10c6bd1dd92fdee22871bc427f9873)>
       *  Retour terrain : la délicate question de l’intégration des données (PY. Arnould) (<http://rbdd.cnrs.fr/IMG/pdf/anf2019\_seshat.pdf?576/575888582b8771a01200c5a6a5e751f0964e0c33)>

   * BLONDEL, Emmanuel, 2018. Ecrire et Lire des métadonnées avec la librairie R geometa. In : *Atelier « Métadonnées et R »*. Montpellier. 2018. <http://rbdd.cnrs.fr/IMG/pdf/workshop\_r\_metadata\_agropolis\_-\_geometa.pdf?504/f5cc31589976b1cd1fc18d406a547ee18122c0e7>

   * [resinfo] Traitement de grosses masses de données (BigData), regroupement de données, filtrage et extraction des données pertinentes (ANF 2016)
   * <https://indico.mathrice.fr/event/5/contribution/19/material/slides/0.pdf>

En sciences environnementales on retrouve de nombreuses méthodes pour essayer de qualifier les données

 * [sist] Suivi de la qualité des mesures de réseaux d'observations océanographiques
       * <https://sist16.sciencesconf.org/data/pages/09\_P\_Techine.pdf>
           - Philippe Téchiné, Laboratoire d'études en Géophysique et océanographie spatiales
 
 * Site Web de diffusion des données "Sahelian Dust Transect"                                         
       * <https://sist16.sciencesconf.org/data/pages/10\_A\_Campos.pdf>
           - André CAMPOS, Laboratoire inter-universitaire des systèmes atmosphèriques      
           

   * ATCQc : Un outil pour le QA/QC de mesures atmosphérique du TGIR ICOS, vidéo
     * <https://sist18.sciencesconf.org/data/pages/29_L_Hazan_ATCQc.pdf>
     * Lynn Hazan, Laboratoire des Sciences du Climat et de lÉnvironnement
      
   * La qualité des données à l'OSU OREME
      * <https://sist18.sciencesconf.org/data/pages/31_F_Fabre_O_Lobry_Qualite_des_donnees_de_l_OSU_OREME_.pdf>
      * Juliette Fabre, Observatoire de REcherche Méditerranéen de l'Environnement - Olivier Lobry, Observatoire de REcherche Méditerranéen de l'Environnement


## Mettre en place des méthodes d'analyse et des chaînes logicielles
La majorité des instruments complexes incluent un ou plusieurs logiciels dont la définition et la mise en place s’inscrivent dans un projet à part entière qui doit parfaitement s’interfacer avec le reste de l’instrument.
Dans ce contexte, il convient d'assurer le suivi des exigences liées au logiciel, la gestion des interfaces avec le reste de l’instrument et l’activité Assurance Qualité Logiciel. 
L’ Assurance Qualité Logiciel permet notamment de répondre à des exigences applicables à un logiciel, du développement à la maintenance de celui-ci.
L’ensemble des activités, normes, contrôles et procédures mis en place doit couvrir la totalité de la durée de vie d’un logiciel. Il est par exemple important de vérifier et valider au travers de tests 
la bonne santé du code et de constamment veiller à la traçabilité qui lui est liée.

* Qu'est-ce qu'un logiciel et qu'est-ce que la qualité ? : 
* <https://jt-aql.sciencesconf.org/data/program/jt_aql_Paris_2019_v5_HV.pdf>
* Plans de Gestion de Logiciel et Assurance Qualité Logiciel, les apports de PRESOFT : 
* <https://jt-aql.sciencesconf.org/data/program/PRESOFT_qualite_en_recherche2019v1_GR.pdf> 
* Référentiels et normes de codage : 
* <https://jt-aql.sciencesconf.org/data/program/ref_normes_codage_ZT.pdf>
* Qualité Logiciel dans un projet de Nanosatellite :
* <https://jt-aql.sciencesconf.org/data/program/qualite_nanosat_v2_CG.pdf>

Certains logiciels comme ODV (Ocean Data view)  pemettent de qualifier les données et d'attribuer un code qualité a des données apres analyse par un expert du domaine.
ODV est un format de fichiers, et un logiciel utilisés par le projet Européen SeadataNet https://www.seadatanet.org/Software/ODV


   * Un partage d'expérience sur la réalisation d’une application d’exploration de données [*donner des précisions]*
       *    [sist] Filtrage interactif de données multi-dimensionnelles
       *    <https://sist16.sciencesconf.org/data/pages/08\_P\_Brockmann.pdf>
           *  Patrick Brockmann, Laboratoire des Sciences du Climat et de l'Environnement [Gif-sur-Yvette]                                                                                       


   *  Gestion des données de cytométrie en flux   
       * <https://sist16.sciencesconf.org/data/pages/06\_S\_Lahbib.pdf>
           - SOUMAYA LAHBIB, MIO - Mathilde DUGENNE, - Melilotus Thyssen, Institut méditerranéen d'océanologie,  - Maurice Libes OSU Pytheas 
           
   * Gestion des données du projet EMSO avec Talend et Erddap
           - <https://sist18.sciencesconf.org/data/pages/05_M_Libes_Getsion_des_donnees_EMSO.pdf>
           -  SOUMAYA LAHBIB, MIO - Maurice Libes OSU Pytheas 

   *   L'usage d'outils, de données et de webservices interopérables pour la cartographie des formations à silex en France : l'expérience du PCR « Réseau de lithothèques "
       * <https://sist16.sciencesconf.org/data/pages/05\_C\_Tuffery.pdf>
           - Christophe Tufféry, Cités, Territoires, Environnement et Sociétés, Institut National de Recherches Archéologiques Préventives                                                                                      

   * Réseau d'observation du Pacifique Sud ‘ReefTEMPS' : évolutions fonctionnelles et optimisation d'un système d'information dédié capteurs et reconstitution de séries temporelles                   
       * <https://sist16.sciencesconf.org/data/pages/12\_R\_Hocde.pdf>                     
           - Régis Hocdé, Institut de Recherche pour le Développement

   * Chaînes de traitement en temps quasi réel des mesures de gaz à effet de serre du TGIR ICOS
        * <https://sist18.sciencesconf.org/data/pages/07_L_Hazan_Chaines_de_traitement_ICOS.pdf>
        * Lynn Hazan, Laboratoire des Sciences du Climat et de lÉnvironnement


### Optimisation des calculs ou des temps d’accès

   * « Le défi MASTODONS : un instrument pour la gestion et l’exploitation de grandes masses de données » : Mokrane Bouzeghoub (*DAS INS2I, Mission pour l’interdisciplinarité, CNRS, projet Mastodons*) - ANF Frédocs2013 - Gestion et valorisation des données de la recherche -  7 au 10 octobre 2013, Aussois
       * <http://renatis.cnrs.fr/IMG/pdf/Mastodons-Fredoc-Aussois.pdf>
       
GR : il me semble qu'il y a des choses dans les JCAD

### Traitements sémantiques/ linguistiques

[CH - Ajout]

L’accroissement massif de la production scientifique (données et publications) et la multitude de canaux de diffusion existants appelle au cœur des activités de recherche, la mise en place de solutions informatiques permettant le repérage, l’extraction, l’exploration et l’analyse de corpus de données. Les méthodes et outil TDM (Text and Data Mining) apportent une aide importante pour explorer et analyser le sens des textes et en donner une représentation compréhensible par les humains et les machines.

En 2013, déjà le réseau Renatis avait accueilli Claire Nedellec et Agnès Girad (INRA)  pour illustrer l’usage possible des technologies sémantiques pour la gestion de l’information scientifique et technique ainsi que Fabien Amarger (IRIT IRSTEA) pour témoigner de la constuction d’une base de connaissance partant d’un cas d’usage : l’annotation des Bulletins de Santé du Végétal 

Dans leur présentation, Claire Nedellec et Agnès Girard expliquent les **principes de l’analyse sémantique de texte** à travers un exemple en recherche documentaire et présentent le projet TirPhase.  La notion d’**indexation sémantique**  à travers un exemple en physiologie animale est abordée en début de présentation sous les traits d’une carte d’identité thématique associée au document.  Les auteurs présentent également la notion de **termino-ontologie** et définissent l’ontologie comme « un graphe où les nœuds sont des concepts et les arcs des relations entre ces concepts ». Elles expliquent que l’analyse sémantique identifie les unités sémantiques du texte et les associe aux concepts de l’ontologie. Partant de là, elle présente le **processus de conception de termino-ontologie** à partir de corpus en deux étapes : extraction automatique de termes avec l’outil Syntex et structuration et modélisation avec l’outil Protégé. La deuxième partie de la présentation est consacrée à la présentation du projet TriPhase. 
Ce projet a pour objectif d’**analyser les publications** d’un département de recherche à des fins stratégiques (analyse quantitative des termes au cours du temps) et disposer d’un moteur de recherche sémantique bibliographique spécialisé. Les auteurs expliquent les différentes phases de la construction de la termino-ontologie TriPhase et l’apport des documentalistes dans ce travail collaboratif.

Fabien Amarger, présente quant à lui un projet qui consiste à **construire une base de connaissanceà partir de source et de données hétérogènes**. Il explique ce qu’est une base de connaissance et comment l’interroger

* [Des technologies sémantiques pour l’information scientifique et technique](http://renatis.cnrs.fr/IMG/pdf/Nedellec_Girard_Fredoc-v4.pdf)  
Claire Nedellec, Agnès Girard, INRA  
Frédocs2013 - Gestion et valorisation des données de la recherche - 7 au 10 octobre 2013, Aussois

* [Annotation des Bulletins de santé du végétal (BSV) et interrogation](http://renatis.cnrs.fr/IMG/pdf/FreDoc_Irstea.pdf)  
Fabien Amarger , IRIT-IRSTEA  
Frédocs2013 - Gestion et valorisation des données de la recherche - 7 au 10 octobre 2013, Aussois


Plus récemment, Laurence El Khoury (DIST-CNRS) et Stéphane Schneider (INIST – CNRS) à l’occasion des Frédocs2018 ont présenté les projets [ISTEX](https://www.istex.fr/),  [VisaTM](https://objectif-tdm.inist.fr/2018/11/29/le-projet-visa-tm/) et [OpenMinteD](http://openminted.eu/) pour illustrer la mise à disposition d’une infrastructure de text-mining. 
ISTEX est une base documentaire qui propose un accès à distance et de manière pérenne à un corpus multidisciplinaire (plus de 23 millions de documents) en texte intégral. Cette base propose également des services permettant la mise en place de traitements des données : extraction, fouille de textes, production de synthèses documentaires. La première partie de l’intervention présente les objectifs, les ressources et les possibilités offertes par la plateforme. 
La seconde partie s’intéresse plus particulièrement à la fouille de texte et au projet VisaTM. Ce projet porté par l’INRA vise à étudier les conditions de production de services TDM à haute valeur ajoutée basés sur l’analyse sémantique à destination des chercheurs pour une généralisation des approches TDM dans les activités de recherche.
Le Projet européen H2020 OpenMinteD présenté en 3ème partie est une e-infrastructure encourageant et facilitant l’utilisation des technologies de fouille de textes. Sa connexion à ISTEX permet l’exploration de corpus. Plateforme open source, ouverte et pérenne elle offre aux chercheurs la possibilité de découvrir, créer, partager et réutiliser des logiciels, des documents et des ressources pour le text-mining, le TALN, l’Extraction d’Information en travaillant à partir de sources documentaires licitement fouillables tels qu’ISTEX, OPENAIRE et CORE. 

* [Bases de ressources numériques et services aux chercheurs. Avec ISTEX et OpenMintedD, l'alliance pour une infrastructure de text-mining]( https://fredoc2018.sciencesconf.org/data/pages/Pres_istex_visa_omtd_ElKhouri_Schneider.pdf)  
Laurence El Khoury (DIST-CNRS), Stéphane Schneider (INIST – CNRS)  
Frédocs2018 - Démarches innovantes en IST : expérimenter, proposer, (se) réinventer, 3-5 octobre 2018, Albi


