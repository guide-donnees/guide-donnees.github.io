# Analyser

## Contexte
La participation à l’analyse des données se réalise au travers d’un accompagnement sur la mise en place de procédures qualité, par l’optimisation des calculs ou des temps d’accès, 
ainsi que dans le choix et la mise en œuvre des techniques et outils d’analyse et éventuellement dans leur conception,en fonction des besoins 
(formats, interopérabilité, performance, ergonomie, visualisation, travail collaboratif...)

   * Traitement de grosses masses de données (BigData), regroupement de données, filtrage et extraction des données pertinentes (ANF 2016)
   * <https://indico.mathrice.fr/event/5/contribution/19/material/slides/0.pdf>
Nouveaux outils d'analyse des masses de données (IA) (JoSy 2018)

   * <https://indico.mathrice.fr/event/130/contribution/3/material/slides/0.pdf>
  
## Mettre en place un contrôle Qualité des données

Par nature, la recherche n’est pas répétitive mais riche en incertitudes contrairement à un processus industriel. 
La confiance dans la qualité d’une recherche consiste donc à établ ir et vérifier que les différentes étapes d’une étude peuvent être répétées en obtenant le même résultat par des chercheurs différents à des moments différents.
Ainsi, une donnée est fiable, si dans des conditions données, aucune déviation n’est constatée en fonction du temps, durant un laps de temps donné. 
Il est donc essentiel de s’assurer que l’ensemble des activités de recherche soit maitrisée, cela est le cas de toute la chaine fonctionnelle d’une analyse (par exemple des pipettes, des balances jusqu’aux équipements d’analyse). 
De ce fait, il convient d'avoir une totale maitrise des équipements :
* <https://qualsimp.sciencesconf.org/data/program/9_Trac_abilite_des_donne_es_de_la_recherche_Virginie_JAN_LOGASSI.pdf>

   * [sist] Suivi de la qualité des mesures de réseaux d'observations océanographiques
       * <https://sist16.sciencesconf.org/data/pages/09\_P\_Techine.pdf>
           - Philippe Téchiné, Laboratoire d'études en Géophysique et océanographie spatiales

   * Plusieurs présentations et ateliers sur ce thème ont eu lieu lors de l'ANF « Sciences des données : un nouveau challenge pour les métiers liés aux bases de données »(<http://rbdd.cnrs.fr/spip.php?article288)> - du 5 au 7 novembre 2018 à Sète. En particulier l'Atelier qualité des données dont les travaux portaient sur les questions suivantes : 
       * Quelles sont les différentes notions de **qualité** des données ?
       * Comment contrôler la **qualité** des données dans la BDD : avant ou pendant l’insertion de données
       * Faut-il automatiser le contrôle de la qualité dans les bases ?
       * Quels sont les outils disponibles et comment les utiliser ?    L'Introduction méthodologique et terminologique (<http://rbdd.cnrs.fr/IMG/pdf/qualite\_des\_donnees\_plumejeaud\_2018\_04112018.pdf?517/365a13edab604bd0700b045bfac29a3607acb649)> a été suivie d'un cours et de TP portant sur "OpenRefine pour traiter son fichier d’entrée" (<http://rbdd.cnrs.fr/IMG/pdf/openrefinecours.pdf?518/a69ce451abd02003a0e96957e39828e0f2e9f2ee> , <http://rbdd.cnrs.fr/IMG/pdf/openrefinedoc.pdf?519/a6de5321fdbedeec29da6cc8b82250d02937ddeb> , <http://rbdd.cnrs.fr/IMG/zip/exos.zip?520/e51f82826431b71f767e4347fd57716fa9175664> )
    On pourra aussi de référer à quelques présentations de l''ANF « Interfacer les outils mobiles avec son système d’information » citée au chapitre 3 (Collecter) :

       *  Outils nomades : validation des données (C. Plumejeaud) (<http://rbdd.cnrs.fr/IMG/pdf/anf\_rbbd\_2019\_outils\_mobiles\_tp\_qualite.pdf?573/e1425561fd10c6bd1dd92fdee22871bc427f9873)>
       *  Retour terrain : la délicate question de l’intégration des données (PY. Arnould) (<http://rbdd.cnrs.fr/IMG/pdf/anf2019\_seshat.pdf?576/575888582b8771a01200c5a6a5e751f0964e0c33)>

   * BLONDEL, Emmanuel, 2018. Ecrire et Lire des métadonnées avec la librairie R geometa. In : *Atelier « Métadonnées et R »*. Montpellier. 2018. <http://rbdd.cnrs.fr/IMG/pdf/workshop\_r\_metadata\_agropolis\_-\_geometa.pdf?504/f5cc31589976b1cd1fc18d406a547ee18122c0e7>

   * [resinfo] Traitement de grosses masses de données (BigData), regroupement de données, filtrage et extraction des données pertinentes (ANF 2016)
   * <https://indico.mathrice.fr/event/5/contribution/19/material/slides/0.pdf>


## Mettre en place des méthodes d'analyse et des chaînes logicielles
La majorité des instruments complexes incluent un ou plusieurs logiciels dont la définition et la mise en place s’inscrivent dans un projet à part entière qui doit parfaitement s’interfacer avec le reste de l’instrument.
Dans ce contexte, il convient d'assurer le suivi des exigences liées au logiciel, la gestion des interfaces avec le reste de l’instrument et l’activité Assurance Qualité Logiciel. 
L’ Assurance Qualité Logiciel permet notamment de répondre à des exigences applicables à un logiciel, du développement à la maintenance de celui-ci.
L’ensemble des activités, normes, contrôles et procédures mis en place doit couvrir la totalité de la durée de vie d’un logiciel. Il est par exemple important de vérifier et valider au travers de tests 
la bonne santé du code et de constamment veiller à la traçabilité qui lui est liée.
    * Qu'est-ce qu'un logiciel et qu'est-ce que la qualité ? : <https://jt-aql.sciencesconf.org/data/program/jt_aql_Paris_2019_v5_HV.pdf>
    * Plans de Gestion de Logiciel et Assurance Qualité Logiciel, les apports de PRESOFT : <https://jt-aql.sciencesconf.org/data/program/PRESOFT_qualite_en_recherche2019v1_GR.pdf> 
    * Référentiels et normes de codage : <https://jt-aql.sciencesconf.org/data/program/ref_normes_codage_ZT.pdf>
    * Qualité Logiciel dans un projet de Nanosatellite : <https://jt-aql.sciencesconf.org/data/program/qualite_nanosat_v2_CG.pdf>

Certains logiciels comme ODV (Ocean Data view)  pemettent de qualifier les données et d'attribuer un code qualité a des données apres analyse par un expert du domaine.
ODV est un logiciel utilisé par le projet Européen SeadataNet https://www.seadatanet.org/Software/ODV


   * Un partage d'expérience sur la réalisation d’une application d’exploration de données [*donner des précisions]*
       *    [sist] Filtrage interactif de données multi-dimensionnelles
       *    <https://sist16.sciencesconf.org/data/pages/08\_P\_Brockmann.pdf>
           *  Patrick Brockmann, Laboratoire des Sciences du Climat et de l'Environnement [Gif-sur-Yvette]                                                                                       
   * Site Web de diffusion des données "Sahelian Dust Transect"                                         
       * <https://sist16.sciencesconf.org/data/pages/10\_A\_Campos.pdf>
           - André CAMPOS, Laboratoire inter-universitaire des systèmes atmosphèriques

   *  Gestion des données de cytométrie en flux   
       * <https://sist16.sciencesconf.org/data/pages/06\_S\_Lahbib.pdf>
           - SOUMAYA LAHBIB, MIO - Mathilde DUGENNE, - Melilotus Thyssen, Institut méditerranéen d'océanologie,  - Maurice Libes OSU Pytheas 

   *   L'usage d'outils, de données et de webservices interopérables pour la cartographie des formations à silex en France : l'expérience du PCR « Réseau de lithothèques "
       * <https://sist16.sciencesconf.org/data/pages/05\_C\_Tuffery.pdf>
           - Christophe Tufféry, Cités, Territoires, Environnement et Sociétés, Institut National de Recherches Archéologiques Préventives                                                                                      

   * Réseau d'observation du Pacifique Sud ‘ReefTEMPS' : évolutions fonctionnelles et optimisation d'un système d'information dédié capteurs et reconstitution de séries temporelles                   
       * <https://sist16.sciencesconf.org/data/pages/12\_R\_Hocde.pdf>                     
           - Régis Hocdé, Institut de Recherche pour le Développement

### Optimisation des calculs ou des temps d’accès

   * « Le défi MASTODONS : un instrument pour la gestion et l’exploitation de grandes masses de données » : Mokrane Bouzeghoub (*DAS INS2I, Mission pour l’interdisciplinarité, CNRS, projet Mastodons*) - ANF Frédocs2013 - Gestion et valorisation des données de la recherche -  7 au 10 octobre 2013, Aussois
       * <http://renatis.cnrs.fr/IMG/pdf/Mastodons-Fredoc-Aussois.pdf>

### Traitements sémantiques/ linguistiques

**« Des technologies sémantiques pour l’information scientifique et technique » **
* Claire Nedellec et Agnès Girard, INRA
* Frédocs2013 - Gestion et valorisation des données de la recherche -  7 au 10 octobre 2013, Aussois
* http://renatis.cnrs.fr/IMG/pdf/Nedellec_Girard_Fredoc-v4.pdf  

Dans cette présentation, les auteurs expliquent les **principes de l’analyse sémantique de texte** à travers un exemple en recherche documentaire et présentent le projet 
TirPhase.  La notion d’**indexation sémantique**  à travers un exemple en physiologie animale est abordée en début de présentation sous les traits d’une carte d’identité 
thématique associée au document.  Les auteurs présentent également la notion de **termino-ontologie** et définissent l’ontologie comme « un graphe où les nœuds sont des 
concepts et les arcs des relations entre ces concepts ». Elles expliquent que l’analyse sémantique identifie les unités sémantiques du texte et les associe aux concepts 
de l’ontologie. Partant de là, elle présente le **processus de conception de termino-ontologie** à partir de corpus en deux étapes : extraction automatique de termes avec 
l’outil Syntex et structuration et modélisation avec l’outil Protégé. La deuxième partie de la présentation est consacrée à la présentation du projet TriPhase. 
Ce projet a pour objectif d’**analyser les publications** d’un département de recherche à des fins stratégiques (analyse quantitative des termes au cours du temps) et 
disposer d’un moteur de recherche sémantique bibliographique spécialisé. Les auteurs expliquent les différentes phases de la construction de la termino-ontologie 
TriPhase et l’apport des documentalistes dans ce travail collaboratif.

** « Annotation des Bulletins de santé du végétal (BSV) et interrogation »**
* Fabien Amarger , IRIT-IRSTEA
* Frédocs2013 - Gestion et valorisation des données de la recherche -  7 au 10 octobre 2013, Aussois
* http://renatis.cnrs.fr/IMG/pdf/FreDoc_Irstea.pdf 

Dans cette présentation, Fabien Amarger présente un projet qui consiste à **construire une base de connaissanceà partir de source et de données hétérogènes**. Il explique ce qu’est une base de connaissance et comment l’interroger.
