# Traiter

## Contexte 
Cette étape du cycle de vie des données est destinée à traiter les données brutes issues des acquisitions, regrouper, choisir les données pertinentes, reformater, gérer les métadonnées :

Les réseaux participent au traitement des jeux de données via la mise en place de procédures ou logiciels assurant la qualité des données et la qualité des traitements 
(modélisation et reformatage des données ; mise en base de données ; mise à disposition de moyens informatiques; aide aux choix de la solution la plus adaptée, détection des
erreurs de saisie ou des incohérences relatives aux modèles de données, mise en place d’un contrôle qualité)


## Mettre en place des chaines et méthodes de traitement


### Métrologie des équipements

Par nature, la recherche n’est pas répétitive mais riche en incertitudes contrairement à un processus industriel. 
La confiance dans la qualité d’une recherche consiste à établir et vérifier que les différentes étapes d’une étude peuvent être répétées en obtenant le même résultat par des chercheurs différents à des moments différents.
Il est donc essentiel de s’assurer que l’ensemble des activités soit maitrisée, cela est le cas de toute la chaine fonctionnelle d’une analyse (des pipettes, balances jusqu’aux équipements d’analyse) et que la traçabilité des activités de recherche soit ainsi assurée. 
   * Métrologie des équipements : <https://qualsimp.sciencesconf.org/data/program/9_Trac_abilite_des_donne_es_de_la_recherche_Virginie_JAN_LOGASSI.pdf>    

De nombreux laboratoires et plateformes de tests du CNRS sont équipés de salles propres, dans des domaines variés tels que la micro et nanotechnologie, la géochimie, l’optique, la médecine, le spatial…
En débutant par un point sur l’état de l’art (définition, réglementation, documentation,…) de ces 2 aspects, l'objectif principal de la journée thématique est de faire bénéficier de retours d’expériences riches sur les bonnes pratiques déjà éprouvées
et sur les écueils à éviter et de répondre entre autres aux questions : 
- Quand a-t-on besoin de travailler en salles propres ? 
- Quelles réglémentations régissent l'installation, la maintenance et le contrôle des salles propres ? 
- Comment préparer l'installation dans nos locaux ? A quoi doit-on penser ? 
- Quelles sont les solutions techniques les mieux adaptées à notre besoin ? 
- Quels sont les critères de surveillance et systèmes de contrôle des installations ? 
- Comment doit-on travailler en salles propres ? Quelles sont les bonnes pratiques de gestion d'une salle propre ?

Journée thématique "Les salles propres de l’installation à l’utilisation, de la théorie à la pratique - Usages et retours d’expériences" : <https://sallespropres17.sciencesconf.org/program>

### Rendre les fichiers de données interopérables

   * Mise en forme des données dans des formats standards, interopérables 
   
Parmi les premiers traitements opérés sur des données brutes, on doit fréquemment filtrer, extraire les données utiles dans des fichiers bruts issus des acquisitions par des capteurs.

Les données issues de capteurs environnementaux sont fréquemment dans des formats propriétaires, souvent illisibles et peu exploitables par un être humain.Il convient alors de traiter 
les fichiers bruts de manière à en extraire les données utiles et les réécrire dans des formats standards utilisables par un grand nombre de logiciels, et une communauté d'utilisateurs


Dans le domaine environnemental et particulierement pour les données atmosphériques et océanographiques, le format de fichier [NetCDF](https://www.unidata.ucar.edu/software/netcdf/)
est particulièrement interessant pour représenter des données qui représente des séries temporelles, de profils verticaux ou de trajectoires.
Il est très utilisé en sciences de l'environnement par exemple en océanographie, météorologie, atmosphere. 

Ce format de fichier présente l'avantage de fournir des données autodocumentées, grace une entete qui permet d'inscrire un grand nombre de métadonnées 
dans le fichier lui même.

Ainsi les métadonnées sont insérées et véhiculées dans le fichier avec les données elles memes.
On peut ainsi décrire d emanière assez précise les données du fichier en inscrivant les unités des données, la licence, les proprietaires etc, ainsi que l'organisation des données, 
sans avoir besoin d'un fichier de description complémentaire.


La [convention CF (climate forecast)](https://cfconventions.org/) permet de standardiser autant que faire se peut les métadonnées inscrites dans une entête NetCDF.
Ce format a été présenté lors des journées du réseau SIST

Ce format est préconisé par le pôle de données Odatis et a été présenté au séminaire SIST19 à l'OMP de Toulouse, par Joel Sudre, maurice Libes et Didier Mallarino

* présentation du format NetCDF
  * <https://sist19.sciencesconf.org/data/pages/SIST19_Atelier_NetCDF_JS.pdf>
* La convention CF (climate forecast) pour les fichiers NetCDF
  * <https://sist19.sciencesconf.org/data/pages/SIST19_Atelier_NetCDF_CF_ML.pdf>
* utilisation de l'API de programmatino Python pour NetCDF
  * <https://sist19.sciencesconf.org/data/pages/SIST19_Atelier_NetCDF_python.pdf>

**retour d'expérience**

   * Copier les succès et rester simple (AMEO) : Mise à disposition de sorties de modèles climatiques avec un NAS, THREDDS et ERDDAP.                                      
       * <https://sist16.sciencesconf.org/data/pages/11\_T\_Valero\_F\_Bongat.pdf>
           - Thierry VALERO, Institut de Recherche pour le Développement, Laboratoire   d'Océanographie et du Climat : Expérimentations et Approches Numérique
              * <https://sist16.sciencesconf.org/data/pages/12\_R\_Hocde.pdf>

   * Réseau d'observation du Pacifique Sud ‘ReefTEMPS' : évolutions fonctionnelles et optimisation d'un système d'information dédié capteurs et reconstitution de séries temporelles                   
       *   <https://sist16.sciencesconf.org/data/pages/12\_R\_Hocde.pdf>                     
           - Régis Hocdé, Institut de Recherche pour le Développement
           - 
Le format ODV (ocean data view) [https://odv.awi.de/](https://odv.awi.de/)  est également un format standard intéressant. Il se rapproche d'un format CSV i.e des colonnes de données séparées 
par des virgules (ou tout autre séparateur), à cette différence près que le format ODV permet l'insertion d'une entete assez riche permettant de placer des métadonnées en début de fichier

Les formats NetCDF et ODV sont les formats recommandés et utilisés par le pôle de données Odatis et par le projet Européen Seadatanet.


### Utiliser un framework d'aggrégation de données

Lorsque les données à traiter sont hétérogènes et que les technologies qui permettent de les fournir sont également différentes, une solution est d'utiliser un framework d'aggrégation de données comme Lavoisier. Cette présentation vous permettra d'en découvrir les aspects techniques et les utilisations possibles. 

* [Lavoisier : un framework d'agrégation de données](https://jcad2018.sciencesconf.org/data/jcad2018_lavoisier_2_.pdf)  
    [vidéo](https://webcast.in2p3.fr/video/lavoisier-un-framework-dagregation-de-donnees-1)  
    Cyril L'Orphelin, Sylvain Reynaud, CC-IN2P3, CNRS, 2018

### Déposer et traiter dans des Plateformes de gestion de données locales


* Présentations génériques
   * Introduction au langage de programmation Julia et son utilisation dans le cadre du traitement de données. Journée Julia, Map Reduce, janvier 2019
       * <https://calcul.math.cnrs.fr/2019-01-journee-julia.html#collapseSupports1>. Xavier Vasseur, ISAE

   * Python et l'écosystème disponible pour la data science: <https://calcul.math.cnrs.fr/2017-12-journee-python-data.html>


* Présentations relatives à différentes disciplines scientifiques :

Les services informatiques de certains laboratoires de recherche et des OSU ("Observatoires des Sciences de l'Univers") ont pour vocation et pour mission d'intervenir dans la gestion des
données d'observation acquises sur le terrain.  Après les phases de collecte et de transfert de données que nous avons vues dans les
précédentes étapes du cycle de vie des données, il est nécessaire de se préoccuper de la valorisation des données en permettant la facilité d'accès et la réutilisation des données

Un certains nombre de logiciel font office de plateforme d'accès et de gestion des données. Ils permettent de présenter les données
avec leur métadonnées, de fournir des interfaces de recherche, de géolocaliser les données, et parfois de visualisation des données
avec des graphes

La gestion des données demande donc aux informaticiens concernés de développer des compétences dans des domaines très divers (gestion
de la production des données, organisation de la donnée, gestion de la géolocalisation, gestion de la donnée dans le temps, mise à
disposition et visualisation des données, modèle OAIS, ...).


Des logiciels sont particulièrement adaptés dans la diffusion et l'affichage des données scientifiques d'observation par le fait qu'ils
utilisent les standards interopérables de l'Open Geospatial Consortium (OGC), ISO19139, Inspire, etc)

*  le serveur cartographique *[Geoserver](http://geoserver.org/)* permet d'afficher et permettre les échanges de données géospatiales sur le web selon les standards (WMS, WFS, ...) de l'OGC ;
    
*  le logiciel *[GeoNetwork](https://geonetwork-opensource.org/)* permet de  cataloguer les métadonnées de jeux de données selon la norme ISO 19139 et la directive Inspire;
    
*  l'application GeoCMS et [GeoOrchestra](https://www.georchestra.org/) permettant la visualisation de données géospatiales sur le web et de mettre en place une Infrastructure de Données Géographique (IDG) 
    
*  Les logiciels comme Thredds et [Erddap](https://coastwatch.pfeg.noaa.gov/erddap/index.html) sont des plateformes web de gestion de données particulièrement adaptée pour rendre les données FAIR dans un établissement et faciliter 
la diffusion des données 

ERDDAP fournit un ensemble complet de fonctionnalités pour la gestion des jeux de données. Il permet :
* de lire et écrire des jeux de données dans de nombreux formats différents,
*     fournir un catalogue des jeux de données gérés par le serveur
*     d'afficher les metadonnées
*     d'interroger et filtrer les données au travers de formulaires,
*     de créer des graphiques et des cartes simples pour visualiser le jeu de données analysé

Une des fonctionnalités intéressantes est que ERDDAP aggrège automatiquement les données nouvelles répondant a un format donné, et qui sont déposées dans un répertoire. Ainsi
pour les séries temporelles cetet foncitonnalité est intéressante puisqu'il suffit de déposer des fichiers dans un répertoire pour que la série temporelle soit automatiquement enrichie
et constituée.

L'objectif est donc de faciliter l'affichage, l'accès et la mise à disposition de données scientifiques à travers des standards interopérables.


L'utilisation de ces logiciels a fait l'objet d'une  Action  nationale de formation du [réseau SIST](http://sist.cnrs.fr) qui permet de savoir installer, configurer et utiliser ces logiciels
   * [<https://sist.cnrs.fr/les-formations/supports-des-anf-gestion-de-donnees-dobservation>](<https://sist.cnrs.fr/les-formations/supports-des-anf-gestion-de-donnees-dobservation>)



Les événements suivants organisés par le réseau SIST fournissent  un certain nombre de connaissances sur l’utilisation d'infrastructure
de données géographiques (IDS, IDG) et de plateforme logicielles de gestion des données


   *  infrastructure de données spatiales et de traitements GEOSUD : des standards à la réalité     
       * <https://sist16.sciencesconf.org/data/pages/01\_JC\_Desconnets.pdf> 
       * Jean-Christophe Desconnets, UMR Espace-Dev 
       
   * Publication automatique de données et de métadonnées dans geOrchestra
       * <https://sist18.sciencesconf.org/data/pages/19_E_Chiarello_GeOrchestra.pdf>
       * Ernest CHIARELLO, Théoriser et modéliser pour aménager

   *  OSUNA : Mise en place d'une IDS pour le programme de recherche Réseau de Suivi et de Surveillance de l'Environnement.
       * <https://sist16.sciencesconf.org/data/pages/02\_L\_Salaun.pdf>
       *  LOIC SALAUN, Observatoire des Sciences de l'Univers Nantes Atlantique 

   *   Sextant, infrastructure de données géographiques marines et littorales
       * <https://sist16.sciencesconf.org/data/pages/03\_M\_Treguer.pdf>
       * Catherine Satra Le Bris, Ifremer - IDM/SISMER - Mickael Treguer, Ifremer    

   *   Parc National de planeurs sous-marins    
       * <https://sist16.sciencesconf.org/data/pages/04\_K\_Bernardet.pdf>
       * Karim Bernardet, Division Technique de l'INSU   

   *   GBIF - Système Mondial d'Information sur la Biodiversité
       * <https://sist16.sciencesconf.org/data/pages/07\_ME\_Lecoq.pdf>
       * Marie-Elise Lecoq, Muséum national d'histoire naturelle                      

   * GBIF – Global Biodiversity Information Facility et réseau des portails nationaux « Living Atlases »
       * <https://sist18.sciencesconf.org/data/pages/17_AS_Archambeau_GBIF.pdf>
       * Anne-Sophie Archambeau, IRD/MNHN/UMS PatriNat/GBIF France


Utilisation de plateformes d'accès aux données 

   * Copier les succès et rester simple (AMEO) : Mise à disposition de sorties de modèles climatiques avec un NAS, THREDDS et ERDDAP. 
       * <https://sist16.sciencesconf.org/data/pages/11\_T\_Valero\_F\_Bongat.pdf>
       * Thierry VALERO, Institut de Recherche pour le Développement, Laboratoire   d'Océanographie et du Climat : Expérimentations et Approches Numérique

VIP, the Virtual Imaging Platform, est un portail qui permet à ses utilisateurs d'accéder simplement à leurs données, de les traiter facilement avec des logiciels pré-installés sur le plateforme. Traitements et données sont distribués sur l'infrastructure EGI. Pour répondre au besoin d'interopérabilité des données, l'API CARMIN est maintenant utilisée par VIP. Cette présentation explique les différentes étapes du fonctionnement du système mis en place. 

* [VIP : towards data interoperability through CARMIN](https://jcad2019.sciencesconf.org/data/VIP_Axel_Bonnet.pdf)  
    [vidéo](https://prismes.univ-toulouse.fr/player.php?code=vC6b9705&width=100%&height=100%)  
    Axel Bonnet, Pascal Wassong, Frederic Cervenansky, Tristan Glatard, Camarasu-Pop Sorina, 2019

## Assurer la curation des données

*[il faudra expliciter le terme et les concepts svp] = organiser /traiter différents éléments d'une même collection pour mieux la diffuser, la valoriser c a d commenter, annoter, décrire etc...*
JJ : pour moi la curation des données englobe tous les aspects pris en compte par les différents points du guide. La curation est un tout. 
ML: oui je suis assez d'accord avec cette définition.. du coup ce paragraphe **n'a pas sa place ici**, mais dans l'intriduction "Imaginer" sur la gestion des données

« Les activités de curation de données permettent de faciliter la découverte et la récupération de données, de maintenir la qualité des données, de leur ajouter de la valeur et d’en fournir pour de futures réutilisations. Ce nouveau champ inclut la représentation, l’archivage, ’authentification, la gestion, la préservation, la récupération, et l’utilisation. »
Digital Humanities Data Curation


**« De quelques défis spécifiques de la curation numérique des données en SHS : petite incursion dans l’univers de l’édition critique de sources au format TEI »**
* Emmanuelle Morlock, HiSoMa)
* Frédocs2013 - Gestion et valorisation des données de la recherche -  7 au 10 octobre 2013, Aussois     
* http://renatis.cnrs.fr/IMG/pdf/emma-morlock-fredocs-2013

Cette présentation s’organise en trois parties : les spécificités de la **« data curation »**, les d**éfis spécifiques aux SHS** et ce que la **TEI** propose pour les relever 
à travers l’exemple de l’**encodage de sources textuelles**. Dans un  premier temps Emmanuelle Morlock définit la **notion de curation**, les notions qui gravitent autour 
et celle de data curation. Elle présente les **activités transversales associées à la curation des données** et l**es défis que cela représente pour les sciences humaines 
et sociales**. Elle s’intéresse ensuite aux **types d’objets de la curation**, les difficultés associées. Elle aborde ensuite le chapitre de l’**édition savante** en précisant 
qu’il s’agit de bien plus qu’une simple reproduction augmentée, indiquant par exemple qu’il convient là de garantir la fiabilité et la lisibilité du texte, de mettre 
en forme un dispositif de lecture en fonction des attentes et des conventions d’un lectorat donné, de faire des choix etc … **Le numérique** apporte à ce stade un certain 
nombre de **promesses** (démultiplie les interfaces de lecture et les possibilité de navigation) mais connait aussi **ses limites** (demande des investissements, des compétences 
spécifiques en encodage, structuration etc.). Ceci l’amène à définir précisément ce qu’est l’**édition numérique** (un texte enrichi, exploitable par des machines) et à 
présenter, définir et expliquer le **processus d’édition dans un format XML TEI**. Elle explique aussi l’apport de la TEI dans la réponse aux défis posés par l’édition 
numérique (distinction de niveaux d’interprétation via le balisage, conservation et documentation des choix de manière formalisée) et termine sa présentation sur le 
**rôle des « curateurs** » pour répérer les manques dans un objectif de réutilisation à long terme ou pour aider les chercheurs à améliorer leurs pratique de documentation 
de leurs données.

*Voir si on place ici la présentation de soizick lesteven ou plus haut dans retours d’expérience*
  
