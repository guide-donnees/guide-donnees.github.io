# Collecter

Cette phase du cycle de vie de la donnée, concerne les aspects d'acquisition et de collecte des données ainsi que la constitution des jeux de données ("dataset" en anglais) avec leurs métadonnées descriptives. 
Il s'agit donc, dans cette phase, de travailler sur les processus d'acquisition des données qui peuvent être obtenues au moyen de divers medias : capteurs environnementaux, instruments, sondages, modèles numériques... selon le domaine étudié. 
Une fois les données acquises, il est nécessaire et indispensable dans l'objectif de les rendre "FAIR", de les décrire avec leurs métadonnées associées.  


La description  de ces jeux de données nécessite d'utiliser, autant que faire se peut, des référentiels de vocabulaires contrôlés (thésaurus) si possible standardisés et les plus appropriés au domaine étudié.

Il est conseillé de gérer les jeux de données dans un environnement technique qui permet d’assurer la sauvegarde, l'archivage, le "versionning",  l’accessibilité et
l'interopérabilité des données. Cette gestion se fait via des infrastructures techniques, des bases ou des supports qui doivent etre fiables et bien documentés, et dans le respect des règles de traitement spécifiques des données personnelles.

Cette phase "Collecter" va nécessiter :

   * de disposer des données et des *metadonnées* précises pour la description des données brutes elles-mêmes (libellés des paramètres, unités de mesure) ainsi que des dispositifs d'acquisition (capteurs de mesures, modèles numériques...);
   * de disposer de la *documentation*  des chaines de collecte : du capteur jusqu'aux espaces disques et aux applications sur des serveurs où les traitements pourront être réalisés;
   * de mettre en place une gestion et conduite de projets pour faire travailler ensemble les différents acteurs intervenant dans la chaîne de collecte : électroniciens, informaticiens, chercheurs...;
   * de disposer de *cahier de laboratoire, tablettes de terrain* ou supports divers pour consigner les relevés et métadonnées observées; 
   * d'utiliser des *protocoles si possibles normalisés* ou standardisés pour présenter les données brutes et les dispositifs d'acquisition (capteurs...) et les rendre interopérables;
   * de définir le stockage nécessaire à la collecte de données : travailler en amont avec une équipe informatique en mode projet (gestion de projet).


## Utiliser des Normes et Standards d'interopérabilité

"L’interopérabilité est la capacité que possède un produit ou un système, dont les interfaces sont intégralement connues, à fonctionner avec d'autres produits ou systèmes existants ou futurs et ce sans restriction d'accès ou de mise en œuvre" (https://aful.org/gdt/interop).

Développer l'interopérabilité consiste donc à mettre en place et utiliser des normes et des standards qui fixent des règles permettant d'assurer le bon fonctionnement de deux systèmes informatiques.

### Formats et métadonnées

Dans l'optique d'une gestion "FAIR" des données, il est nécessaire, autant que faire se peut et lorsqu'elles existent, de suivre des normes et des standards dans la description des métadonnées, les formats de fichiers et les protocoles d'échange de données.

Catherine Morel-Pair propose une présentation très riche et très complète sur les formats et métadonnées qu’elle détaille de manière très approfondie et restitue dans le cadre de leur utilisation pour la gestion de contenu et la documentation des données. Elle aborde en introduction les notions de données de la recherche, de Fair Data, d’intéropérabilité et de Data Management Plan. 
- La première partie de sa présentation porte sur les fichiers de données (organisation et nommage, format et critères d’interopérabilité-pérénnité) 
- la deuxième partie est dédiée aux métadonnées et à la documentation  (définitions, présentation des standards, des identifiants pérennes pour les données et syntaxes d’échange). Elle termine par un focus sur les sites de de dépôt, de portails ou d’entrepôts de données et leur schéma de métadonnées associés.

* [Participer à l’organisation du management des données de la recherche : gestion de contenu et documentation des données](https://anfdonnees2016.sciencesconf.org/data/pages/2016_07_07_ANF_Renatis_Formats_Standards_et_Metadonnees_1.pdf)
* [vidéo](https://youtu.be/obGDFrXyBiU?t=3)  
Catherine Morel-Pair , INIST, CNRS  
ANF "Participer à l'organisation du management des données de la recherche : gestion de contenu et documentation des données" -  6-8 juil. 2016 Paris (France)   

**Les métadonnées dans un DMP**

Cette présentation très riche de Marie Puren a été conçue pour animer un atelier de formation qui avait pour objectif de définir et comprendre l’importance des metadonnées dans le cadre de la rédaction d’un DMP. Elle définit, en donnant des exemples, ce qu’est une métadonnée, à quoi elle sert, quelle information elle donne. Elle distingue et détaille la spécificité des métadonnées de description, des métadonnées de gestion et des métadonnées de préservation. 

Elle aborde ensuite le chapitre du cycle de vie des métadonnées (créer, entretenir, mettre à jour, stocker, gérer la suppression des données, publier). Elle spécifie les métadonnées à faire figurer dans un DMP, explique comment les collecter et propose quelques outils d’extraction automatique de métadonnées. Autour de la notion de métadonnée, elle précise l’importance de définir des responsabilités en s’appuyant sur les chercheurs, documentalistes, bibliothécaires et informaticiens. Elle complète sa présentation avec une description des principaux standards interdisciplinaires et disciplinaires de métadonnées. Elle explique où et comment choisir ces standards. 
Elle explique également l’intérêt d’associer des ontologies ou vocabulaires contrôlés. Les dernières recommandations de sa présentation portent sur la gestion des métadonnées à long-terme, l’importance d’évaluer leur qualité et revient sur la notion d’ouverture des métadonnées et la nécessité de choisir des licences pour nos métadonnées.

* [Les métadonnées dans un DMP](https://anfdonnees2017.sciencesconf.org/data/pages/20170706_dmp_metadonnees_puren_1.pdf)  
Marie Puren,  INRIA  
ANF "Participer à l'organisation du management des données de la recherche : gestion de contenu et documentation des données" -  3-6 juil. 2017 Paris (France) 

### Référentiels de métadonnées 

Les référentiels de métadonnées qui peuvent être soit des standards, soit des normes, sont des documents importants qui se chargent de définir les *informations nécessaires pour décrire les données elles mêmes*. Elles sont de ce fait utilisées pour structurer les ressources informatiques et faciliter leur interopérabilité. Il est donc fortement recommandé de décrire ses données avec des normes ou des standards reconnus dans les disciplines concernées.

On trouve plusieurs standards et normes qui permettent de définir un ensemble de métadonnées sur des jeux de données. 

Parmi les standards les plus connus ou utilisés, citons :
- **Le Dublin Core**, standard généraliste issu d'un consensus international et multidisciplinaire a pour objectif de fournir un socle commun d'éléments descriptifs suffisamment structuré pour permettre une interopérabilité minimale entre des systèmes conçus indépendamment les uns des autres. Le Dublin Core <https://fr.wikipedia.org/wiki/Dublin_Core> est un vocabulaire du web sémantique utilisé pour exprimer les données dans un modèle RDFa. 

Le Dublin Core définit un ensemble d'items de métadonnées obligatoires pour décrire les données <https://fr.wikipedia.org/wiki/Dublin_Core#Dublin_Core_element_set> :

1. Titre (métadonnée) 	Title 	Nom donné à la ressource
2. Créateur (métadonnée) 	Creator 	Nom de la personne, de l'organisation ou du service responsable de la création du contenu de la ressource
3. Sujet (métadonnée) ou mots clés 	Subject 	Thème du contenu de la ressource (mots clés, expressions, codes de classification)
4. Description (métadonnée) 	Description 	Présentation du contenu de la ressource (résumé, table des matières, représentation graphique du contenu, texte libre)
5. Éditeur 	Publisher 	Nom de la personne, de l'organisation ou du service responsable de la mise à disposition ou de la diffusion de la ressource
6. Contributeur 	Contributor 	Nom de la personne, de l'organisation ou du service responsable de contributions au contenu de la ressource
7. Date (métadonnée) 	Date 	Date de création ou de mise à disposition de la ressource
8. Type 	Type 	Nature ou genre de la ressource (catégories, fonctions, genres généraux, niveaux d'agrégation du contenu)
9. Format 	Format 	Manifestation physique ou numérique de la ressource
10. Identifiant de la ressource 	Identifier 	Référence univoque à la ressource dans un contexte donné (URI, ISBN)
11. Source 	Source 	Référence à une ressource dont la ressource décrite est dérivée (URI)
12. Langue (métadonnée) 	Language 	Langue du contenu intellectuel de la ressource
13. Relation (métadonnée) 	Relation 	Référence à une ressource apparentée
14. Couverture (métadonnée) 	Coverage 	Couverture spatio-temporelle de la ressource (domaine d'application)
15. Gestion de droits (métadonnée) 	Rights 	Informations sur les droits associés à la ressource (IPR, copyright, etc.)

Le choix d'un standard de métadonnées va dépendre du type de ressource, du domaine d'application mais également de la communauté à laquelle on s'adresse. Le site du Digital Curation Centre (https://www.dcc.ac.uk/guidance/standards/metadata) recense les standards de métadonnées par grandes disciplines (biologie, physique...). Des outils informatiques, permettant de passer d'un standard à un autre, sont également disponibles.  


### Référentiels de métadonnées en information Géographique 

Pour la description des jeux de données géolocalisées, les normes ISO 19115 et ISO 19139 sont des normes de référence pour l'information géographique dans le domaine des métadonnées. Elles sont notamment utilisées dans les disciplines nécessitant de représenter des données géospatialisées.

La norme ISO 19139 est l'implémentation XML de la norme ISO 19115. Elle définit le codage XML des méta-données géographiques, une implémentation de schéma XML dérivée de la norme ISO 19115. La norme ISO 19139 est le modèle principal utilisé pour décrire des données dans le logiciel GeoNetwork et constituer ainsi un catalogue de données géospatialisées que l'on abordera dans le chapitre 7. Publier de ce guide.

**Les données dans le domaine environnemental**
[MCQ - début : pour moi tout ce passage concerne le catalogage et pas les métadonnées et perso je trouve cela trop long - je ferai bien une annexe 97 - les données en information géographique : fin - MCQ]
[ca traite du cas des normes de metadonnées geographiques, j'ai enlevé le paragraphe des catalogues qui est abordé au chap. 7]

Dans le domaine environnemental, pour des données qui sont souvent *géolocalisées* par des coordonnées latitude/longitude, l'Open Geospatial Consortium (OGC) (https://www.ogc.org/standards/), est un consortium international qui a pour objectif de développer et promouvoir des standards ouverts, les spécifications OpenGIS, afin de garantir l'interopérabilité des contenus, des services et des échanges dans les domaines de la géomatique et de l'information géographique. 

Ces standards OGC ont été présentés à plusieurs reprises par François André dans les réseaux DEVLOG, et dans le réseau SIST de l'INSU préoccupés par les besoin d'interopérabilités dans les données des Observatoires de l'INSU.

* [Les Normes OGC (Open Geospatial Consortium)](https://sist15.sciencesconf.org/data/program/ogc.pdf)   
François André, Aeris   
Séminaire SIST15 à Marseille OSU Pytheas 

Véronique Chaffard nous présente la mise en peuvre des standards de l'OGC dans le projet AMMA-CATCH :
* [Portail Web d'accès aux données de l'observatoire AMMA-CATCH et mise en oeuvre des standards d'échange des données OGC ](<https://sist15.sciencesconf.org/program>)   
Véronique CHAFFARD Institut de Recherche pour le Développement, Laboratoire d'étude des transferts en hydrologie et environnement  
Séminaire SIST15, Marseille 

Parmi les standards les plus connus édictés par l'OGC et utilisés dans nos réseaux métiers chez les gestionnaires de données environnementales, on peut citer :

-    **[CS-W](https://www.ogc.org/standards/cat)** - Catalog Service for the Web : Le protocole est destiné à diffuser et explorer des fiches de métadonnées présentes dans un catalogue, et permettre l'interrogation de catalogues de métadonnées. Une très bonne implémentation de ce protocole est réalisée dans le logiciel "Geonetwork" <https://geonetwork-opensource.org/> utilisé pour constituer des catalogues et des inventaires de jeux de données et les présenter sur le Web de manière interopérable. 
Grace à ce protocole on peut constituer des réseaux de catalogues tels qu'ils sont demandés par la [Directive Européenne Inspire](http://cnig.gouv.fr/?page_id=8991).

-  **[WMS](https://www.ogc.org/standards/wms)** - *Web Map Service* est un protocole de communication standard qui permet d'obtenir des cartes de données géoréférencées à partir de différents serveurs de données. Cela permet de mettre en place un réseau de serveurs cartographiques à partir desquels des clients peuvent construire des cartes interactives.
   <https://fr.wikipedia.org/wiki/Web_Map_Service>. 
   
Ce protocole est notamment utilisé dans des serveurs cartographiques pour positionner des données sur une carte et fournir les données relatives aux points de mesure. Le réseau SIST a organisé des formations spécifiques sur le standard WMS : https://sist.cnrs.fr/les-formations/anf-2017-1, https://sist.cnrs.fr/les-formations/anf-2018 qui est mis en oeuvre dans des logiciels comme les logiciels "GeoServer", "GeoCMS" 

Les supports de ces formations sont disponibles en ligne sur le site du réseau SIST (https://sist.cnrs.fr/les-formations/supports-des-anf-gestion-de-donnees-dobservation)   


### Les systèmes d'acquisition : maîtriser l’acquisition et la collecte des données

Il est important que le processus de collecte des données soit clairement défini et validé. Par exemple, il conviendra de s'assurer que les systèmes d'acquisition sont bien étalonnés. Par ailleurs, l'ensemble des données produites doit être parfaitement répertorié et enregistré. Nous disposons pour ce faire d’un certain nombre de supports tels que les cahiers de laboratoires, les carnets de terrain... 

#### Métrologie des équipements

Par nature, la recherche n’est pas un processus répétitif, elle est pleine d'aleas et d'incertitudes contrairement à un processus industriel. 
La confiance dans la qualité d’une recherche consiste à établir et vérifier que les différentes étapes d’une étude peuvent être répétées en obtenant le même résultat par des chercheurs différents à des moments différents.

Il est donc essentiel de s’assurer que l’ensemble des activités soit tracées et maitrisées, cela est le cas de toute la chaine fonctionnelle d’une analyse (des pipettes, balances jusqu’aux équipements d’analyse) et que la traçabilité des activités de recherche soit ainsi assurée. 

* [Confirmation métrologique des équipements](<https://qualsimp.sciencesconf.org/data/program/9_Trac_abilite_des_donne_es_de_la_recherche_Virginie_JAN_LOGASSI.pdf>)    
Virginie JAN LOGASSI, DAPEQ LUE   
ANF Outils qualité, réseau QeR, 2019    

De nombreux laboratoires et plateformes de tests du CNRS sont équipés de salles propres, dans des domaines variés tels que la micro et nanotechnologie, la géochimie, l’optique, la médecine, le spatial…
En débutant par un point sur l’état de l’art (définition, réglementation, documentation,…) de ces 2 aspects, l'objectif principal de la journée thématique est de faire bénéficier de retours d’expériences riches sur les bonnes pratiques déjà éprouvées
et sur les écueils à éviter et de répondre entre autres aux questions : 
- Quand a-t-on besoin de travailler en salles propres ? 
- Quelles réglémentations régissent l'installation, la maintenance et le contrôle des salles propres ? 
- Comment préparer l'installation dans nos locaux ? A quoi doit-on penser ? 
- Quelles sont les solutions techniques les mieux adaptées à notre besoin ? 
- Quels sont les critères de surveillance et systèmes de contrôle des installations ? 
- Comment doit-on travailler en salles propres ? Quelles sont les bonnes pratiques de gestion d'une salle propre ?

*  [Les salles propres de l’installation à l’utilisation, de la théorie à la pratique - Usages et retours d’expériences](<https://sallespropres17.sciencesconf.org/program>)
Réseau QeR   
Journée thématique, 2017   


#### Les capteurs

Diverses communautés scientifiques sont intéressées par les problématiques inhérentes aux systèmes d'acquisitions et aux instruments associés. 

Différents aspects de collecte de données existent qu'ils proviennent d'un équipement, d'un capteur automatisé, d'un modèle numérique ou qu'ils soient obtenus par un personnel de terrain, par une enquête, au moyen d'interfaces... 
Dès lors, il convient d'élaborer des méthodologies de collecte, de se documenter sur les choix des référentiels de métadonnées et des thésaurus de vocabulaire, mais également de développer les procédures d'intégration des données dans les bases.

Dans le milieu "Ocean-Atmosphere" cette problématique occupe une place importante, à tel point que, depuis plusieurs décennies, METEO-FRANCE et l'INSU depuis 1966, l'IFREMER depuis 2002, l'IRD et le CNES depuis 2004, le SHOM depuis 2005, organisent un atelier dédié aux rencontres portant sur l'expériementation et l'instrumentation. Cet Atelier Expérimentation et Instrumentation (AEI)
[http://www.aei-ocean-atmosphere.org/](http://www.aei-ocean-atmosphere.org/) permet de réunir la communauté scientifique spécialisée dans la recherche instrumentale et de traiter divers thèmes d'actualité <http://www.aei-ocean-atmosphere.org/Editions-Precedentes>

L'AEI traite de manière privilégiée les aspects de mesure et de méthodologie, sans exclure pour autant l'exploitation scientifique des résultats. Il a lieu alternativement à Paris, Toulon, Lille et Brest, généralement en début d'année. L'AEI permet aux équipes de recherche d'exposer leurs résultats dans un colloque francophone. C'est un lieu de rencontre pour les participants, issus des différents organismes et groupes industriels, afin de favoriser les synergies et coopérations. 

Les gestionnaires de données environnementales mettent en place des chaînes de collecte de données provenant de capteurs de terrains, ou de modèles numériques. Ils se préoccupent de l’utilisation de normes interopérables dans les protocoles d'échange et dans les formats de données.

L'[OGC (Open Geospatial Consortium)](https://www.ogc.org/standards) publie différents standards d’interopérabilité, dont *SWE* "Sensor Web Enablement",  qui permet de présenter des données de capteurs de manière standardisée et interopérable.
François André a présenté ces protocoles standards de l'OGC lors du Séminaire du réseau SIST2015 à l'OSU Pytheas de Marseille 

Le protocole "SOS" ("_Sensor observation service_") de l'OGC permet de présenter de manière standardisée les données issues de capteurs de terrain de manière interopérable. Ce standard définit une interface de service Web qui permet d'interroger les observations, les métadonnées des capteurs, ainsi que les représentations des caractéristiques observées. En outre, cette norme définit les moyens d'enregistrer de nouveaux capteurs et de supprimer les capteurs existants. Elle définit également les opérations permettant d'insérer de nouvelles observations de capteurs. 
 
* [Sensor Web Enablement Standards & Technology](<https://nuage.osupytheas.fr/s/iMx5S9orQ9zyoxk>)   
Christoph Stasch, Simon Jirka   
Séminaire SIST15, Marseille

Actuellement on trouve deux implémentations intéressantes du protocole SOS dans la gestion des données de capteurs environnementaux. Il sagit de 
- *52North* : logiciel de la société éponyme (<https://52north.org/software/software-projects/sos/>) est une application qui fournit une interface web interopérable pour l'insertion et l'interrogation des données et
des descriptions des capteurs. Il regroupe les observations provenant de capteurs in-situ en direct ainsi que des ensembles de données historiques (données de séries chronologiques).

- *istSOS* : [istSOS](http://www.istsos.org/) est une implémentation de serveur OGC SOS écrite en Python. istSOS permet de gérer et d'envoyer des observations provenant de capteurs de surveillance selon la norme Sensor Observation Service. Le projet fournit également une interface utilisateur graphique qui permet de faciliter les opérations quotidiennes et une api RESTFull Web pour automatiser les procédures d'administration. 

istSOS est publié sous licence GPL et fonctionne sur toutes les principales plates-formes (Windows, Linux, Mac OS X), même s'il n'a été utilisé en production que dans l'environnement Linux.
* [Présentation du logiciel *istSOS*](https://nuage.osupytheas.fr/index.php/s/ROh4LCpHZCWdlHz?path=%2FVideos-presentations-2409-apresmidi)   
Massimiliano Canata   
Séminaire SIST15, Marseille 

Ces 2 logiciels ont été présentés par leurs auteurs lors du séminaire du réseau [SIST](http://sist.cnrs.fr) en 2015 à Marseille ! <https://nuage.osupytheas.fr/index.php/s/ROh4LCpHZCWdlHz>

Le [réseau SIST](http://sist.cnrs.fr) a organisé deux actions de formation nationale (ANF) sur ces logiciels mettant en oeuvre les standards d'interopérabilité WMS, CSW, SOS : <https://sist.cnrs.fr/les-formations>. Cette formation intitulée "Gestion des données d'observation : les outils informatiques pour la valorisation" permet aux personnels d’améliorer la gestion et la diffusion de leurs données scientifiques d'observation en apprenant à installer, configurer et utiliser différents outils choisis pour leur aptitude à répondre de manière standardisée (standards de l'Open Geospatial Consortium (OGC), ISO19139, Inspire, etc) à ces problématiques.

La formation et la documentation associée permettent de :
- savoir installer, configurer et utiliser le logiciel "52°North" pour diffuser et visualiser des données de capteurs selon le standard SOS de l'OGC.
- mettre en place un serveur cartographique "Geoserver" pour afficher et permettre les échanges de données géospatiales sur le web selon les standards (WMS, WFS, ...) de l'OGC ;
- cataloguer les jeux de données avec leurs métadonnées avec le logiciel "GeoNetwork" selon la norme ISO 19139 et la directive Inspire;     
- mettre en place une Infrastructure de Données Géographiques (IDG)  avec l'application "geoCMS" permettant la visualisation de données géospatiales sur le web
- mettre en exploitation une plateforme web de gestion et de diffusion des données avec des logiciels comme Thredds et [Erddap](https://coastwatch.pfeg.noaa.gov/erddap/index.html)


Regis Hocdé nous présente le Réseau de capteurs mis en place dans le cadre du réseau d'observation ReefTEMPS :  
* [Réseau d'observation du Pacifique Sud ‘ReefTEMPS' : évolutions fonctionnelles et optimisation d'un système d'information dédié capteurs et reconstitution de séries temporelles](<https://sist16.sciencesconf.org/data/pages/12_R_Hocde.pdf>)   
Régis Hocdé, Institut de Recherche pour le Développement    
Séminaire SIST16, Montpellier    


* [De la définition au déploiement de standards d'interopérabilité : retour d'expérience de la Direction des Systèmes d'Information (DSI) du BRGM](<https://sist15.sciencesconf.org/program>)   
Grellet Sylvain, BRGM - Stéphane Loigerot, BRGM   
Séminaire SIST15, Marseille


#### Web scrapping, collecte automatique et analyse de données 
"Le web scraping (parfois appelé harvesting) est une technique d'extraction du contenu de sites Web, via un script ou un programme, dans le but de le transformer pour permettre son utilisation dans un autre contexte" (https://fr.wikipedia.org/wiki/Web_scraping)

Depuis l’explosion quantitative des données numériques, il est devenu extrêmement intéressant d’apprendre à recueillir, comprendre et exploiter les informations issues du web. On constate ces dernières années, dans le domaine des sciences sociales, l'intérêt croissant des chercheurs ou ingénieurs pour l'utilisation de nouvelles techniques de collecte et de traitement automatisé des données et en particulier des big data. 

Pour collecter ces données, il est d’usage de recourir à des technique informatiques : programmation simplifiée ([Web-Harvest](http://web-harvest.sourceforge.net/), [scrapy](https://scrapy.org/)) langages de programmation pour la collecte et l’analyse ([R](https://www.r-project.org/), [Python](https://www.python.org/)), logiciels d’analyse statistiques ([SAS](https://www.python.org/)), [SPAD](https://ia-data-analytics.fr/logiciel-data-mining/) , [SPSS](https://www.ibm.com/fr-fr/analytics/spss-statistics-software)), aspirateurs de site ([HTTrack](https://www.httrack.com/), [Web Crawler](https://raptor-dmt.com/seo-tools/web-crawler/?gclid=EAIaIQobChMIlK7j9e3v6gIV1_hRCh3MwwSEEAAYASAAEgKkxPD_BwE)) logiciels de mise en forme de données ([Open Refine](https://openrefine.org/)) etc. 

Chaque utilisateur en fonction de son profil et de ses compétences peut choisir une technologie partant de simples outils comme les aspirateurs de site qui permettent de réaliser des opérations basiques de scraping jusqu’à l’utilisation de langages plus performants comme R ou Python pour des utilisateurs plus avancés.

Au-delà des fonctionnalités de web scraping, la présentation « [Analyse de données avec R](https://hpecout.gitpages.huma-num.fr/R_presentation_FR/#/) » proposée par Hugues Pécout (CNRS) donne un exemple de l’analyse de donnée avec le logiciel R. En plus d’une présentation du logiciel R et de RStudio, elle contextualise R dans le paysage de l’analyse de donnée en le comparant à des logiciels propriétaires existants sur le marché ainsi qu’au langage python.

Ces outils sont depuis quelques années en plein essor car ils permettent d’automatiser la constitution des bases de données, de collecter des sommes de données importantes, inaccessibles il y a quelques années comme les données de réseaux sociaux, de compiler des données pour créer ses propres indicateurs (impossible avec des techniques de collecte classiques) ou encore de nettoyer, structurer des données déjà existantes…     
Ces modes de collecte automatisés renvoient aussi aux notions de Crawling, Scrapping, data harvesting …. 

Dans la pratique, des questions juridiques peuvent se poser au regard de l’exploitation des données récoltées en masses par ces moyens qui sont susceptibles d’être des données personnelles ou protégées par la propriété intellectuelle.


### Les cahiers de laboratoire

L’ensemble des données produites par la recherche doit être répertorié et enregistré dans l’objectif d’une réutilisation potentielle. Nous disposons pour ce faire d’un certain nombre de supports comme les cahiers de laboratoire. Le cahier de laboratoire est un outil non obligatoire mais fortement recommandé pour toute structure générant des données donnant lieu à des connaissances diffusables et valorisables. Il constitue un véritable outil scientifique et ce, dès le commencement d’un projet. Les cahiers de laboratoire répondent également aux obligations légales et contractuelles, en apportant la preuve de l’invention et de ses inventeurs. 

[Cahier de laboratoire et gestion des données de la recherche](<http://renatis.cnrs.fr/IMG/pdf/DIALOGIST_9_2020_Rivet.pdf>)   
Alain Rivet, CNRS-CERMAV  
Atelier DAILOGU’IST « Rendre FAIR les données, mais quelles données préserver ? », 9 juillet 2020   

Les plaquettes ["Le cahier de laboratoire national : Pourquoi l’utiliser ?" et "Le cahier de laboratoire national : 
Comment l’utiliser ?"](<https://www.curie.asso.fr/-Cahier-de-laboratoire-national-.html>) présentent des recommandations sur la bonne gestion de ce dernier. 

 Les apports du numérique sont multiples en améliorant la traçabilité des recherches, la lutte contre la fraude et la gestion des données. Les cahiers de laboratoire électroniques présentent plusieurs avantages par rapport à leur version papier : 
-	le partage de l’information avec un rattachement des données brutes ; 
-	une recherche d’informations facilitée ;
-	une datation assurée des expériences par l’horodatage.

Le site [datacc.org](datacc.org) consacre la mise en œuvre d’un service d’accompagnement sur la gestion des données en physique et en chimie, dans le cadre d’un projet CollEx-Persée. Le site fournit des contenus nourris sur les cahiers de laboratoire électroniques (<https://www.datacc.org/bonnes-pratiques/utiliser-un-cahier-de-laboratoire-numerique/>), issus d’une expérimentation menée avec des chimistes de Lyon 1 et de Grenoble.

Diverses expérimentations ont également été réalisées :

* [Les cahiers de laboratoire électroniques, : atelier elabFTW](<http://www.quares.fr/index.php/evenements/ecoles/73-presentations-eqrs2020>  
Alain Rivet, Henri Valeins, CNRS    
Ecole QUARES, Montpellier, 2020

* [Utilisation du cahier de laboratoire électronique BIOVIA au sein de l'Institut de Biologie Structurale](<https://qer-2017.sciencesconf.org/data/program/2017_ANF_tracabilite_laguri.pdf>)   
Cédric Laguri, IBS   
ANF "Traçabilité des activités de recherche et gestion des connaissances", Réseau Qualité en Recherche, Grenoble, 16-18 octobre 2017   

L'INSERM s'est intéressé à la version numérique des cahiers de laboratoires, comme une réplique du cahier papier. 
Ils pensent que si la version électronique reste une solution d’enregistrement au quotidien des expériences scientifiques, c’est désormais devenu un outil différent, fortement axé sur la qualité, la gestion de la connaissance, la gestion de projets et le travail collaboratif. Paul Guy Dupré et ses collaborateurs présentent les cahiers de laboratoires qui ont été mis en place à l'INSERM.

* [Expérimentation du cahier de laboratoire électronique à l’Inserm](<https://qer-2017.sciencesconf.org/data/program/2017_ANF_tracabilite_dupre.pdf>  
Paul-Guy Dupré, INSERM    
ANF "Traçabilité des activités de recherche et gestion des connaissances", Réseau Qualité en Recherche, Grenoble, 16-18 octobre 2017 

* [Expérimentation du cahier de laboratoire électronique à l’Inserm : les apports de l’électronique au cahier de laboratoire](https://conf-ng.jres.org/2017/document_revision_2866.html?download)   
Paul Guy Dupré, Fanny Brizzi Inserm, DSI   
[JRES2017  ](https://2017.jres.org/fr/contenu-th%C3%A9matique)    

* [Déploiement du cahier de laboratoire  électronique à l’INSERM et nouvelles perspectives]( https://conf-ng.jres.org/2019/document_revision_5213.html?download)   
Paul Guy Dupré Inserm & Claudia Gallina-Muller - Inserm DSI   
[JRES2019  ](https://www.jres.org/fr/programme/)   

* [Expérimentation du cahier de laboratoire électronique à l’Inserm](<https://qer-2017.sciencesconf.org/data/program/2017_ANF_tracabilite_dupre.pdf>  
Paul-Guy Dupré, INSERM    
ANF "Traçabilité des activités de recherche et gestion des connaissances", Réseau Qualité en Recherche, Grenoble, 16-18 octobre 2017 

### Tablettes et carnets de terrain

Les données et documents produits directement sur le terrain témoignent de l’activité de recherche dans diverses disciplines, notamment en sciences humaines et sociales, en sciences de la terre... Il s’agit aussi bien de carnets issus d’entretiens de sociologues, d’ethnologues, de carnets de prélèvements en géochimie, géologie, de carnets de fouilles en archéologie, de notes, de photographies prises sur le terrain… 
 
Par ailleurs, certaines données peuvent se révéler d'une valeur inestimable, qu'il s'agisse de données fortement temporelles (images satellites de la banquise, données sur les glaciers alpins) ou de données provenant de sites aujourd'hui endommagés ou détruits (Notre Dame de Paris, cité antique de Palmyre...). Il est de ce fait essentiel que ces données soient répertoriées et archivées. 

L’utilisation de carnets de terrain électroniques que sont les tablettes permet de profiter des avantages d'appareils nomades pour faciliter la saisie des observations que l’on fait sur le terrain, en milieu naturel. L'utilisation de cet outil "nomade" va permettre :

- d'améliorer la qualité des données collectées
- de pouvoir utiliser les données plus rapidement
- de réduire le coût (temps de ressaisie)

Au niveau logiciel, diverses stratégies sont possibles :

- utiliser une application nomade existante
- utiliser une application web existante
- développer une application nomade spécifique avec un langage de programmation
- développer une application nomade en utilisant une boite à outils de génération de carnets de terrain
- développer une application nomade en adaptant des logiciels existants (par exemple QGIS, Lizmap)

Ce sujet a donné lieu à de nombreuses actions du réseau rBDD où des solutions logicielles différentes ont été présentées avec leur avantages et inconvénients :
* [Atelier « Carnets de terrain électroniques »](http://rbdd.cnrs.fr/spip.php?article270)   
Réseau rBDD   
Montpellier, 2018

* [Séminaire « Système d’information embarqué, cahier/carnet de terrain et de laboratoire électronique : quelles interactions avec les bases de données ? »](http://rbdd.cnrs.fr/spip.php?article206)  
Réseau rBDD   
Paris, 2016

* [Séminaire « les technologies mobiles : retours d'expériences et prospectives »](https://indico.mathrice.fr/event/27/other-view?view=standard)   
Réseau rBDD   
Paris, 2016

Ces nouvelles technologies très « ludiques » et « faciles » d’utilisation, nécessitent une réflexion importante pour définir de façon précise son besoin afin de ne pas être pénalisé sur le terrain. Elles nécessitent aussi une adaptation technologique pour permettre un stockage efficient et pérenne en bases de données. Ces aspects ont tété abordés à l'occasion de l'ANF « Interfacer les outils mobiles avec son système d’information » (http://rbdd.cnrs.fr/spip.php?article317).

Deux solutions, étudiées au CEFE, ont été présentées à l'occasion de cette ANF : le développement d'une application nomade basée sur QGIS ainsi q'une application nomade utilisant ODK. C'est d'ailleurs la solution basée sur ODK qui a été utilisé lors de l'ANF « Interfacer les outils mobiles avec son système d’information » cité précédemment car la solution ODK permet de couvrir les étapes allant de la création du formulaire à la sécurisation en bases de données.

Lors de cette ANF, deux interventions concernent plus particulièrement la collecte de données tels que la définition de ses besoins pour les outils nomades par C. Plumejeaud, S.Ladet ou une présentation de Pierre Yves Arnould sur XLSForm qui est un standard de formulaire basé Excel. Cette solution est d'ailleurs utilisée dans KoboToolbox, qui est en quelque sorte un ODK "packagé".

* AKOUETTE, Ata Franck, 2018. Collecte de données terrain avec un smartphone : Prise en main de Kobotolbox et de Kobocollect. *FOSS4G-fr 2018*, Marne-la-vallée. 2018. <https://github.com/OSGeo-fr/FOSS4G-fr-2018/blob/master/ateliers/Atelier-kobo.pdf>

* [Carnet de terrain électronique, Retour d’expérience sur la création d’une boite à outils](http://video.rmll.info/videos/carnet-de-terrain-electronique/)
Marie-Claude Quidoz, Centre d’Écologie Fonctionnelle et Évolutive
15èmes Rencontres Mondiales du Logiciel Libre, juillet 2014 
Ce retour d'expérience détaille l'étude et le développement d'un ensemble d'outils interopérables avec le système d'information du laboratoire CEFE et qui permet aux intervenants sur le terrain de collecter les données.

## Environnements de stockage - Sauvegarder les données

Dés la phase de collecte, il convient de se préoccuper des aspects de stockage et de sauvegarde qui seront plus largement abordés dans la phase 6. du cycle de vie des données. En effet, dès le début d'un projet, il va être nécessaire, d'une part, d'estimer le stockage nécessaire à la collecte de données et d'autre part, de mettre en place les moyens de sauvegarde des données récoltées. A cet effet, il conviendra de travailler en amont avec une équipe informatique.

* [Rappels théoriques concernant les architectures de stockage traditionnel](https://indico.mathrice.fr/event/5/contribution/2/material/slides/0.pdf)  
Sylvain Maurin  
ANF  2016 : Stockage Distribué 

* [Outils algorithmiques et logiciels pour le stockage distribué](https://indico.mathrice.fr/event/5/contribution/3/material/slides/0.pdf)  
Benoit Parrein  
ANF Des données au BigData : exploitez le stockage distribué ! Savoir anticiper les nouveaux outils, les technologies émergentes en matière (ANF 2016) 
 
<table border="1" cellspacing="1" cellpadding="1" width="100%" align="center"><colgroup span="4" > </colgroup><thead><tr>
 <th>Point de vigilance : Respecter le RGPD !  </th>
 </tr></thead><tbody><tr><td>Dès lors que l’on va collecter des données personnelles (données permettant l’identification directe ou indirecte d’une personne), il sera important de respecter des principes essentiels sur la durée de conservation des données,le droit à l’information et l’obligation de sécuriser les données. Il ne faut pas hésiter à se rapprocher du correspondant du Délégué à la protection des données (DPD) de votre délégation (pour le CNRS) ou du Délégué à la protection des données de votre établissement.
</td></td></tr>
 </tr></tbody></table>
