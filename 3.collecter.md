(collecter)=
# Collecter

Cette phase du cycle de vie de la donnée concerne les aspects d'acquisition et de collecte des données ainsi que la constitution des jeux de données ("dataset" en anglais) avec leurs métadonnées descriptives. Il s'agit donc, dans cette phase, de travailler sur les processus d'acquisition des données qui peuvent être obtenues au moyen de divers médias selon le domaine étudié : capteurs environnementaux, instruments, sondages, modèles numériques... Une fois les données acquises, il est nécessaire et indispensable dans l'objectif de les rendre "FAIR", de les décrire avec leurs métadonnées associées.  

La description  de ces jeux de données nécessite d'utiliser, autant que faire se peut, des référentiels de vocabulaires contrôlés (thésaurus) si possible standardisés et les plus appropriés au domaine étudié. Il est conseillé de gérer les jeux de données dans un environnement technique qui permette d’assurer la sauvegarde, l'archivage, le "versionning",  l’accessibilité et l'interopérabilité des données. Cette gestion se fait via des infrastructures techniques, des bases ou des supports qui doivent être fiables et bien documentés, et ce dans le respect des règles de traitement spécifiques des données personnelles.

Cette phase "Collecter" va nécessiter :

   * de disposer des données et des *métadonnées* précises pour la description des données brutes elles-mêmes (libellés des paramètres, unités de mesure) ainsi que des dispositifs d'acquisition (capteurs de mesures, modèles numériques...);
   * de mettre en place des chaines de collecte : du capteur jusqu'aux espaces disques et aux applications sur des serveurs où les traitements pourront être réalisés, avec la documentation adaptée;
   * de mettre en place une gestion et conduite de projets pour faire travailler ensemble les différents acteurs intervenant dans la chaîne de collecte : électroniciens, informaticiens, chercheurs...;
   * de disposer de *cahier de laboratoire, tablettes de terrain* ou supports divers pour consigner les relevés et métadonnées observées; 
   * d'utiliser des protocoles si possibles normalisés ou standardisés pour présenter les données brutes et les dispositifs d'acquisition (capteurs...) et les rendre interopérables;
   * de définir le stockage nécessaire à la collecte de données : travailler en amont avec une équipe informatique en mode projet (gestion de projet).


## Utiliser des Normes et Standards d'interopérabilité

L'AFUL donne une définition de [l’interopérabilité](https://aful.org/gdt/interop) qui est "la capacité que possède un produit ou un système, dont les interfaces sont intégralement connues, à fonctionner avec d'autres produits ou systèmes existants ou futurs, et ce sans restriction d'accès ou de mise en œuvre" . Développer l'interopérabilité consiste donc à mettre en place et utiliser des normes et des standards qui fixent des règles permettant d'assurer le bon fonctionnement de deux systèmes informatiques.

Appliquée aux données, l'interopérabilité permet de rendre les données accessibles et réutilisables. Pour parvenir à cela, il est nécessaire de se préoccuper d'utiliser des protocoles d'accès et des formats des données "ouverts", normés ou standardisés, d'une part, au niveau des formats de fichiers et d'autre part, dans les outils informatiques qui serviront à échanger, diffuser et lire les données.


### Les standards de métadonnées 

Dans l'optique d'une gestion "FAIR" des données, il est nécessaire, dans la mesure du possible, de suivre des normes et des standards pour la description des métadonnées, les formats de fichiers et les protocoles d'échange de données.

Catherine Morel-Pair propose une présentation riche et complète sur les formats et métadonnées qu’elle détaille de manière très approfondie et restitue dans le cadre de leur utilisation pour la gestion de contenu et la documentation des données. Elle aborde en introduction les notions de données de la recherche, de Fair Data, d’interopérabilité et de Data Management Plan. 
- La première partie de sa présentation porte sur les fichiers de données (organisation et nommage, format et critères d’interopérabilité-pérennité) 
- la deuxième partie est dédiée aux métadonnées et à la documentation  (définitions, présentation des standards, des identifiants pérennes pour les données et syntaxes d’échange). Elle termine par un focus sur les sites de dépôt, de portails ou d’entrepôts de données et leur schéma de métadonnées associées.
:::{admonition}Réference
[Participer à l’organisation du management des données de la recherche : gestion de contenu et documentation des données](https://anfdonnees2016.sciencesconf.org/data/pages/2016_07_07_ANF_Renatis_Formats_Standards_et_Metadonnees_1.pdf)
[vidéo](https://youtu.be/obGDFrXyBiU?t=3)  
Catherine Morel-Pair , INIST, CNRS  
ANF "Participer à l'organisation du management des données de la recherche : gestion de contenu et documentation des données" -2016 Paris   
:::

**Les métadonnées dans un DMP**

Cette présentation de Marie Puren a été conçue pour animer un atelier de formation qui avait pour objectif de définir et comprendre l’importance des métadonnées dans le cadre de la rédaction d’un DMP. Elle définit, en donnant des exemples, ce qu’est une métadonnée, à quoi elle sert, quelle information elle donne. Elle distingue et détaille la spécificité des métadonnées de description, des métadonnées de gestion et des métadonnées de préservation. Elle aborde ensuite le chapitre du cycle de vie des métadonnées (créer, entretenir, mettre à jour, stocker, gérer la suppression des données, publier). Elle spécifie les métadonnées à faire figurer dans un DMP, explique comment les collecter et propose quelques outils d’extraction automatique de métadonnées. Autour de la notion de métadonnée, elle précise l’importance de définir des responsabilités en s’appuyant sur les chercheurs, documentalistes, bibliothécaires et informaticiens. Elle complète sa présentation avec une description des principaux standards interdisciplinaires et disciplinaires de métadonnées. Elle explique où et comment choisir ces standards. Elle explique également l’intérêt d’associer des ontologies ou vocabulaires contrôlés. Les dernières recommandations de sa présentation portent sur la gestion des métadonnées à long terme, l’importance d’évaluer leur qualité et revient sur la notion d’ouverture des métadonnées et la nécessité de choisir des licences pour nos métadonnées.
:::{admonition}Réference
[Les métadonnées dans un DMP](https://anfdonnees2017.sciencesconf.org/data/pages/20170706_dmp_metadonnees_puren_1.pdf)  
Marie Puren, INRIA  
ANF "Participer à l'organisation du management des données de la recherche : gestion de contenu et documentation des données", réseau Renatis, Paris, 2017   
:::

### Référentiels de métadonnées 

Les référentiels de métadonnées peuvent être des standards ou des normes; ce sont des documents importants qui se chargent de définir les *informations nécessaires pour décrire les données elles-mêmes*. Ils sont de ce fait utilisés pour donner toutes les informations nécessaires à la compréhension et à l'utilisation des données et ainsi faciliter leur réutilisabilité. Il est donc fortement recommandé de décrire ses données avec des normes ou des standards reconnus dans les disciplines concernées. Le choix d'un standard de métadonnées va dépendre du type de ressource, du domaine d'application, mais également de la communauté à laquelle on s'adresse. 

À cet effet, le site du [Digital Curation Centre](https://www.dcc.ac.uk/guidance/standards/metadata) recense les standards de métadonnées par grandes disciplines (biologie, physique, sciences sociales, sciences de la terre...). Des outils informatiques, permettant de passer d'un standard à un autre, sont également disponibles.

On trouve plusieurs standards et normes qui permettent de définir un ensemble de métadonnées sur des jeux de données. Parmi les standards les plus connus ou utilisés, citons :
- **Le Dublin Core**, est un standard généraliste issu d'un consensus international et multidisciplinaire a pour objectif de fournir un socle commun d'éléments descriptifs suffisamment structuré pour permettre une interopérabilité minimale entre des systèmes conçus indépendamment les uns des autres. Le Dublin Core <https://fr.wikipedia.org/wiki/Dublin_Core> est un vocabulaire du web sémantique utilisé pour exprimer les données dans un modèle RDFa. 

Le Dublin Core définit un ensemble d'items de métadonnées obligatoires pour décrire les données <https://fr.wikipedia.org/wiki/Dublin_Core#Dublin_Core_element_set> :

1. Titre (métadonnée) 	Title 	Nom donné à la ressource
2. Créateur (métadonnée) 	Creator 	Nom de la personne, de l'organisation ou du service responsables de la création du contenu de la ressource
3. Sujet (métadonnée) ou mots clés 	Subject 	Thème du contenu de la ressource (mots clés, expressions, codes de classification)
4. Description (métadonnée) 	Description 	Présentation du contenu de la ressource (résumé, table des matières, représentation graphique du contenu, texte libre)
5. Éditeur 	Publisher 	Nom de la personne, de l'organisation ou du service responsables de la mise à disposition ou de la diffusion de la ressource
6. Contributeur 	Contributor 	Nom de la personne, de l'organisation ou du service responsables de contributions au contenu de la ressource
7. Date (métadonnée) 	Date 	Date de création ou de mise à disposition de la ressource
8. Type 	Type 	Nature ou genre de la ressource (catégories, fonctions, genres généraux, niveaux d'agrégation du contenu)
9. Format 	Format 	Manifestation physique ou numérique de la ressource
10. Identifiant de la ressource 	Identifier 	Référence univoque à la ressource dans un contexte donné (URI, ISBN)
11. Source 	Source 	Référence à une ressource dont la ressource décrite est dérivée (URI)
12. Langue (métadonnée) 	Language 	Langue du contenu intellectuel de la ressource
13. Relation (métadonnée) 	Relation 	Référence à une ressource apparentée
14. Couverture (métadonnée) 	Coverage 	Couverture spatio-temporelle de la ressource (domaine d'application)
15. Gestion de droits (métadonnée) 	Rights 	Informations sur les droits associés à la ressource (IPR, copyright, etc.)

Pour la description des jeux de données géolocalisées, les [normes ISO 19115 et ISO 19139](https://pro.arcgis.com/fr/pro-app/help/metadata/create-iso-19115-and-iso-19139-metadata.htm) sont des normes de référence pour l'information géographique dans le domaine des métadonnées. Elles sont notamment utilisées dans les disciplines nécessitant de représenter des données géospatialisées.

- **ISO19115** fournit une structure permettant de décrire et de découvrir des données géospatiales, y compris le moment et l'endroit de leur localisation, une vue d'ensemble de leur contenu, de leurs propriétés, de leur qualité, de leur utilisation adéquate, du mécanisme de distribution, des points de contact pour les demandes d'informations, etc.

La norme ISO 19139 est l'implémentation XML de la norme ISO 19115. Elle définit le codage XML des métadonnées géographiques, une implémentation de schéma XML dérivée de la norme ISO 19115. La norme ISO 19139 est le modèle principal utilisé pour décrire des données dans le logiciel GeoNetwork et constituer ainsi un catalogue de données géospatialisées que l'on abordera dans le chapitre 7 {ref}`publier` de ce guide.


### Protocoles standards en information Géographique 

L'échange de données d'une plateforme à l'autre se fait au travers de protocoles informatiques. De ce fait qui on veut que les systèmes soit interopérables entre eux, il est nécessaire d'utiliser des protocoles ouverts et standards pour permettre l'interopérabilité.

Dans le domaine environnemental, pour des données qui sont souvent *géolocalisées* par des coordonnées Latitude/Longitude, [l'Open Geospatial Consortium (OGC)](https://www.ogc.org/standards/), est un consortium international qui a pour objectif de développer et promouvoir des standards ouverts, les spécifications OpenGIS, afin de garantir l'interopérabilité des contenus, des services et des échanges dans les domaines de la géomatique et de l'information géographique. 

Les standards OGC sont importants à connaitre  dans la mesure où ils définissent les protocoles et formats à suivre pour être interopérables. Ils ont été présentés par François André dans les réseaux DEVLOG, et dans le réseau SIST de l'INSU. Pour ce dernier réseau, l'interopérabilité dans la gestion des données des Observatoires de l'INSU est un enjeu important.

:::{admonition}Réference
[Les Normes OGC (Open Geospatial Consortium)](https://sist15.sciencesconf.org/data/program/ogc.pdf)   
François André, Aeris   
Séminaire SIST15 - OSU Pytheas Marseille
:::

Parmi les standards de l'OGC les plus utilisés dans nos réseaux métiers chez les gestionnaires de données environnementales, on peut citer :
-    **[CS-W](https://www.ogc.org/standards/cat)** - Catalog Service for the Web : ce protocole est destiné à diffuser des métadonnées ISO19139, et permettre l'interrogation de catalogues de métadonnées. Une très bonne implémentation de ce protocole est réalisée dans le logiciel ["Geonetwork"](<https://geonetwork-opensource.org/>) utilisé pour constituer des catalogues et des inventaires de jeux de données et les présenter sur le Web de manière interopérable, et que l'on détaillera dans le chapitre 7 {ref}`publier` dédié à la publication des jeux de données. 
Grâce à ce protocole, on peut constituer des réseaux de catalogues tels qu'ils sont demandés par la [Directive Européenne Inspire](http://cnig.gouv.fr/?page_id=8991).

-  **[WMS](https://www.ogc.org/standards/wms)** - *Web Map Service* est un protocole de communication standard qui permet de constituer des cartes de données géoréférencées à partir de différents serveurs de données cartographiques. <https://fr.wikipedia.org/wiki/Web_Map_Service>. 
   
Le réseau SIST a organisé deux actions de formation nationale (ANF) sur ces logiciels mettant en oeuvre les standards d'interopérabilité WMS, CSW, SOS et qui permettent aux personnels d’améliorer la gestion et la diffusion de leurs données scientifiques d'observation en apprenant à installer, configurer et utiliser différents outils logiciels, choisis pour leur aptitude à répondre de manière standardisée à ces problématiques.

:::{admonition}Réference
["Gestion des données d'observation : les outils informatiques pour la valorisation" ](https://sist.cnrs.fr/les-formations/supports-des-anf-gestion-de-donnees-dobservation/supports-des-anf-gestion-de-donnees-dobservation-les-outils-informatiques-pour-la-valorisation)   
ANF SIST17 Fréjus - ANF SIST18 Autrans   
:::

Un bon nombre d'Instituts et d'auteurs, gestionnaires de données suivent d'ailleurs ces standards OGC : 

Sylvain Grelet communique par exemple le retour d'expérience sur l'utilisation et le déploiement des standards d'interopérabilité au BRGM :
:::{admonition}Réference
[De la définition au déploiement de standards d'interopérabilité :](https://nuage.osupytheas.fr/index.php/s/ROh4LCpHZCWdlHz#pdfviewer) retour d'expérience de la Direction des Systèmes d'Information (DSI) du BRGM   
Grellet Sylvain, Stéphane Loigerot, BRGM   
[Séminaire SIST15, Marseille](http://sist15.sciencesconf.org)  
:::

Véronique Chaffard nous présente la mise en oeuvre des standards de l'OGC dans le projet AMMA-CATCH :
:::{admonition}Réference 
[Portail Web d'accès aux données de l'observatoire AMMA-CATCH et mise en oeuvre des standards d'échange des données OGC ](<https://nuage.osupytheas.fr/index.php/s/ROh4LCpHZCWdlHz>)   
Véronique CHAFFARD Institut de Recherche pour le Développement, Laboratoire d'étude des transferts en hydrologie et environnement  
[Séminaire SIST15, Marseille](http://sist15.sciencesconf.org)
:::

### Les systèmes d'acquisition : maîtriser l’acquisition et la collecte des données

Il est important que le processus de collecte des données soit clairement défini et validé. Par exemple, il conviendra de s'assurer que les systèmes d'acquisition sont bien étalonnés. Par ailleurs, l'ensemble des données produites doit être parfaitement répertorié et enregistré. Nous disposons pour ce faire d’un certain nombre de supports tels que les cahiers de laboratoires, les carnets de terrain... 

#### Métrologie des équipements

Par nature, la recherche n’est pas un processus répétitif, elle est pleine d'aléas et d'incertitudes contrairement à un processus industriel. 
La confiance dans la qualité d’une recherche consiste à établir et vérifier que les différentes étapes d’une étude peuvent être répétées en obtenant le même résultat par des chercheurs différents à des moments différents.Il est donc essentiel de s’assurer que l’ensemble des activités soit tracées et maitrisées, cela est le cas de toute la chaine fonctionnelle d’une analyse (des pipettes, balances jusqu’aux équipements d’analyse) et que la traçabilité des activités de recherche soit ainsi assurée. 

:::{admonition}Réference
[Confirmation métrologique des équipements](<https://qualsimp.sciencesconf.org/data/program/9_Trac_abilite_des_donne_es_de_la_recherche_Virginie_JAN_LOGASSI.pdf>)    
Virginie JAN LOGASSI, DAPEQ LUE   
ANF Outils qualité, réseau QeR, 2019       
:::

De nombreux laboratoires et plateformes de tests du CNRS sont équipés de salles propres, dans des domaines variés tels que la micro et nanotechnologie, la géochimie, l’optique, la médecine, le spatial…
En débutant par un point sur l’état de l’art (définition, réglementation, documentation…) de ces 2 aspects, l'objectif principal de la journée thématique est de faire bénéficier de retours d’expériences riches sur les bonnes pratiques déjà éprouvées
et sur les écueils à éviter et de répondre entre autres aux questions : 
- Quand a-t-on besoin de travailler en salles propres ? 
- Quelles règlementations régissent l'installation, la maintenance et le contrôle des salles propres ? 
- Comment préparer l'installation dans nos locaux ? A quoi doit-on penser ? 
- Quelles sont les solutions techniques les mieux adaptées à notre besoin ? 
- Quels sont les critères de surveillance et systèmes de contrôle des installations ? 
- Comment doit-on travailler en salles propres ? Quelles sont les bonnes pratiques de gestion d'une salle propre ?

:::{admonition}Réference
[Les salles propres de l’installation à l’utilisation, de la théorie à la pratique - Usages et retours d’expériences](<https://sallespropres17.sciencesconf.org/program>)   
Journée thématique, réseau QeR, 2017   
:::


#### Les capteurs

Diverses communautés scientifiques sont intéressées par les problématiques inhérentes aux systèmes d'acquisitions et aux instruments associés. Différents aspects de collecte de données existent qu'ils proviennent d'un équipement, d'un capteur automatisé, d'un modèle numérique ou qu'ils soient obtenus par un personnel de terrain, par une enquête, au moyen d'interfaces... Dès lors, il convient d'élaborer des méthodologies de collecte, de se documenter sur les choix des référentiels de métadonnées et des thésaurus de vocabulaire, mais également de développer les procédures d'intégration des données dans les bases.

Dans le milieu "Ocean-Atmosphere" cette problématique occupe une place importante, à tel point que, depuis plusieurs décennies, METEO-FRANCE et l'INSU depuis 1966, l'IFREMER depuis 2002, l'IRD et le CNES depuis 2004, le SHOM depuis 2005, organisent un atelier dédié aux rencontres portant sur l'expériementation et l'instrumentation. Cet Atelier Expérimentation et Instrumentation (AEI)
[http://www.aei-ocean-atmosphere.org/](http://www.aei-ocean-atmosphere.org/) permet de réunir la communauté scientifique spécialisée dans la recherche instrumentale et de traiter divers thèmes d'actualité <http://www.aei-ocean-atmosphere.org/Editions-Precedentes>
L'AEI traite de manière privilégiée les aspects de mesure et de méthodologie, sans exclure pour autant l'exploitation scientifique des résultats. Il a lieu alternativement à Paris, Toulon, Lille et Brest, généralement en début d'année. L'AEI permet aux équipes de recherche d'exposer leurs résultats dans un colloque francophone. C'est un lieu de rencontre pour les participants, issus des différents organismes et groupes industriels, afin de favoriser les synergies et coopérations. 

Les gestionnaires de données environnementales mettent en place des chaînes de collecte de données provenant de capteurs de terrains, ou de modèles numériques. Ils se préoccupent de l’utilisation de normes interopérables dans les protocoles d'échange et dans les formats de données.

Pour la gestion des capteurs, l'[OGC (Open Geospatial Consortium)](https://www.ogc.org/standards) que nous avons vu plus haut, publie un standard d’interopérabilité, *SWE* "Sensor Web Enablement",  qui permet de présenter des données de capteurs de manière standardisée et interopérable. Ce protocole et les logiciels qui les implémentent sont bien adaptés à la description des capteurs et à la gestion des séries temporelles.

Le protocole "SOS" ("_Sensor observation service_") de l'OGC permet de présenter de manière standardisée les données issues de capteurs de terrain de manière interopérable. Ce standard définit une interface de service Web qui permet d'interroger les observations, les métadonnées des capteurs, ainsi que les représentations des caractéristiques observées. En outre, cette norme définit les moyens d'enregistrer de nouveaux capteurs et de supprimer les capteurs existants. Elle définit également les opérations permettant d'insérer de nouvelles observations de capteurs. 

:::{admonition}Réference
[Sensor Web Enablement Standards & Technology](<https://nuage.osupytheas.fr/s/iMx5S9orQ9zyoxk>)   
Christoph Stasch, Simon Jirka   
[Séminaire SIST15, Marseille](http://sist15.sciencesconf.org)
:::

Actuellement on trouve deux implémentations intéressantes du protocole SOS dans la gestion des données de capteurs environnementaux. Il s'agit de 
- *52North* : logiciel de la société éponyme (<https://52north.org/software/software-projects/sos/>) est une application qui fournit une interface web interopérable pour l'insertion et l'interrogation des données et
des descriptions des capteurs. Il regroupe les observations provenant de capteurs in-situ en direct ainsi que des ensembles de données historiques (données de séries chronologiques).

- *istSOS* : [istSOS](http://www.istsos.org/) est une implémentation de serveur OGC SOS écrite en Python. istSOS permet de gérer et d'envoyer des observations provenant de capteurs de surveillance selon la norme Sensor Observation Service. Le projet fournit également une interface utilisateur graphique qui permet de faciliter les opérations quotidiennes et une api RESTFull Web pour automatiser les procédures d'administration. 

istSOS est un logiciel libre et fonctionne sur toutes les principales plates-formes (Windows, Linux, Mac OS X), même s'il n'a été utilisé en production que dans l'environnement Linux.
:::{admonition}Réference
[Présentation du logiciel *istSOS*](https://nuage.osupytheas.fr/index.php/s/ROh4LCpHZCWdlHz?path=%2FVideos-presentations-2409-apresmidi)   
Massimiliano Canata   
[Séminaire SIST15, Marseille](http://sist15.sciencesconf.org) 
:::

Ces 2 logiciels ont été présentés par Christoph Stasch, et Massimiliano Canata lors du séminaire du réseau [SIST](http://sist.cnrs.fr) en 2015 à l'OSU Pytheas Marseille <https://nuage.osupytheas.fr/index.php/s/ROh4LCpHZCWdlHz>

Stephane Debard présente l'utilisation d'istSOS dans la gestion de mesures altimétriques radars : 
:::{admonition}Réference
[Mise en accord de mesures  altimétriques radars avec le standard de l’OGC - SOS ](https://sist19.sciencesconf.org/data/pages/SIST19_S_Debard.pdf)   
Stéphane Debard IRD   
[Séminaire SIST19 OMP Toulouse](http://sist19.sciencesconf.org)   
:::

### Des chaines de collecte

Regis Hocdé nous présente le Réseau de capteurs mis en place dans le cadre du réseau d'observation ReefTEMPS : 
:::{admonition}Réference
SI-TEC-PSO: retour d'expérience sur le système d'information dédié capteurs et reconstitution de séries temporelles de ReefTEMPS, [le réseau de suivi de température des eaux côtières dans la région du Pacifique Sud et Sud-Ouest  - ](https://nuage.osupytheas.fr/index.php/s/ROh4LCpHZCWdlHz?path=%2F#pdfviewer)   
Sylvie Fiat, Régis Hocdé, Institut de Recherche pour le Développement    
[Séminaire SIST15, Marseille](http://sist15.sciencesconf.org)   
:::

*Retours d'expériences sur les capteurs environnementaux*
:::{admonition}Réference
[Réseau d'observation du Pacifique Sud ‘ReefTEMPS' : évolutions fonctionnelles et optimisation d'un système d'information dédié capteurs et reconstitution de séries temporelles](<https://sist16.sciencesconf.org/data/pages/12_R_Hocde.pdf>)   
Régis Hocdé, Institut de Recherche pour le Développement    
Séminaire SIST16, OSU OREME Montpellier 
:::

Alban Thomas nous présente la technologie utilisée à base de Raspberry et de développement en Python, dans la constitution d'un réseau de stations météorologiques de la région rennaise.
:::{admonition}Réference
[Collecte de mesures météorologiques à l’aide d’un système autonome](https://nuage.osupytheas.fr/index.php/s/ROh4LCpHZCWdlHz?path=%2F#pdfviewer) :  exemple de la métropole rennaise (Zone Atelier Armorique)   
Alban Thomas - Hervé Quénol UMR LETG Rennes    
[Séminaire SIST15, Marseille](http://sist15.sciencesconf.org)   
:::

#### Web scraping, collecte automatique et analyse de données 

"Le Web scraping est une technique permettant de convertir des données présentes dans un format non structuré (balises HTML) sur le Web en un format structuré facilement utilisable. Les exemples peuvent aller du texte sur Wikipedia, à des images sur Flickr en passant par les commentaires sur TripAdvisor, les articles d’actualité ou de chercheurs ou n’importe quelle page web présente sur Internet" [Introduction au Webscraping](https://stateofther.github.io/finistR2018/atelier1_webscraping_intro.html).

Depuis l’explosion quantitative des données numériques, il est devenu extrêmement intéressant d’apprendre à recueillir, comprendre et exploiter les informations issues du web. On constate ces dernières années, dans le domaine des sciences sociales, l'intérêt croissant des chercheurs ou ingénieurs pour l'utilisation de nouvelles techniques de collecte et de traitement automatisé des données et en particulier des big data. Chaque utilisateur en fonction de son profil et de ses compétences peut choisir une technologie partant de simples outils comme les aspirateurs de site qui permettent de réaliser des opérations basiques de scraping jusqu’à l’utilisation de langages plus performants comme R ou Python pour des utilisateurs plus avancés.

Au-delà des fonctionnalités de web scraping, la présentation « [Analyse de données avec R](https://hpecout.gitpages.huma-num.fr/R_presentation_FR/#/) » proposée par Hugues Pécout (CNRS) donne un exemple de l’analyse de donnée avec le logiciel R. En plus d’une présentation du logiciel R et de RStudio, elle contextualise R dans le paysage de l’analyse de donnée en le comparant à des logiciels propriétaires existants sur le marché ainsi qu’au langage python. En python, il faut utiliser le package BeautifulSoup, qui est très populaire [Webscraping avec Python](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx/notebooks/TD2A_Eco_Web_Scraping.html).

Ces outils sont depuis quelques années en plein essor, car ils permettent d’automatiser la constitution des bases de données, de collecter des sommes de données importantes, inaccessibles il y a quelques années comme les données de réseaux sociaux, de compiler des données pour créer ses propres indicateurs (impossible avec des techniques de collecte classiques) ou encore de nettoyer, structurer des données déjà existantes…     
Ces modes de collecte automatisés renvoient aussi aux notions de Crawling, Scrapping, data harvesting …. 

Dans la pratique, des questions juridiques peuvent se poser au regard de l’exploitation des données récoltées en masses par ces moyens qui sont susceptibles d’être des données personnelles ou protégées par la propriété intellectuelle.


### Les cahiers de laboratoire

L’ensemble des données produites par la recherche doit être répertorié et enregistré dans l’objectif d’une réutilisation potentielle. Nous disposons pour ce faire d’un certain nombre de supports comme les cahiers de laboratoire. Le cahier de laboratoire est un outil non obligatoire, mais fortement recommandé pour toute structure générant des données donnant lieu à des connaissances diffusables et valorisables. Il constitue un véritable outil scientifique et ce, dès le commencement d’un projet. Les cahiers de laboratoire répondent également aux obligations légales et contractuelles, en apportant la preuve de l’invention et de ses inventeurs. 

:::{admonition}Réference
[Cahier de laboratoire et gestion des données de la recherche](<http://renatis.cnrs.fr/IMG/pdf/DIALOGIST_9_2020_Rivet.pdf>)   
Alain Rivet, CERMAV  
Atelier Dialog’IST « Rendre FAIR les données, mais quelles données préserver ? »,réeau Renatis, 2020   
:::

Les plaquettes ["Le cahier de laboratoire national : Pourquoi l’utiliser ?" et "Le cahier de laboratoire national : 
Comment l’utiliser ?"](<https://www.curie.asso.fr/-Cahier-de-laboratoire-national-.html>) présentent des recommandations sur la bonne gestion de ce dernier. 

 Les apports du numérique sont multiples en améliorant la traçabilité des recherches, la lutte contre la fraude et la gestion des données. Les cahiers de laboratoire électroniques présentent plusieurs avantages par rapport à leur version papier : 
-	le partage de l’information avec un rattachement des données brutes ; 
-	une recherche d’informations facilitée ;
-	une datation assurée des expériences par l’horodatage.

Le site [datacc.org](https://www.datacc.org) consacre la mise en œuvre d’un service d’accompagnement sur la gestion des données en physique et en chimie, dans le cadre d’un projet CollEx-Persée. Le site fournit des contenus nourris sur les cahiers de laboratoire électroniques (<https://www.datacc.org/bonnes-pratiques/utiliser-un-cahier-de-laboratoire-numerique/>), issus d’une expérimentation menée avec des chimistes de Lyon 1 et de Grenoble.

Diverses expérimentations ont également été réalisées :
:::{admonition}Réference
[Les cahiers de laboratoire électroniques : atelier elabFTW](https://eq2020.sciencesconf.org/data/pages/06_QuaRES_EQ2020_A1_Rivet_Valeins.pdf)  
Alain Rivet, Henri Valeins, CNRS    
Ecole QUARES, Montpellier, 2020
:::

:::{admonition}Réference
[Utilisation du cahier de laboratoire électronique BIOVIA au sein de l'Institut de Biologie Structurale](https://qer-2017.sciencesconf.org/data/program/2017_ANF_tracabilite_laguri.pdf)   
Cédric Laguri, IBS   
ANF "Traçabilité des activités de recherche et gestion des connaissances", Réseau Qualité en Recherche, Grenoble, 16-18 octobre 2017   
:::

L'INSERM s'est également intéressé à la version numérique des cahiers de laboratoires, comme une réplique du cahier papier. 
L'INSERM pense que si la version électronique reste une solution d’enregistrement au quotidien des expériences scientifiques, c’est désormais devenu un outil différent, fortement axé sur la qualité, la gestion de la connaissance, la gestion de projets et le travail collaboratif. Paul-Guy Dupré et ses collaborateurs présentent les cahiers de laboratoires qui ont été mis en place à l'INSERM.
:::{admonition}Réference
[Expérimentation du cahier de laboratoire électronique à l’Inserm](https://qer-2017.sciencesconf.org/data/program/2017_ANF_tracabilite_dupre.pdf)  
Paul-Guy Dupré, INSERM    
ANF "Traçabilité des activités de recherche et gestion des connaissances", Réseau QeR, Grenoble, 2017 
:::
:::{admonition}Réference
[Expérimentation du cahier de laboratoire électronique à l’Inserm : les apports de l’électronique au cahier de laboratoire](https://conf-ng.jres.org/2017/document_revision_2866.html?download)   
Paul-Guy Dupré, Fanny Brizzi Inserm, DSI   
[JRES2017  ](https://2017.jres.org/fr/contenu-th%C3%A9matique)  
:::
:::{admonition}Réference
[Déploiement du cahier de laboratoire  électronique à l’INSERM et nouvelles perspectives]( https://conf-ng.jres.org/2019/document_revision_5213.html?download)   
PaulGuy Dupré Inserm & Claudia Gallina-Muller - Inserm DSI   
[JRES2019  ](https://www.jres.org/fr/programme/)   
:::

### Tablettes et carnets de terrain

Les données et documents produits directement sur le terrain témoignent de l’activité de recherche dans diverses disciplines, notamment en sciences humaines et sociales, en sciences de la terre, etc. Il s’agit aussi bien de carnets issus d’entretiens de sociologues, d’ethnologues, de carnets de prélèvements en géochimie, géologie, de carnets de fouilles en archéologie, de notes, de photographies prises sur le terrain, etc.
 
Par ailleurs, certaines données peuvent se révéler d'une valeur inestimable, qu'il s'agisse de données fortement temporelles (images satellites de la banquise, données sur les glaciers alpins) ou de données provenant de sites aujourd'hui endommagés ou détruits (Notre Dame de Paris, cité antique de Palmyre, etc). Il est de ce fait essentiel que ces données soient répertoriées et archivées. 

L’utilisation de carnets de terrain électroniques que sont les tablettes permet de profiter des avantages d'appareils nomades pour faciliter la saisie des observations que l’on fait sur le terrain, en milieu naturel. L'utilisation de cet outil "nomade" va permettre :

- d'améliorer la qualité des données collectées
- de pouvoir utiliser les données plus rapidement
- de réduire le coût (temps de ressaisie)

Cependant, ces nouvelles technologies très « ludiques » et « faciles » d’utilisation, nécessitent une réflexion importante pour définir de façon précise son besoin afin de ne pas être pénalisé sur le terrain. Elles nécessitent aussi une adaptation technologique pour permettre un stockage efficient et pérenne en bases de données.

Au niveau logiciel, cinq stratégies sont possibles pour développer des carnets de terrain électroniques :

1. utiliser une application nomade existante
2. utiliser une application web existante
3. développer une application nomade spécifique avec un langage de programmation
4. développer une application nomade en utilisant une boite à outils de génération de carnets de terrain
5. développer une application nomade en adaptant des logiciels existants (par exemple QGIS, Lizmap)

Deux solutions ont été étudiées au Centre d'Ecologie Fonctionnelle et Evolutive (CEFE) : le développement d'une application nomade basée sur le système d'information géographique, libre, multiplate-forme, publié sous licence GPL [QGIS](https://www.qgis.org/fr/site/) ainsi qu'une application nomade utilisant une boite à outils de génération de carnets de terrain électronique [Open Data Kit](https://opendatakit.org/). 

Un premier retour d'expérience basé sur QGIS détaille l'étude et le développement d'un ensemble d'outils interopérables avec le système d'information du laboratoire CEFE et qui permet aux intervenants sur le terrain de collecter les données : 
:::{admonition}Réference
[Carnet de terrain électronique, Retour d’expérience sur la création d’une boite à outils](http://video.rmll.info/videos/carnet-de-terrain-electronique/)   
Marie-Claude Quidoz, CEFE  
15èmes Rencontres Mondiales du Logiciel Libre, Montpellier, 2014    
:::

Les deux solutions étudiées au CEFE (application nomade basée sur QGIS et application nomade utilisant ODK) ont été présentées à l'occasion de l'ANF [Interfacer les outils mobiles avec son système d’information](http://rbdd.cnrs.fr/spip.php?article317). C'est d'ailleurs la solution basée sur ODK qui a été utilisée lors de l'ANF car la solution ODK permet de couvrir les étapes allant de la création du formulaire à la sécurisation en bases de données.

Des applicatifs  « clef en main » ont été développés à partir du moteur ODK. Le plus connu est sans doute [KoboToolbox](https://www.kobotoolbox.org/), qui aux fonctionnalités de base a ajouté quelques fonctionnalités supplémentaires, telles que le Formbuilder et la bibliothèque de questions.
:::{admonition}Réference
[Collecte de données terrain avec un smartphone : Prise en main de Kobotolbox et de Kobocollect](https://github.com/OSGeo-fr/FOSS4G-fr-2018/blob/master/ateliers/Atelier-kobo.pdf)   
AKOUETTE, Ata Franck   
FOSS4G-fr, Marne-la-vallée, 2018   
:::

De nombreuses autres solutions sont aussi envisageables, nous invitons le lecteur a consulté les ateliers et séminaires suivants pour en découvrir leurs avantages et inconvénients. 
:::{admonition}Réference
[Atelier « Carnets de terrain électroniques »](http://rbdd.cnrs.fr/spip.php?article270)   
Réseau Zones Ateliers, Montpellier, 2018   
:::   
[Séminaire « Système d’information embarqué, cahier/carnet de terrain et de laboratoire électronique : quelles interactions avec les bases de données ? »](http://rbdd.cnrs.fr/spip.php?article206)   
Réseau rBDD, Paris, 2016   
:::

Il est à noter que la collecte sur le terrain nécessite de s'équiper d'un matériel apte à être utilisé sur des terrains parfois hostiles. Le choix de l'équipement conditionne aussi le choix de la solution logicielle comme le montre Marie-Claude Quidoz lors de cette présentation :
:::{admonition}Réference
[Carnet de terrain électronique](https://indico.mathrice.fr/event/27/contribution/11/material/slides/)   
[video](https://indico.mathrice.fr/event/27/contribution/11/material/0/)   
Marie-Claude Quidoz, CEFE   
Séminaire « les technologies mobiles : retours d'expériences et prospectives », Réseau ResInfo, Paris, 2016   
:::

### La gestion des collections 

[Collec-Science](https://www.collec-science.org) est un logiciel web qui a été créé pour suivre les échantillons collectés lors des campagnes d’acquisition, et permet de répondre, entre autres, à ces questions :
-  où est stocké l’échantillon ?
-  d’où vient-il, quelle est sa généalogie (protocole de collecte, métadonnées associées à l’échantillon et ceux de ces ancêtres) ?
-  quelles transformations ou opérations a-t-il subies ?
-  sous quelle forme est-il conservé, existe-t-il un risque à le manipuler ?

Fruit d’une collaboration initiale entre Irstea (centre de Bordeaux), le laboratoire Epoc à Bordeaux, le LIENSs à La Rochelle, il a été enrichi avec la participation de nombreux autres laboratoires, dont les laboratoires Chrono-environnement à Besançon, Edytem à l’Université Savoie - Mont Blanc, etc. Il a été choisi par le Réseau des Zones Ateliers pour assurer le suivi des échantillons.

:::{admonition}Référence
[Stockez et retrouvez vos échantillons avec Collec-Science](http://rbdd.cnrs.fr/spip.php?article304)   
Marie-Claude Quidoz   
:::

## Environnements de stockage - Sauvegarder les données

Dès la phase de collecte, il convient de se préoccuper des aspects de stockage et de sauvegarde qui seront plus largement abordés dans la phase 6. du cycle de vie des données. En effet, dès le début d'un projet, il est nécessaire, d'une part, d'estimer le stockage nécessaire à la collecte de données et d'autre part, de mettre en place les moyens de sauvegarde des données récoltées. La duplication des données par stockage redondant sur des supports différents de ceux de l'équipement utilisé (poste de travail fixe, mobile, serveur, ...) est un des principes de base d’une bonne conservation. Il convient de préférez un archivage centralisé conformément à la règle du 3-2-1 généralement recommandée (3 copies sur 2 supports différents dont 1 sur un lieu déporté). À cet effet, il conviendra de travailler en amont avec une équipe informatique afin que les organes de stockage soient disponibles.

:::{admonition}Réference
[Rappels théoriques concernant les architectures de stockage traditionnel](https://indico.mathrice.fr/event/5/contribution/2/material/slides/0.pdf)  
Sylvain Maurin  
ANF  2016 : Stockage Distribué   
:::

:::{admonition}Réference
[Outils algorithmiques et logiciels pour le stockage distribué](https://indico.mathrice.fr/event/5/contribution/3/material/slides/0.pdf)  
Benoit Parrein  
ANF Des données au BigData : exploitez le stockage distribué ! Savoir anticiper les nouveaux outils, les technologies émergentes en matière (ANF 2016)   
:::

Divers outils de sauvegarde des données sont fréquemment utilisés dans les milieux informatiques comme [backuppc](https://backuppc.github.io/backuppc/), [bacula](https://bacula.org/), [rdiff-backup](https://rdiff-backup.net/)

Un nouveau paradigme dans la sauvegarde consiste à introduire et utiliser des fonctionnalités de _déduplication_. Cette technologie consistant à réduire les volumes sauvegardés et les durées de sauvegarde en découpant les gros fichiers en fragments (blocs) et en ne sauvegardant qu'une seule fois les fragments identiques.

Un retour d'expérience sur le [logiciel borgbackup](https://borgbackup.readthedocs.io/en/stable/) donne des résultats intéressants et prend tout son sens quand on a beaucoup de gros fichiers peu différents.

:::{admonition}Réference
[Sauvegardes dédupliquées avec BorgBackup : retour d'expérience](https://2017.jres.org/fr/presentation?id=35)  
Maurice Libes - Didier Mallarino OSU Pytheas   
JRES 2017 Nantes   
:::


:::{admonition,warning} Point de vigilance : Respecter le RGPD !
Enfin n'oublions pas que, dès lors que l’on va collecter des données personnelles (données permettant l’identification directe ou indirecte d’une personne), il sera important de respecter des principes essentiels sur la durée de conservation des données, le droit à l’information et l’obligation de sécuriser les données. Il ne faut pas hésiter à se rapprocher du correspondant du Délégué à la protection des données (DPD) de votre délégation (pour le CNRS) ou du Délégué à la protection des données de votre établissement.
:::
