# Collecter

Cette phase du cycle de vie de la donnée, concerne les aspects d'acquisition et de collecte des données ainsi que la constitution des jeux de données ("dataset" en anglais) avec leurs métadonnées descriptives. 
Il s'agit donc, dans cette phase, de travailler sur les processus d'acquisition des données qui peuvent être obtenus au moyen de divers medias : capteurs environnementaux, instruments, sondages, modèles numériques... selon le domaine étudié. 
Une fois les données acquises, il est nécessaire et indispensable dans l'objectif de les rendre "FAIR", de les décrire avec leurs métadonnées associées.  

La description  de ces jeux de données nécessite d'utiliser, autant que faire se peut, des référentiels de vocabulaires contrôlés (thésaurus) si possible standardisés et les plus appropriés au domaine étudié.

Il est conseillé de gérer les jeux de données dans un environnement technique qui permette d’assurer la sauvegarde, l'archivage, le «versionning»,  l’accessibilité et
l'interopérabilité des données. Cette gestion se fait via des infrastructures techniques, des bases ou des supports qui doivent etre fiables et bien documentés, et dans le respect des règles de traitement spécifiques des données personnelles.

Cette phase "Collecter" va nécessiter :

   * de disposer des données et des *metadonnées* précises pour la description des données brutes elles-mêmes (libellés des parametres, unités de mesure), ainsi que des dispositifs d'acquisition (capteurs de mesures, modèles numériques...);
   * de disposer de la *documentation*  des chaines de collecte : du capteur jusqu'aux disques et aux applications sur des serveurs où les traitements pourront être établis;
   * de mettre en place une gestion et conduite de projets pour faire travailler ensemble les différents acteurs intervenant dans la chaîne de collecte : électroniciens, informaticiens, chercheurs...;
   * de disposer de *cahier de laboratoire, tablettes de terrain* ou supports divers pour consigner les relevés et métadonnées observées; 
   * d'utiliser des *protocoles si possibles normalisés* ou standardisés pour présenter les données brutes et les dispositifs d'acquisition  (capteurs...) et les rendre interopérables;
   * de définir le stockage nécessaire à la collecte de données : travailler en amont avec une équipe informatique en mode projet (gestion de projet).


## Utiliser des Normes et Standards d'interopérabilité

## Formats et métadonnées

Dans l'optique d'une gestion FAIR des données, il est nécessaire, autant que faire se peut et lorsqu'elles existent, de suivre des normes et des standards dans la description des métadonnées, les formats de fichiers et les protocoles d'échange de données.

Catherine Morel-Pair propose une présentation très riche et très complète sur les formats et métadonnées qu’elle détaille de manière très approfondie et restitue 
dans le cadre de leur utilisation pour la gestion de contenu et la documentation des données. Elle aborde en introduction les notions de données de la recherche, de 
Fair Data, d’intéropérabilité et de Data Management Plan. 
- La première partie de sa présentation porte sur les fichiers de données (organisation et nommage, format et 
critères d’interopérabilité-pérénité) 
- la deuxième partie est dédiée aux métadonnées et à la documentation  (définitions, présentation des standards, des identifiants 
pérennes pour les données et syntaxes d’échange). Elle termine par un focus sur les sites de de dépôt, de portails ou d’entrepôts de données et leur schéma de métadonnées associés.

* [Participer à l’organisation du management des données de la recherche : gestion de contenu et documentation des données](https://anfdonnees2016.sciencesconf.org/data/pages/2016_07_07_ANF_Renatis_Formats_Standards_et_Metadonnees_1.pdf)
* [vidéo](https://youtu.be/obGDFrXyBiU?t=3)  
Catherine Morel-Pair , INIST, CNRS  
ANF "Participer à l'organisation du management des données de la recherche : gestion de contenu et documentation des données" -  6-8 juil. 2016 Paris (France)

**Les métadonnées dans un DMP**

Cette présentation très riche de Marie Puren a été conçue pour animer un atelier de formation qui avait pour objectif de définir et comprendre l’importance des 
métadonnées dans le cadre de la rédaction d’un DMP. Elle définit en donnant des exemples ce qu’est une métadonnée, à quoi elle sert, quelle information elle donne. 
Elle distingue et détaille la spécificité des métadonnées de description, des métadonnées de gestion et des métadonnées de préservation. Elle aborde ensuite le 
chapitre du cycle de vie des métadonnées (créer, entretenir, mettre à jour, stocker, gérer la suppression des données, publier). Elle spécifie les métadonnées à faire figurer dans un DMP, explique comment les collecter et propose quelques outils d’extraction automatique de métadonnées. Autour de la notion de métadonnée, elle précise l’importance de définir des responsabilités en s’appuyant sur les chercheurs, documentalistes, bibliothécaires et informaticiens. Elle complète sa 
présentation avec une description des principaux standards interdisciplinaires et disciplinaires de métadonnées. Elle explique où et comment choisir ses standards. 
Elle explique également l’intérêt d’associer des ontologies ou vocabulaires contrôlés. Les dernières recommandations de sa présentation portent sur la gestion des 
métadonnées à long-terme, l’importance d’évaluer leur qualité et revient sur la notion d’ouverture des métadonnées et la nécessité de choisir des licences pour nos métadonnées.

* [Les métadonnées dans un DMP](https://anfdonnees2017.sciencesconf.org/data/pages/20170706_dmp_metadonnees_puren_1.pdf)  
Marie Puren,  INRIA  
ANF "Participer à l'organisation du management des données de la recherche : gestion de contenu et documentation des données" -  3-6 juil. 2017 Paris (France) 

### Normes de métadonnées 

Les normes de métadonnées sont des normes qui se chargent de définir les informations nécessaires pour décrire les données elles mêmes. Elles sont de ce fait utilisées pour structurer les ressources informatiques et faciliter leur interopérabilité. Dèslors, il est recommandé de décrire ses données avec des normes ou des standards reconnus dans les disciplines concernées.

On trouve plusieurs standards et Normes qui permettent de définir un ensemble de métadonnées sur des jeux de données. Parmi les plus connus ou utilisés, citons :
- **Le Dublin Core**, standard généraliste issu d'un consensus international et multidisciplinaire a pour objectif de fournir un socle commun d'éléments descriptifs suffisamment structuré pour permettre une interopérabilité minimale entre des systèmes conçus indépendamment les uns des autres. Le Dublin Core <https://fr.wikipedia.org/wiki/Dublin_Core> est un vocabulaire du web sémantique utilisé pour exprimer les données dans un modèle RDFa. 
Le Dublin Core définit un ensemble d'items de métadonnées obligatoires pour décrire les données <https://fr.wikipedia.org/wiki/Dublin_Core#Dublin_Core_element_set> :

1. Titre (métadonnée) 	Title 	Nom donné à la ressource
2. Créateur (métadonnée) 	Creator 	Nom de la personne, de l'organisation ou du service responsable de la création du contenu de la ressource
3. Sujet (métadonnée) ou mots clés 	Subject 	Thème du contenu de la ressource (mots clés, expressions, codes de classification)
4. Description (métadonnée) 	Description 	Présentation du contenu de la ressource (résumé, table des matières, représentation graphique du contenu, texte libre)
5. Éditeur 	Publisher 	Nom de la personne, de l'organisation ou du service responsable de la mise à disposition ou de la diffusion de la ressource
6. Contributeur 	Contributor 	Nom de la personne, de l'organisation ou du service responsable de contributions au contenu de la ressource
7. Date (métadonnée) 	Date 	Date de création ou de mise à disposition de la ressource
8. Type 	Type 	Nature ou genre de la ressource (catégories, fonctions, genres généraux, niveaux d'agrégation du contenu)
9. Format 	Format 	Manifestation physique ou numérique de la ressource
10. Identifiant de la ressource 	Identifier 	Référence univoque à la ressource dans un contexte donné (URI, ISBN)
11. Source 	Source 	Référence à une ressource dont la ressource décrite est dérivée (URI)
12. Langue (métadonnée) 	Language 	Langue du contenu intellectuel de la ressource
13. Relation (métadonnée) 	Relation 	Référence à une ressource apparentée
14. Couverture (métadonnée) 	Coverage 	Couverture spatio-temporelle de la ressource (domaine d'application)
15. Gestion de droits (métadonnée) 	Rights 	Informations sur les droits associés à la ressource (IPR, copyright, etc.)

### La directive Européenne "INSPIRE"

La directive INSPIRE, élaborée par la Direction générale de l’environnement de la Commission européenne, vise à établir en Europe une infrastructure de données géographiques pour assurer l’interopérabilité entre bases de données et faciliter la diffusion, la disponibilité, l’utilisation et la réutilisation de l’information géographique en Europe. INSPIRE vise ainsi à mieux partager les données de la recherche.

Marc Leobet , chargé de mission à la Mission information géographique du ministère en charge du développement durable pose, dans cette présentation réalisée en 2013, le cadre de la Directive Inspire. Il présente tout d’abord l’utilité de cette Directive (identification des données, gestion de la confidentialité, des problèmes de conventionnement  et qualité des données), son contexte, les obligations qu’elle induit, le contexte autour de la réutilisation des données du secteur public et  l’application de la Directive inspire dans le domaine de la recherche. 

* [Gestion et valorisation des données de la recherche](http://renatis.cnrs.fr/IMG/pdf/Leobet_INSPIRE_Fredoc2013.pdf)
Marc Leobet, Chargé de mission et PCE INSPIRE
Frédocs2013 -7 au 10 octobre 2013, Aussois    
 

### Normes de métadonnées en information Géographique 

Pour la description des jeux de données géolocalisés, les normes ISO 19115 et ISO 19139 sont des normes de référence pour l'information géographique dans le domaine des métadonnées. Elles sont notamment utilisées dans les disciplines nécessitant de représenter des données géospatialisées.

La norme ISO 19139 est l'implémentation XML de la norme ISO 19115. Elle définit le codage XML des méta-données géographiques, une implémentation de schéma XML dérivée de la norme ISO 19115. La norme ISO 19139 est le modèle principal utilisé pour décrire des données dans le logiciel GeoNetworke et constituer ainsi un catalogue de données géospatialisées.

**Les données dans le domaine environnemental**

Dans le domaine environnemental, pour des données qui sont souvent *géolocalisées* par des coordonnées latitude/longitude, l'Open Geospatial Consortium (OGC), est un consortium international 
qui a pour objectif de développer et promouvoir des standards ouverts, les spécifications OpenGIS, afin de garantir l'interopérabilité des contenus, des services et des échanges dans les domaines de la géomatique et de l'information géographique. 

Les standards OGC, développés par les membres du consortium sont destinés à rendre les données géospatialisées FAIR "Findable, Accessible, Interoperable and Reusable".  On trouvera les informations sur ces protocoles sur le site de l'OGC <https://www.ogc.org/standards/>
L'intéret de ces standards est qu'ils reposent sur des opérations standards et documentées :

- *GetCapabilities* retourne les méta-données qui décrivent le contenu du service et les paramètres acceptés,
- *GetMap* retourne une image d'une carte dont les paramètres géospatiaux et dimensionnels sont correctement représentés,
- *GetFeatureInfo* retourne des informations sur un objet représenté sur la carte.

Ces standards OGC ont été présentés à plusieurs reprises par François André dans les réseaux DEVLOG, et dans le réseau SIST de l'INSU préoccupés par les besoin d'interopérabilités dans les données des Observatoires de l'INSU.

* [Les Normes OGC (Open Geospatial Consortium)](https://sist15.sciencesconf.org/data/program/ogc.pdf)
François André
Séminaire SIST15 à Marseille OSU Pytheas

* [Infrastructure de Données géographiques et Spatialisées](http://devlog.cnrs.fr/_media/ids2014_presentationogc_francoisandre.pdf?id=ids2014_j2&cache=cache)
ANDRE, François, 2014. Normes OGC. 
In : *ANF2014 DEVLOG-RBDD 

Parmi les standards les plus connus édictés par l'OGC et utilisés dans nos réseaux métiers chez les gestionnaires de données environnementales, on peut citer :

-    **[CS-W](https://www.ogc.org/standards/cat)** - Catalog Service for the Web : Le protocole est destiné à diffuser et explorer des fiches de métadonnées présentes dans un catalogue, et permettre l'interrogation de catalogues de métadonnées. Une très bonne implémentation de ce protocole est réalisée dans le logiciel "Geonetwork" <https://geonetwork-opensource.org/> utilisé pour constituer des catalogues et des inventaires de jeux de données et les présenter sur le Web de manière interopérable. 
Grace à ce protocole on peut constituer des réseaux de catalogues tels qu'ils sont demandés par la [Directive EUropéenne Inspire](http://cnig.gouv.fr/?page_id=8991).

**Les catalogues de métadonnées**

Le logiciel de catalogage "GeoNetwork"  prend en charge les normes ISO19115, ISO19139 utilisées pour les ressources spatiales ainsi que le format standard *"Dublin Core"* pour décrire les jeux de données et les présenter sur un portail de données ouvertes.

 Pour la gestion des jeux de données et pour en permettre leur catalogage, leur affichage et diffusion,  le logiciel "GeoNetwork" 
 <https://geonetwork-opensource.org/> utilise  le standard CSW de l'OGC et permet de décrire des jeux de données avec des fiches utilisant des metadonnées issues des normes ISO 19115, ISO 19139 et du Dublin Core.
 
 Pour exemple De nombreux OSU (Observatoire des Sciences de l'Univers) mettent en oeuvre ces catalogues très utiles pour inventorier les jeux de données disponibles dans les Instituts :
 - <https://sig.oreme.org/geonetwork>
 - <http://portail.indigeo.fr/geonetwork>
 - <https://dataset.osupytheas.fr/geonetwork>


Le recueil des métadonnées ainsi que la rédaction et la mise à jour des métadonnées dans des fiches adaptées peuvent etre ressentis comme contraignants. Il est néanmoins nécessaire de pouvoir automatiser la constitution des catalogues avec des interfaces de programmation (API) disponibles. Plusieurs développements se sont interessés à l'utilisation de l'interface de programmation (API) de Geonetwork pour pouvoir insérer automatiquement des métadonnées dans les fiches avec des programmes écrits en langage "R". 

Olivier Lobry et Juliette Fabre indiquent comment alimenter un catalogue de données GeoNetwork de l'OSU Oreme,
de maniere automatique à partir de données stockées dans une base de données interne à l'unité.
* [Mise en place de catalogues INSPIRE et de leur alimentation automatique.](https://sist16.sciencesconf.org/data/pages/14\_C\_Bernard\_J\_Fabre.pdf)
Cyril Bernard, CEFE - Juliette Fabre, OSU OREME - Olivier Lobry, OSU OREME
Séminaire SIST16 Montpellier <https://sist16.sciencesconf.org/program>

De la même manière Julien Barde, Emmanuel Blondel,  présentent un ensemble de librairies de programmation écrites en "R", destinées
à faciliter l'insertion de métadonnées dans les catalogues "GeoNetwork". Ces développements ont été présentés lors d'un atelier organisépar le réseau RBDD et SIST
- https://sist.cnrs.fr/les-formations/2018-atelier-metadonnees-et-r
  - Écrire et Lire des métadonnées avec la librairie R *geometa*
  - Gérer des données dans GeoServer avec la librairie R *geosapi* 
  - Gérer des métadonnées dans GeoNetwork avec la librairie R *geonapi*

* "GeoFlow" pour gérer les données spatiales
https://sist19.sciencesconf.org/data/pages/SIST19_J_Barde.pdf
séminaire SIST19 Toulouse <https://sist19.sciencesconf.org/program>


-  **[WMS](https://www.ogc.org/standards/wms)** - *Web Map Service* est un protocole de communication standard qui permet d'obtenir des cartes de données géoréférencées à partir de différents serveurs de données. Cela permet de mettre en place un réseau de serveurs cartographiques à partir desquels des clients peuvent construire des cartes interactives.
   <https://fr.wikipedia.org/wiki/Web_Map_Service>. 
   
   Ce protocole est notamment utilisé dans les logiciels "GeoNetWork" et "GeoServer" et GeoCMS pour positionner des données sur une carte et fournir les données relatives aux points de mesure.

Le réseau SIST a organisé des formations spécifiques sur les logiciels "GeoServer", "GeoCMS" mettant en oeuvre le standard WMS
https://sist.cnrs.fr/les-formations/anf-2017-1
https://sist.cnrs.fr/les-formations/anf-2018

Pour lesquelles , les supports de formation sont disponiles en lignes
https://sist.cnrs.fr/les-formations/supports-des-anf-gestion-de-donnees-dobservation


### Les systèmes d'acquisition : capteurs

Diverses communautés scientifiques sont intéressées par les problématiques inhérentes aux systèmes d'acquisitions et aux instruments associés. 

Dans le milieu "Ocean-Atmosphere" cette problématique occupe une place importante, à tel point 
que depuis plusieurs décennies METEO-FRANCE et l'INSU depuis 1966, l'IFREMER depuis 2002, l'IRD et le CNES depuis 2004, le SHOM depuis 2005, 
organise un Atelier dédié aux rencontres portant sur l'expériementation et l'instrumentation. Cet Atelier Expérimentation et Instrumentation (AEI)
[http://www.aei-ocean-atmosphere.org/](http://www.aei-ocean-atmosphere.org/) permet de réunir la communauté scientifique spécialisée dans la recherche instrumentale
et traiter divers thèmes d'actualité <http://www.aei-ocean-atmosphere.org/Editions-Precedentes>

L'AEI traite de manière privilégiée les aspects de mesure et de méthodologie, sans exclure pour autant l'exploitation scientifique des résultats. il a lieu alternativement à Paris, Toulon, Lille et Brest, généralement en début d'année. 

L'AEI permet aux équipes de recherche d'exposer leurs résultats dans un colloque francophone. 
Cet Atelier est un lieu de rencontre pour les participants, issus des différents organismes et groupes industriels, afin de favoriser les synergies et coopérations. 


Les gestionnaires de données environnementales mettent en place des chaînes de collecte de données provenant de capteurs de terrains, ou de modèles numériques. Ils se préoccupent de l’utilisation de normes interopérables dans les protocoles d'échange et dans les formats de données

L'[OGC (Open Geospatial Consortium)](https://www.ogc.org/standards) publie différents standards d’interopérabilité, dont *SWE* "Sensor Web Enablement",  qui permet de présenter des données de capteur de manière standardisée et interopérable
Ces protocoles standards de l'OGC ont été présentés lors du Séminaire du réseau SIST2015 à l'OSU Pytheas de Marseille <https://sist15.sciencesconf.org/program>

* Présentation des différents protocoles interopérables proposés par l'OGC
<https://sist15.sciencesconf.org/data/program/ogc.pdf>
François André OMP Toulouse

Le protocole SOS (Sensor observation service) de l'OGC permet de présenter de manière standardisée les données issues de capteurs de terrain de manière interopérable.. 


Ce standard définit une interface de service Web qui permet d'interroger les observations, les métadonnées des capteurs, ainsi que les représentations des caractéristiques observées. 
En outre, cette norme définit les moyens d'enregistrer de nouveaux capteurs et de supprimer les capteurs existants.
Elle définit également les opérations permettant d'insérer de nouvelles observations de capteurs. 
    

* [Présentation du standard "SWE" Sensor Web Enablement et "SOS" Sensor Observation Service](<https://nuage.osupytheas.fr/s/iMx5S9orQ9zyoxk>)
christoph Stasch co-auteur du logiciel "52North"
Séminaire SIST15 OSU Pytheas Marseille <https://sist15.sciencesconf.org/program> 


Actuellement on trouve 2 implémentations intéressantes du protocole SOS dans la gestion des donnée de capteurs environnementaux. Il sagit de 
- *52North* : logiciel de la société éponyme, est <https://52north.org/software/software-projects/sos/> une application qui fournit une interface web interopérable pour l'insertion et l'interrogation des données et
des descriptions des capteurs.  Il regroupe les observations provenant de capteurs in-situ en direct ainsi que des ensembles de données historiques (données de séries chronologiques).

- *istSOS* : [istSOS](http://www.istsos.org/) est une implémentation de serveur OGC SOS écrite en Python. istSOS permet de gérer et d'envoyer des observations provenant de capteurs de surveillance selon la norme Sensor Observation Service. Le projet fournit également une interface utilisateur graphique qui permet de faciliter les opérations quotidiennes et une api RESTFull Web 
pour automatiser les procédures d'administration. 

istSOS est publié sous licence GPL et fonctionne sur toutes les principales plates-formes (Windows, Linux, Mac OS X), même s'il n'a été utilisé en production que dans l'environnement Linux.

* [Présentation du logiciel *istSOS*](https://nuage.osupytheas.fr/index.php/s/ROh4LCpHZCWdlHz?path=%2FVideos-presentations-2409-apresmidi)
Massimiliano Canata
Séminaire SIST15 OSU Pytheas Marseille <https://sist15.sciencesconf.org/program>


Ces 2 logiciels ont été présentés par leurs auteurs lors du séminaire du réseau [SIST](http://sist.cnrs.fr) en 2015 à Marseille ! <https://nuage.osupytheas.fr/index.php/s/ROh4LCpHZCWdlHz>

Le [réseau SIST](http://sist.cnrs.fr) a  organisé 2 actions de formation nationale (ANF) sur ces logiciels mettant en oeuvre les standards d'interopérabilité WMS, CSW, SOS : <https://sist.cnrs.fr/les-formations> 

Cette formation intitulée *"Gestion des données d'observation : les outils informatiques pour la valorisation"*
permet aux personnels d’améliorer la gestion et la diffusion de leurs données scientifiques d'observation en apprenant à 
installer, configurer et utiliser différents outils choisis pour leur aptitude à répondre de manière standardisée (standards de l'Open Geospatial Consortium (OGC), ISO19139, Inspire, etc) à ces problématiques.

La formation et la documentation associée permettent de :
- savoir installer, configurer et utiliser le logiciel "52°North" pour diffuser et visualiser des données de capteurs selon le standard SOS de l'OGC.
- mettre en place un serveur cartographique "Geoserver" pour afficher et permettre les échanges de données géospatiales sur le web selon les standards (WMS, WFS, ...) de l'OGC ;
- cataloguer les jeux de données avec leurs métadonnées avec le logiciel "GeoNetwork" selon la norme ISO 19139 et la directive Inspire;     
- mettre en place une Infrastructure de Données Géographique (IDG)  avec l'application "geoCMS" permettant la visualisation de données géospatiales sur le web
- mettre en exploitation une plateforme web de gestion et de diffusion des données avec des logiciels comme Thredds et [Erddap](https://coastwatch.pfeg.noaa.gov/erddap/index.html)


[je sais pas ou insérer le projet SeadataNet : ML]
En ce sens, le projet Européen "*Seadatanet*" vise à élaborer et mettre en place un portail Européen d'accès aux données marines. 
Il se base sur de nombreux standards rendants les données FAIR

Seadatanet est un exemple d'envergure Européenne pour la mise en place de standards d'interopérabilité. Il repose sur de nombreux vocabulaires controlés fournis par le BODC 
* Présentation du projet *SeaDataNet, interopérabilité à l'échelle pan-européenne *
<https://sist15.sciencesconf.org/program> (mettre le lien exact vers le fichier)
Michèle Fichaut, Systèmes d'Informations Scientifiques pour la Mer

   * Portail Web d'accès aux données de l'observatoire AMMA-CATCH et mise en oeuvre des standards d'échange des données OGC  
       * <https://sist15.sciencesconf.org/program> (mettre le lien exact vers le fichier)
           * Véronique CHAFFARD, Institut de Recherche pour le Développement, Laboratoire d'étude des transferts en hydrologie et environnement

   *  De la définition au déploiement de standards d'interopérabilité : retour d'expérience de la Direction des Systèmes d'Information (DSI) du BRGM
       * <https://sist15.sciencesconf.org/program> (mettre le lien exact vers le fichier)
           *  Grellet Sylvain, BRGM - Stéphane Loigerot, BRGM        


   *  Le projet Dat@OSU de gestion et valorisation des données de la recherche   
       *  <https://sist16.sciencesconf.org/data/pages/13\_B\_Debray.pdf>                                      
           *  - Bernard Debray, Univers, Transport, Interfaces, Nanostructures, Atmosphère et environnement, Molécules 



## Assurer la traçabilité des données 

[*CH : je pencherais plutôt pour supprimer les deux sous titres et faire un paragraphe unique qui traite de la tracabililité au stade de la collecte*]


### Activités de recherche et gestion des connaissances 
[*CH : Je changerais ce titre car c'est le titre de la présentation *]
[CH *Il me semble ici que la présentation est très généraliste et je ne percois pas bien le lien avec l'activité de collecte - il faudrait peut-être la recontextualiser dans le champs des besoins de traçabilité pour la collecte - En quoi cette traçabilité est particulièrement importante au stade de la collecte*]

[@Alain: à déplacer ailleurs]

Alain Rivet présente la *problématique de la donnée dans la perspective de la traçabilité des activités de recherche* (contexte et enjeux). Il pose la **question du défi organisationnel  de la gestion des données dans les laboratoires et les établissements face aux contraintes de plus en plus fortes des autorités de tutelles**

 – Il souligne le *besoin d’optimiser le fonctionnement de nos laboratoires, la solution étant de s’appuyer sur des référentiels*.
Il introduit la *notion de qualité en recherche* qui consiste tout simplement à répondre à un besoin de recherche. Il insiste sur la **nécessaire confiance en la qualité d’une recherche qui suppose une maitrise de l’ensemble des moyens d’acquisition de traitement, de diffusion et de conservation des résultats**.

La présentation contient une *liste des référentiels intéressant les unités de recherche* et les fascicules existants,  L’ISO 9001, norme généraliste définit le management de la qualité – OCDE et bonnes pratiques de laboratoire etc.

Il aborde le sujet *d'une démarche qualité* au service de la gestion des données dans les laboratoires en précisant qu’il ne suffit pas qu’une donnée soit riche en métadonnées pour que ce soit une donnée de qualité. Il faut avoir une maitrise des différentes étapes qui produisent la donnée pour garantir une donnée de qualité. 

Il ajoute que les *aspects organisationnels et qualitatifs autour de la donnée ont nécessairement un impact sur la qualité de la recherche*. On constate de plus en plus la limite de l’évaluation par les pairs pour garantir la qualité d’une recherche (exemples de fraudes et scandales médiatiques) 

–Il y a toutefois une prise de conscience des tutelles de ces problèmes qui mettent en place une stratégie nationale avec la rédaction début 2016 d’une charte de déontologie qui insiste sur l’importance de permettre la traçabilité des travaux expérimentaux. 
La présentation comprend un **éclairage sur une fiche projet type en chimie qui retrace le processus simplifié d’une recherche et répond aux besoins de traçabilité des activités de recherche**
En conclusion, il signale l’importance de développer une démarche qualité comme outil de gestion des données : la qualité étant un outil, pas un objectif, c’est une norme organisationnelle qui est peu contraignante.

* [Activités de recherche et gestion des connaissances](https://anfdonnees2016.sciencesconf.org/data/pages/donnees_renatis_Al1_Rivet_2016.pdf)  
Alain Rivet, CNRS_CERMAV  
ANF "Participer à l'organisation du management des données de la recherche : gestion de contenu et documentation des données" -  6-8 juil. 2016 Paris (France) 

Voir aussi les vidéo :
[La problématique de la donnée dans la perspective de la traçabilité des activités de recherche : contexte et enjeux]( https://youtu.be/UhBwjJDQcdg) (séquence 1)
[Le défi organisationnel de la gestion des données dans les laboratoires et les établissements](https://youtu.be/6IaYJ_Rvm28) (séquence 2)
[La qualité en recherche : contexte et définition](https://youtu.be/7apGMS9gg5g) (séquence 3)
[La qualité au service de la gestion des données dans les laboratoires]( https://youtu.be/Ld5vEByAMNA) (séquence 4)
[Conclusion : Développer les démarches "qualité" comme outil de gestion de données](https://youtu.be/wVgQ_2fM10s) (séquence 5)

Le réseau Qualité en Recherche a élaboré un guide «* [Traçabilité des activités de recherche et gestion des connaissances](http://qualite-en-recherche.cnrs.fr/IMG/pdf/guide_tracabilite_activites_recherche_gestion_connaissances.pdf)», à destination des unités de recherche.
Il a comme objectif de fournir des recommandations et bonnes pratiques pouvant être appliquées dans tous les domaines d’activités, tant administratifs, techniques que scientifiques, afin d’assurer la traçabilité des activités de recherche et d’améliorer la gestion des données de la recherche.


* [Outils algorithmiques et logiciels pour le stockage distribué](https://indico.mathrice.fr/event/5/contribution/3/material/slides/0.pdf)
Benoit Parrein
Savoir anticiper les nouveaux outils, les technologies émergentes en matière (ANF 2016)
[AR -> que fait cette réf ici ? à déplacer]

### Tracabilité des BD
[@MCQ rediger qqq lignes sur la tracabilité des BD]

* Présentation générale sur la problématique de la traçabilité des données appliquée aux bases de données
Marie-Claude QUIDOZ, 2018
<http://rbdd.cnrs.fr/IMG/pdf/atelier\_tracabilite.pdf?523/29abaadfb5e2e0fff8aed53afd88d7aad1ded34f>
[AR -> Réf ???]

Cette présentation synthétique présente les différentes facettes de la traçabilité d'un jeu de données. 

* E-Maj comme "Enregistrement des Mises A Jour" : Et vos données PostgreSQL voyagent dans le temps ! Un cas d’utilisation pour tracer les données PostgreSQL, Marie-Claude QUIDOZ, Philippe BEAUDOIN, 2018
* <http://rbdd.cnrs.fr/IMG/pdf/emaj.2.3.1\_overview\_fr.pdf?521/c82f6d6408a4f4848d9792a0ab3715a09b5eea5f> et <http://rbdd.cnrs.fr/IMG/pdf/tp\_e-maj.pdf?522/cbfcf7b13ae9a4d8d20ec495c1ef5ea1d09e0a3f>
[AR -> réf inaccessible]
Cet atelier rBDD dont sont fournis ici les liens vers les transparents des présentations, permet de découvrir E-Maj. E-Maj est composé d'un client web « Emaj_web » et de l’extension PostgreSQL « E-Maj » sous licence GPL.

E-Maj sert à :
- Déplacer dans le temps des contenus de données, avec une
granularité de niveau table 
- En enregistrant les mises à jours sur des ensembles de tables applicatives, on peut : 
    - les dénombrer,
    - les consulter,
    - les annuler,
    - les rejouer.

E-Maj est utilisable avec :
    - des applications en test ou en production,
    - des bases de données de toute taille.
  

## Maîtriser l’acquisition et la collecte des données


Différents aspects de collecte de données existent qu'ils proviennent d'un équipement, d'un capteur automatisé, par un modèle numérique ou qu'ils soient obtenus par un personnel de terrain, par une enquête, au moyen d'interfaces... 
Dès lors, il convient d'élaborer des méthodologies de collecte, de conseiller quant au choix des référentiels de métadonnées et des thésaurus de vocabulaire, ainsi que de développer les procédures d'intégration des données dans les bases...


### Web scrapping, collecte automatique et analyse de données 
[*Partie Ajoutée par CH - Revoir le titre en fonction du reste ? En attente de relecture par un collègue que j'ai sollicité*]
[ok ML et AR]

Depuis l’explosion quantitative des données numériques, il est devenu extrêmement intéressant d’apprendre à recueillir, comprendre et exploiter les informations issues du web. On constate ces dernières années, dans le domaine des sciences sociales, le développement de nouvelles techniques de collecte rapide et de traitement automatisé des données et en particulier des big data. 

Pour collecter ces données, il est d’usage de recourir à des technique informatiques : programmation simplifiée ([Web-Harvest](http://web-harvest.sourceforge.net/), [scrapy](https://scrapy.org/))logiciels ou langages de programmation pour la collecte et l’analyse ([R](https://www.r-project.org/), [Python](https://www.python.org/)), logiciels d’analyse statistiques ([SAS](https://www.python.org/)), [SPAD](https://ia-data-analytics.fr/logiciel-data-mining/) , [SPSS](https://www.ibm.com/fr-fr/analytics/spss-statistics-software)), aspirateurs de site ([HTTrack](https://www.httrack.com/), [Web Crawler](https://raptor-dmt.com/seo-tools/web-crawler/?gclid=EAIaIQobChMIlK7j9e3v6gIV1_hRCh3MwwSEEAAYASAAEgKkxPD_BwE)) logiciels de mise en forme de données ([Open Refine](https://openrefine.org/)) etc. 
Pour un tableau non exhaustif de quelques outils utilisés, la présentation « [Data analysis with R](https://hpecout.gitpages.huma-num.fr/R_presentation_EN/#/) » proposée par Hugues Pécout (CNRS) et traduite par Violaine Jurie (Univ. Paris Diderot) donne un exemple de l’analyse de donnée avec le logiciel R. En plus d’une présentation du logiciel R et de RStudio, elle contextualise R dans le paysage de l’analyse de donnée en le comparant à des logiciels propriétaires existants sur le marché ainsi qu’au langage python.

Ces outils sont depuis quelques années en plein essor car ils permettent d’automatiser la constitution des bases de données, de collecter des sommes de données importantes, de compiler des données pour créer ses propres indicateurs (impossible avec des techniques de collecte classiques) ou encore de nettoyer, structurer des données déjà existantes… 

Ces modes de collecte automatisées renvoient aussi aux notions de Crawling, Scrapping, data harvesting ….

Dans la pratique, des questions juridiques peuvent se poser au regard de l’exploitation des données récoltées en masses par ces moyens qui sont susceptibles d’être des données personnelles ou protégées par la propriété intellectuelle.


### Respecter le RGPD 
[CH *Voir aussi la partie Imaginer*]
 
Dès lors que l’on va collecter des données personnelles (données permettant l’identification directe ou indirecte d’une personne), il sera important de respecter des principes essentiels sur la durée de conservation des données,le droit à l’information et l’obligation de sécuriser les données. Il ne faut pas hésiter à se rapprocher du correspondant du Délégué à 
la protection des données (DPD) de votre délégation (pour le CNRS) ou du Délégué à la protection des données de votre établissement.

Dans ce cadre, *Le Règlement Général sur la Protection des Données* (RGPD) est une modernisation fondamentale des lois européennes sur la protection des données qui place l’idée de confidentialité comme droit fondamental des personnes.

On trouvera dans la présentation de D. Guillot la définition des "données personnelles", des données "sensibles"
* [Prise en compte des données personnelles - Évolution de la règlementation](https://qer-2017.sciencesconf.org/data/program/2017_ANF_tracabilite_guillot_1.pdf)
Patrick Guillot, DPO Univ. Grenoble Alpes
ANF "Traçabilité des activités de recherche et gestion des connaissances", Réseau Qualité en Recherche, Grenoble, 16-18 octobre 2017 


### Utiliser des Cahiers de laboratoire

L’ensemble des données produites par la recherche doit ainsi être répertorié et enregistré dans l’objectif d’une réutilisation potentielle. Nous disposons pour ce faire d’un certain nombre de supports comme les cahiers de laboratoire. 
Le cahier de laboratoire est un outil non obligatoire mais fortement recommandé pour toute structure générant des données donnant lieu à des connaissances diffusables et valorisables.
Il constitue un véritable outil scientifique et ce, dès le commencement d’un projet.

[Cahier de laboratoire et gestion des données de la recherche](<http://renatis.cnrs.fr/IMG/pdf/DIALOGIST_9_2020_Rivet.pdf)
Alain Rivet, CNRS-CERMAV
Atelier DAILOGU’IST « Rendre FAIR les données, mais quelles données préserver ? », 9 juillet 2020

Les plaquettes "Le cahier de laboratoire national : Pourquoi l’utiliser ?" et "Le cahier de laboratoire national : 
Comment l’utiliser ?" (<https://www.curie.asso.fr/-Cahier-de-laboratoire-national-.html>) présentent des recommandations sur la bonne gestion de ce dernier. 

Les cahiers de laboratoire électroniques présentent plusieurs avantages qui intéressent les unités de recherche : 
-	le partage de l’information avec un rattachement des données brutes ; 
-	une recherche d’informations facilitée ;
-	une datation assurée des expériences par l’horodatage.

Le site [datacc.org](datacc.org) consacre la mise en œuvre d’un service d’accompagnement sur la gestion des données en physique et en chimie, dans le cadre d’un projet CollEx-Persée. 
Le site fournit également des contenus nourris sur les cahiers de laboratoire électroniques (<https://www.datacc.org/bonnes-pratiques/utiliser-un-cahier-de-laboratoire-numerique/>), issus d’une expérimentation menée avec des chimistes de Lyon 1 et de Grenoble.

Diverses expérimentations ont également été réalisées :

   * [Expérimentation du cahier de laboratoire électronique à l’Inserm] (<https://qer-2017.sciencesconf.org/data/program/2017_ANF_tracabilite_dupre.pdf>  
Paul-Guy Dupré, INSERM    
ANF "Traçabilité des activités de recherche et gestion des connaissances", Réseau Qualité en Recherche, Grenoble, 16-18 octobre 2017 

   * [Utilisation du cahier de laboratoire électronique BIOVIA au sein de l'Institut de Biologie Structurale] (<https://qer-2017.sciencesconf.org/data/program/2017_ANF_tracabilite_laguri.pdf>)
Cédric Laguri, IBS
ANF "Traçabilité des activités de recherche et gestion des connaissances", Réseau Qualité en Recherche, Grenoble, 16-18 octobre 2017 
   

### Tablettes et carnets de terrain

 Les données et documents produits directement sur le terrain témoignent de l’activité de recherche dans diverses disciplines, notamment en sciences humaines et sociales, en sciences de la terre... 
 Il s’agit aussi bien de carnets issus d’entretiens de sociologues, d’ethnologues, de carnets de prélèvements en géochimie, géologie, de carnets de fouilles en archéologie, de notes, de photographies prises sur le terrain… 
 Certains sites peuvent aujourd’hui être des terrains de guerre et seuls les documents produits lors d’une mission restent utilisables pour la recherche actuelle et future. Il est de ce fait essentiel que ces données soient répertoriées et archivées. 
 L’utilisation des tablettes pour consigner les relevés et métadonnées observées permet de profiter des avantages de ces appareils nomades. 


* AKOUETTE, Ata Franck, 2018. Collecte de données terrain avec un smartphone : Prise en main de Kobotolbox et de Kobocollect. *FOSS4G-fr 2018*, Marne-la-vallée. 2018. <https://github.com/OSGeo-fr/FOSS4G-fr-2018/blob/master/ateliers/Atelier-kobo.pdf>

* BORDÈRES, Serge, 2018. Pi 4x4 : [Conception d'une tablette de terrain pour la recherche](http://rbdd.cnrs.fr/IMG/pdf/borderes_atelier2018.pdf?441/72f4dc952e0ac74b725a41d43bc28bb07ecdef38). In : *Atelier "carnets de terrain électroniques"* 
. Montpellier. 2018.


* [Retour d’expérience sur le montage d’une tablette PI4*4](http://rbdd.cnrs.fr/IMG/pdf/pi_4_4.pdf?578/687ae458554248a59a0359cc7f13b28fc2a602d5) 
Mathias Rouan

* [Outils nomades : définir ses besoins. Tour d’horizon des applis embarquées et retour d’expérience ](http://rbdd.cnrs.fr/IMG/pdf/anf_rbbd_2019_outils_mobiles_cours-besoins_2juin2019.pdf?577/a94b104f974483c5dbda6c4939ef0db9422bd1dd)
C. Plumejeaud, S.Ladet, 2019 

* [Carnet de terrain électronique, Retour d’expérience sur la création d’une boite à outils](http://video.rmll.info/videos/carnet-de-terrain-electronique/)
Marie-Claude Quidoz, Centre d’Écologie Fonctionnelle et Évolutive
15èmes Rencontres Mondiales du Logiciel Libre, juillet 2014 


Ce retour d'expérience détaille l'étude et le développement d'un ensemble d'outils interopérables avec le système d'information du laboratoire CEFE et qui permet aux intervenants sur le terrain de collecter les données.

Dans une Action nationale de Formation « Interfacer les outils mobiles avec son système d’information », 03-05 juin 2019, Sète
M-C Quidoz présente comment Déployer une application mobile avec ODK
* [Présentation de la solution ODK](http://rbdd.cnrs.fr/IMG/pdf/deployer_avec_odk.pdf?569/9c7a0e508a744bc2ddd757184ad6427b9505b214) 
MC. Quidoz

*  [Comment faire un formulaire avec XLSForm](http://rbdd.cnrs.fr/IMG/pdf/xlsform_anf2019-2.pdf?575/d45519bbb0360384f14bcf6f4c072313eb7c60a5) ? 
PY. Arnould


*  [Mobile Atlas Creator](http://rbdd.cnrs.fr/IMG/pdf/mobac.pdf?572/f49254091f49630e2b2088ec36964cbd931e0278)
(A. Cheylan)

## Structurer et organiser les données

### La gestion des collections 

  * "Stockez et retrouvez vos échantillons avec Collec-Science, un logiciel web permettant de gérer les échantillons collectés lors des campagnes d’acquisition"
       * <http://rbdd.cnrs.fr/spip.php?article304>
       * contact : Christine Plumejeaud christine.plumejeaud-perreau@univ-lr.fr).  
       * Pour plus d’informations : https://www.collec-science.org

Collec-Science est un logiciel web qui a été créé pour suivre les échantillons collectés lors des campagnes d’acquisition, et permet de répondre, entre autres, à ces questions :
    -   où est stocké l’échantillon ?
    -   d’où vient-il, quelle est sa généalogie (protocole de collecte, métadonnées associées à l’échantillon et ceux de ces ancêtres) ?
    -   quelles transformations ou opérations a-t-il subi ?
    -   sous quelle forme est-il conservé, existe-t-il un risque à le manipuler ?

Fruit d’une collaboration initiale entre Irstea (centre de Bordeaux), le laboratoire Epoc à Bordeaux, le LIENSs à La Rochelle, il a été enrichi avec la participation de nombreux autres laboratoires, dont les laboratoires Chrono-environnement à Besançon, Edytem à l’Université Savoie - Mont Blanc, etc. Il a été choisi par le Réseau des Zones Ateliers pour assurer le suivi des échantillons.



## Développer les procédures d'intégration des données dans les bases de données                       
[ML: manque du texte explicatif et introductif ici]

*  [Intégrer les données dans sa base métier](http://rbdd.cnrs.fr/IMG/pdf/integrer_donnees.pdf?570/2006217c4509e4d59e6cbf44a291f997e7500153) 
MC. Quidoz 


*  [UUID avec PostgreSQL : Pourquoi ? Comment ?](http://rbdd.cnrs.fr/IMG/pdf/uuid_postgres.pdf?405/e6315023727441ee71c5d63415dd28285bc24952) 
N. Raidelet

*  [Les avantages et les inconvénients de la solution ODK](http://rbdd.cnrs.fr/IMG/pdf/bilan_odk.pdf?579/014d2664a47a1b155dde7a4ce7cf84db388194fa) 
MC. Quidoz \& PY. Arnould \& les stagiaires

*  [Référence sur ODK](http://rbdd.cnrs.fr/IMG/pdf/bibliographie.pdf?571/bbecc7bd883e4751efb6ccba6f99517ded5a6305) 
MC. Quidoz

    
*  [Outils nomades : validation des données](http://rbdd.cnrs.fr/IMG/pdf/anf_rbbd_2019_outils_mobiles_tp_qualite.pdf?573/e1425561fd10c6bd1dd92fdee22871bc427f9873) 
C. Plumejeaud

*  [Retour terrain : la délicate question de l’intégration des données](http://rbdd.cnrs.fr/IMG/pdf/anf2019_seshat.pdf?576/575888582b8771a01200c5a6a5e751f0964e0c33)
PY. Arnould



## Environnements de stockage - Sauvegarder les données

Dés la phase de collecte il convient de  se préoccuper des aspects de stockageet suavegardes dont nous reparlerons
plus loin dans la phase 6. Du cyclde de vie des données

prévoir, estimer le stockage nécessaire à la collecte de données : travailler en amont avec une équipe informatique en mode projet (gestion de projet)

* Architectures de stockage traditionnels (ANF 2016)
* <https://indico.mathrice.fr/event/5/contribution/2/material/slides/0.pdf>