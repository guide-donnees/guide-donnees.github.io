# Collecter

Cette phase du cycle de vie de la donnée, concerne les aspects d'acquisition et de collecte des données ainsi que la constitution des jeux de données ("dataset" en anglais) avec leurs métadonnées descriptives. 
Il s'agit donc, dans cette phase, de travailler sur les processus d'acquisition des données qui peuvent être obtenus au moyen de divers medias : capteurs environnementaux, instruments, sondages, modèles numériques... selon le domaine étudié. 

Une fois les données acquises, il est nécessaire et indispensable dans l'objectif de les rendre "FAIR", de les décrire avec leurs métadonnées associées.  

La description  de ces jeux de données nécessite d'utiliser, autant que faire se peut, des référentiels de vocabulaires contrôlés (thésaurus) si possible standardisés et les plus appropriés au domaine étudié.

Il est conseillé de gérer les jeux de données dans un environnement technique qui permette d’assurer la sauvegarde, l'archivage, le «versionning»,  l’accessibilité et
l'interopérabilité des données. Cette gestion se fait via des infrastructures techniques, des bases ou des supports qui doivent etre fiables et bien documentés, et dans le respect des règles de traitement spécifiques des données personnelles.

Cette phase "Collecter" va nécessiter :

   * de disposer des données et des *metadonnées* précises pour la description des dispositifs d'acquisition (capteurs de mesures, modèles numériques...);
   * de disposer de la *documentation*  des chaines de collecte : du capteur jusqu'aux disques et aux applications sur des serveurs où les traitements pourront être établis;
   * de mettre en place une gestion et conduite de projets pour faire travailler ensemble les différents acteurs intervenant dans la chaîne de collecte : électroniciens, informaticiens, chercheurs...;
   * de disposer de *cahier de laboratoire, tablettes de terrain* ou supports divers pour consigner les relevés et métadonnées observées; 
   * d'utiliser des *protocoles si possibles normalisés* ou standardisés pour présenter les données de capteurs et les rendre interopérables;
   * de définir le stockage nécessaire à la collecte de données : travailler en amont avec une équipe informatique en mode projet (gestion de projet).


[CH *Remarques générales : Il me semble que nous devrions ici appuyer pour toutes les parties sur les finalités des actions préconisées par rapport à la collecte (quelles sont les choses à respecter pour assurer une collecte de qualité, pour des données réutilisables) On les perçois pas toujours très bien pour la traçabilité, la structuration, l'organisation (par ex on cite bcp les standards mais sans dire assez à mon goût pourquoi il faut les maîtriser) mais peut-être aussi parce que le travail est en cours*] 

## Utiliser des Normes et Standards d'interopérabilité

## Format et métadonnées

** Formats et métadonnées**

Dans une optique d'une gestion FAIR des données, il est nécessaire, autant que faire se peut, et lorsqu'elles existent, de suivre des normes et des standards dans divers secteurs comme
la description des métadonnées, les formats de fichiers et les protocoles d'échange.

Catherine Morel-Pair propose une présentation très riche et très complète sur les *formats et métadonnées* qu’elle détaille de manière très approfondie et restitue 
dans le cadre de *leur utilisation pour la gestion de contenu et la documentation des données*. Elle aborde en introduction les notions de données de la recherche, de 
Fair Data, d’intéropérabilité et de Data Management Plan. 
- La première partie de sa présentation porte sur les **fichiers de données (organisation et nommage, format et 
critères d’interopérabilité-pérénité)** 
- la deuxième partie est dédiée aux **métadonnées et à la documentation  (définitions, présentation des standards, des identifiants 
pérennes pour les données et syntaxes d’échange)**. Elle termine par un focus sur les sites de de dépôt, de portails ou d’entrepôts de données et leur schéma de métadonnées associés.

* [Participer à l’organisation du management des données de la recherche : gestion de contenu et documentation des données](https://anfdonnees2016.sciencesconf.org/data/pages/2016_07_07_ANF_Renatis_Formats_Standards_et_Metadonnees_1.pdf)
* [vidéo](https://youtu.be/obGDFrXyBiU?t=3)  
Catherine Morel-Pair , INIST, CNRS  
ANF "Participer à l'organisation du management des données de la recherche : gestion de contenu et documentation des données" -  6-8 juil. 2016 Paris (France)

** les métadonnées dans un DMP**

Cette présentation très riche de Marie Puren a été conçue pour animer un atelier de formation qui avait pour objectif de **définir et comprendre l’importance des 
métadonnées dans le cadre de la rédaction d’un DMP**. Elle définit en donnant des exemples ce qu’est une métadonnée, à quoi elle sert, quelle information elle donne. 
Elle distingue et détaille la **spécificité des métadonnées de description, des métadonnées de gestion et des métadonnées de préservation**. Elle aborde ensuite le 
chapitre du **cycle de vie des métadonnées (créer, entretenir, mettre à jour, stocker, gérer la suppression des données, publier)**. Elle spécifie les **métadonnées à 
faire figurer dans un DMP**, explique **comment les collecter** et propose quelques **outils d’extraction automatique de métadonnées**. Autour de la notion de métadonnée, 
elle précise l’importance de définir des responsabilités en s’appuyant sur les chercheurs, documentalistes, bibliothécaires et informaticiens. Elle complète sa 
présentation avec une description des **principaux standards interdisciplinaires et disciplinaires de métadonnées**. Elle explique **où et comment choisir ses standards**. 
Elle explique également l’**intérêt d’associer des ontologies ou vocabulaires contrôlés**. Les dernières recommandations de sa présentation portent sur la **gestion des 
métadonnées à long-terme**, l’importance d’**évaluer leur qualité** et revient sur la **notion d’ouverture des métadonnées** et la nécessité de **choisir des licences** pour nos métadonnées.

* [Les métadonnées dans un DMP](https://anfdonnees2017.sciencesconf.org/data/pages/20170706_dmp_metadonnees_puren_1.pdf)  
Marie Puren,  INRIA  
ANF "Participer à l'organisation du management des données de la recherche : gestion de contenu et documentation des données" -  3-6 juil. 2017 Paris (France) 

### Normes de métadonnées et d'information Géographique 
#### les standards de l'OGC en sciences de l'environnement

Dans le domaine environnemental et pour des données qui sont souvent géolocalisées (nécessité de savoir d'où proviennent les données), l'Open Geospatial Consortium, ou OGC, est un consortium international 
qui a pour objectif de développer et promouvoir des standards ouverts, les spécifications OpenGIS, afin de garantir l'interopérabilité des contenus, des services et des échanges dans les domaines de la géomatique et de l'information géographique. 

Les standards OGC, développés par les membres du consortium sont destinés à rendre les informations géospatialisées FAIR "Findable, Accessible, Interoperable and Reusable". 

Parmi les standards les plus connus édictés par l'OGC et assez utilisés dans nos réseaux métiers chez les gestionnaires de données environnementales, on peut citer :

-  **[WMS](https://www.ogc.org/standards/wms)** - Web Map Service est un protocole de communication standard qui permet d'obtenir des cartes de données géoréférencées à partir de différents serveurs de données. Cela permet de mettre en place un réseau de serveurs cartographiques à partir desquels des clients peuvent construire des cartes interactives.
   <https://fr.wikipedia.org/wiki/Web_Map_Service>. Ce protocole est notamment utilisé dans les logiciels "GeoNetWork" et "GeoServer" pour positionner des données sur une carte et fournir les données relatives aux points de mesure.

-    **[CS-W](https://www.ogc.org/standards/cat)** - Catalog Service for the Web : Le protocole est destiné à diffuser et explorer des fiches de métadonnées présentes dans un
catalogue, et permettre l'interrogation de catalogues de métadonnées. Une très bonne implémentation de ce protocole est réalisée dans le logiciel "Geonetwork"
<https://geonetwork-opensource.org/> utilisé pour constituer des catalogues et des inventaires de jeux de données et les présenter sur le Web de manière interopérable. Grace  ce protocole
on peut constituer des réseaux de catalogues tels qu'ils sont demandés par la[ Directive EUropéenne Inspire ](http://cnig.gouv.fr/?page_id=8991) : directive qui
vise à établir en Europe une infrastructure de données géographiques pour assurer l’interopérabilité entre bases de données et faciliter la diffusion, la disponibilité, l’utilisation et 
la réutilisation de l’information géographique en Europe.

Le logiciel de catalogage "GeoNetwork"  prend en charge les normes ISO19115, ISO 19139 utilisées pour les ressources spatiales ainsi que le format standard *"Dublin Core"* pour décrire les jeux de données et en faire un portail de données ouvertes.


- **SOS** - Sensor Obervation Service: La norme SOS est destinée à diffuser et distribuer les données et de métadonnées de capteurs de manière interopérable.

Ce standard définit une interface de service Web qui permet d'interroger les observations, les métadonnées des capteurs, ainsi que les représentations des caractéristiques observées. 
En outre, cette norme définit les moyens d'enregistrer de nouveaux capteurs et de supprimer les capteurs existants.
Elle définit également les opérations permettant d'insérer de nouvelles observations de capteurs. 
    
Actuellement on trouve 2 implémentations intéressantes du protocole SOS dans la gestion des donnée de capteurs environnementaux. Il sagit de 
- 52North : logiciel de la société éponyme, est <https://52north.org/software/software-projects/sos/> une application qui fournit une interface web interopérable pour l'insertion et l'interrogation des données et
des descriptions des capteurs.  Il regroupe les observations provenant de capteurs in-situ en direct ainsi que des ensembles de données historiques (données de séries chronologiques).

- istSOS : [istSOS](http://www.istsos.org/) est une implémentation de serveur OGC SOS écrite en Python. istSOS permet de gérer et d'envoyer des observations provenant de capteurs de surveillance selon 
la norme Sensor Observation Service. Le projet fournit également une interface utilisateur graphique qui permet de faciliter les opérations quotidiennes et une api RESTFull Web 
pour automatiser les procédures d'administration. 
istSOS est publié sous licence GPL et fonctionne sur toutes les principales plates-formes (Windows, Linux, Mac OS X), même s'il n'a été utilisé en production que dans l'environnement Linux.


Ces 2 logiciels ont été présentés par leurs auteurs lors du séminaire du réseau [SIST](http://sist.cnrs.fr) en 2015 à Marseille ! <https://nuage.osupytheas.fr/index.php/s/ROh4LCpHZCWdlHz>

Le [réseau SIST](http://sist.cnrs.fr) a  organisé 2 actions de formation nationale (ANF) sur ces logiciels mettant en noeuvre les standards d'interopérabilité WMS, CSW, SOS : <https://sist.cnrs.fr/les-formations> 
intitulé *"Gestion des données d'observation : les outils informatiques pour la valorisation"*
Cette formation permet aux personnels d’améliorer la gestion et la diffusion de leurs données scientifiques d'observation en apprenant à 
installer, configurer et utiliser différents outils choisis pour leur aptitude à répondre de manière standardisée (standards de l'Open Geospatial Consortium (OGC), ISO19139, Inspire, etc)
à ces problématiques.

cette formation permet de :

- savoir installer, configurer et utiliser le logiciel "52°North" pour diffuser et visualiser des données de capteurs selon le standard SOS de l'OGC.
- mettre en place un serveur cartographique "Geoserver" pour afficher et permettre les échanges de données géospatiales sur le web selon les standards (WMS, WFS, ...) de l'OGC ;
- cataloguer les jeux de données avec leurs métadonnées avec le logiciel "GeoNetwork" selon la norme ISO 19139 et la directive Inspire;     
- mettre en place une Infrastructure de Données Géographique (IDG)  avec l'application "geoCMS" permettant la visualisation de données géospatiales sur le web
- mettre en exploitation une plateforme web de gestion et de diffusion des données avec des logiciels comme Thredds et [Erddap](https://coastwatch.pfeg.noaa.gov/erddap/index.html)



D'autres standards OGC sont également définis 

-   WMTS - Web Map Tile Service
-   WFS : Web Feature Service 
-   WCS - Web Coverage Service
-   WPS - Web Processing Service
-   KML - Keyhole Markup Language
-   SWE- Sensor Web Enablement
-   GPKG - GeoPackage
-   etc..

On trouvera les informations sur ces protocoles sur le site de l'OGC <https://www.ogc.org/standards/>
L'intéret de ces standards est qu'ils reposent sur des opérations standards et documentées :

- *GetCapabilities* retourne les méta-données qui décrivent le contenu du service et les paramètres acceptés,
- *GetMap* retourne une image d'une carte dont les paramètres géospatiaux et dimensionnels sont correctement représentés,
- *GetFeatureInfo* retourne des informations sur un objet représenté sur la carte.


Ces standards ont été présentés à plusieurs reprises par François André dans les réseaux DEVLOG, et dans le réseau SIST de l'INSU préoccupés par les besoin d'interopérabilités dans les données des Observatoires de l'INSU.

* [Les Normes OGC (Open Geospatial Consortium)](https://sist15.sciencesconf.org/data/program/ogc.pdf)
  François André
  Séminaire SIST15 à Marseille OSU Pytheas

* ANDRE, François, 2014. Normes OGC. In : *ANF2014 DEVLOG-RBDD : Infrastructure de Données géographiques et Spatialisées 
  http://devlog.cnrs.fr/_media/ids2014_presentationogc_francoisandre.pdf?id=ids2014_j2&cache=cache
[*AR -> même référence à revoir] 

### Normes de métadonnées et d'information Géographique 
[*CH - Je ne sais trop quoi penser de ce titre ?*]

Les normes de métadonnées sont des normes qui décrivent les informations sur les données elles mêmes. Elles sont employées pour la structuration des ressources informatiques en général et l'interopérabilité informatique.  [*CH - Ajouter la source :  wikipédia ?*]

- **Les normes ISO 19115 et ISO 19139** sont des normes de référence pour l'information géographique dans le domaine des métadonnées, notamment utilisées dans les disciplines nécessitant de représenter des données géospatialisées.

La norme ISO 19139 est l'implémentation XML de la norme ISO 19115. Elle définit le codage XML des méta-données géographiques, une implémentation de schéma XML dérivée de la norme ISO 19115.
la norme ISO 19139 est le modèle principal utilisé pour décrire des données dans le logiciel GeoNetworke et constituer ainsi un catalogue de données géospatialisées.

- **Le Dublin Core**   [*CH - Attention il me semble qu'ici on pourrait croire que le Dublin Core est spécifique à l'information géographique or il s'agit véritablement d'un standard généraliste*]  
[*Les standards sont présentés plus haut dans la partie format et métadonnées. Du coup parler du Dublin Core ici me semble redondant à moins de décrire spécifiquement l'apport du DC pour la structuration de l'information géographique - Il n'est peut-être pas inutile de faire un focus cela dit sur le DC qui est quand même un standard important mais alors il faut le mettre plus haut au dessus de la présentation de Catherine Morel Pair qui en parle dans son ppt*]

Le Dublin Core <https://fr.wikipedia.org/wiki/Dublin_Core>  est un des modèle utilisé pour décrire des données dans le logiciel GeoNetwork
C'est un vocabulaire du web sémantique utilisé pour exprimer les données dans un modèle RDFa. Issu d'un consensus international et multidisciplinaire

Pour exemple, le Dublin Core définit un ensemble d'item de métadonnées [obligatoires CH ??] pour décrire les données <https://fr.wikipedia.org/wiki/Dublin_Core#Dublin_Core_element_set>  [*CH - un lien suffit peut-être plutôt que le détail des méta - Eventuellement un guide d'application si on en trouve un bon*]

1. Titre (métadonnée) 	Title 	Nom donné à la ressource
2. Créateur (métadonnée) 	Creator 	Nom de la personne, de l'organisation ou du service responsable de la création du contenu de la ressource
3. Sujet (métadonnée) ou mots clés 	Subject 	Thème du contenu de la ressource (mots clés, expressions, codes de classification)
4. Description (métadonnée) 	Description 	Présentation du contenu de la ressource (résumé, table des matières, représentation graphique du contenu, texte libre)
5. Éditeur 	Publisher 	Nom de la personne, de l'organisation ou du service responsable de la mise à disposition ou de la diffusion de la ressource
6. Contributeur 	Contributor 	Nom de la personne, de l'organisation ou du service responsable de contributions au contenu de la ressource
7. Date (métadonnée) 	Date 	Date de création ou de mise à disposition de la ressource
8. Type 	Type 	Nature ou genre de la ressource (catégories, fonctions, genres généraux, niveaux d'agrégation du contenu)
9. Format 	Format 	Manifestation physique ou numérique de la ressource
10. Identifiant de la ressource 	Identifier 	Référence univoque à la ressource dans un contexte donné (URI, ISBN)
11. Source 	Source 	Référence à une ressource dont la ressource décrite est dérivée (URI)
12. Langue (métadonnée) 	Language 	Langue du contenu intellectuel de la ressource
13. Relation (métadonnée) 	Relation 	Référence à une ressource apparentée
14. Couverture (métadonnée) 	Coverage 	Couverture spatio-temporelle de la ressource (domaine d'application)
15. Gestion de droits (métadonnée) 	Rights 	Informations sur les droits associés à la ressource (IPR, copyright, etc.)

[*Puisqu'il est question des normes et standards en information géographique ne faut-il pas ici parler de la d'INSPIRE ?*]


**INSPIRE un cadre pour mieux partager les données de la recherche** [CH-*Pas certaine que cette présentation soit à sa place ici car il parle surtout de la Directive et de ce qu'elle induit et de l'obligation de l'appliquer*]

* Marc Leobet, Chargé de mission et PCE INSPIRE
* Frédocs2013 - Gestion et valorisation des données de la recherche -  7 au 10 octobre 2013, Aussois    
* http://renatis.cnrs.fr/IMG/pdf/Leobet_INSPIRE_Fredoc2013.pdf

Cette présentation réalisée en 2013 par Marc Leobet , chargé de mission à la Mission information géographique du ministère en charge du développement durable pose 
le **cadre de la Directive Inspire**. Il présente tout d’abord l’**utilité de cette Directive** (identification des données, gestion de la confidentialité, des problèmes 
de conventionnement  et qualité des données), son **contexte**, les **obligations qu’elle induit**, le contexte autour de la **réutilisation des données du secteur public et 
l’application de la Directive inspire dans le domaine de la recherche**
 (partage des données et diffusion des métadonnées)
 
L'obtention et la rédaction des métadonnées dans des fiches adaptées peut etre ressentie comme contraignante. Il est nécessaire autant que faire se peut de pouvoir automatiser
la constitution des catalogues avec les API disponibles.

Dans cette présentation Olivier Lobry et Juliette Fabre indiquent comment alimenter un catalogue de données GeoNetwork de l'OSU Oreme,
de maniere automatique à partir de données stockées dans une base de données interne à l'unité.

    * Mise en place de catalogues INSPIRE et de leur alimentation automatique.                                       
       * <https://sist16.sciencesconf.org/data/pages/14\_C\_Bernard\_J\_Fabre.pdf>
           *   - Cyril Bernard, CEFE - Juliette Fabre, OSU OREME - Olivier Lobry, OSU OREME



## Assurer la traçabilité des données 

[*CH : je pencherais plutôt pour supprimer les deux sous titres et faire un paragraphe unique qui traite de la tracabililité au stade de la collecte*]


### Activités de recherche et gestion des connaissances 
[*CH : Je changerais ce titre car c'est le titre de la présentation *]
[CH *Il me semble ici que la présentation est très généraliste et je ne percois pas bien le lien avec l'activité de collecte - il faudrait peut-être la recontextualiser dans le champs des besoins de traçabilité pour la collecte - En quoi cette traçabilité est particulièrement importante au stade de la collecte*]
[@Alain: à déplacer ailleurs]

Alain Rivet présente la *problématique de la donnée dans la perspective de la traçabilité des activités de recherche* (contexte et enjeux). Il pose la **question du défi organisationnel  de la gestion des données dans les laboratoires et les établissements face aux contraintes de plus en plus fortes des autorités de tutelles**

 – Il souligne le *besoin d’optimiser le fonctionnement de nos laboratoires, la solution étant de s’appuyer sur des référentiels*.
Il introduit la *notion de qualité en recherche* qui consiste tout simplement à répondre à un besoin de recherche. Il insiste sur la **nécessaire confiance en la qualité d’une recherche qui suppose une maitrise de l’ensemble des moyens d’acquisition de traitement, de diffusion et de conservation des résultats**.

La présentation contient une *liste des référentiels intéressant les unités de recherche* et les fascicules existants,  L’ISO 9001, norme généraliste définit le management de la qualité – OCDE et bonnes pratiques de laboratoire etc.

Il aborde le sujet *d'une démarche qualité* au service de la gestion des données dans les laboratoires en précisant qu’il ne suffit pas qu’une donnée soit riche en métadonnées pour que ce soit une donnée de qualité. Il faut avoir une maitrise des différentes étapes qui produisent la donnée pour garantir une donnée de qualité. 

Il ajoute que les *aspects organisationnels et qualitatifs autour de la donnée ont nécessairement un impact sur la qualité de la recherche*. On constate de plus en plus la limite de l’évaluation par les pairs pour garantir la qualité d’une recherche (exemples de fraudes et scandales médiatiques) 

–Il y a toutefois une prise de conscience des tutelles de ces problèmes qui mettent en place une stratégie nationale avec la rédaction début 2016 d’une charte de déontologie qui insiste sur l’importance de permettre la traçabilité des travaux expérimentaux. 
La présentation comprend un **éclairage sur une fiche projet type en chimie qui retrace le processus simplifié d’une recherche et répond aux besoins de traçabilité des activités de recherche**
En conclusion, il signale l’importance de développer une démarche qualité comme outil de gestion des données : la qualité étant un outil, pas un objectif, c’est une norme organisationnelle qui est peu contraignante.

* [Activités de recherche et gestion des connaissances](https://anfdonnees2016.sciencesconf.org/data/pages/donnees_renatis_Al1_Rivet_2016.pdf)  
Alain Rivet, CNRS_CERMAV  
ANF "Participer à l'organisation du management des données de la recherche : gestion de contenu et documentation des données" -  6-8 juil. 2016 Paris (France) 

Voir aussi les vidéo :
[La problématique de la donnée dans la perspective de la traçabilité des activités de recherche : contexte et enjeux]( https://youtu.be/UhBwjJDQcdg) (séquence 1)
[Le défi organisationnel de la gestion des données dans les laboratoires et les établissements](https://youtu.be/6IaYJ_Rvm28) (séquence 2)
[La qualité en recherche : contexte et définition](https://youtu.be/7apGMS9gg5g) (séquence 3)
[La qualité au service de la gestion des données dans les laboratoires]( https://youtu.be/Ld5vEByAMNA) (séquence 4)
[Conclusion : Développer les démarches "qualité" comme outil de gestion de données](https://youtu.be/wVgQ_2fM10s) (séquence 5)

Le réseau Qualité en Recherche a élaboré un guide «* [Traçabilité des activités de recherche et gestion des connaissances](http://qualite-en-recherche.cnrs.fr/IMG/pdf/guide_tracabilite_activites_recherche_gestion_connaissances.pdf)», à destination des unités de recherche.
Il a comme objectif de fournir des recommandations et bonnes pratiques pouvant être appliquées dans tous les domaines d’activités, tant administratifs, techniques que scientifiques, afin d’assurer la traçabilité des activités de recherche et d’améliorer la gestion des données de la recherche.


* [Outils algorithmiques et logiciels pour le stockage distribué](https://indico.mathrice.fr/event/5/contribution/3/material/slides/0.pdf)
Benoit Parrein
Savoir anticiper les nouveaux outils, les technologies émergentes en matière (ANF 2016)
[AR -> que fait cette réf ici ? à déplacer]

### Tracabilité des BD
[@MCQ rediger qqq lignes sur la tracabilité des BD]

* Présentation générale sur la problématique de la traçabilité des données appliquée aux bases de données
Marie-Claude QUIDOZ, 2018
<http://rbdd.cnrs.fr/IMG/pdf/atelier\_tracabilite.pdf?523/29abaadfb5e2e0fff8aed53afd88d7aad1ded34f>
[AR -> Réf ???]

Cette présentation synthétique présente les différentes facettes de la traçabilité d'un jeu de données. 

* E-Maj comme "Enregistrement des Mises A Jour" : Et vos données PostgreSQL voyagent dans le temps ! Un cas d’utilisation pour tracer les données PostgreSQL, Marie-Claude QUIDOZ, Philippe BEAUDOIN, 2018
* <http://rbdd.cnrs.fr/IMG/pdf/emaj.2.3.1\_overview\_fr.pdf?521/c82f6d6408a4f4848d9792a0ab3715a09b5eea5f> et <http://rbdd.cnrs.fr/IMG/pdf/tp\_e-maj.pdf?522/cbfcf7b13ae9a4d8d20ec495c1ef5ea1d09e0a3f>
[AR -> réf inaccessible]
Cet atelier rBDD dont sont fournis ici les liens vers les transparents des présentations, permet de découvrir E-Maj. E-Maj est composé d'un client web « Emaj_web » et de l’extension PostgreSQL « E-Maj » sous licence GPL.

E-Maj sert à :
- Déplacer dans le temps des contenus de données, avec une
granularité de niveau table 
- En enregistrant les mises à jours sur des ensembles de tables applicatives, on peut : 
    - les dénombrer,
    - les consulter,
    - les annuler,
    - les rejouer.

E-Maj est utilisable avec :
    - des applications en test ou en production,
    - des bases de données de toute taille.
  

## Maîtriser l’acquisition et la collecte des données


Différents aspects de collecte de données existent qu'ils proviennent d'un équipement, d'un capteur automatisé, par un modèle numérique ou qu'ils soient obtenus par un personnel de terrain, 
par une enquête, au moyen d'interfaces... 
Dès lors, il convient d'élaborer des méthodologies de collecte, de conseiller quant au choix des référentiels de métadonnées (et thésaurus ?)
de développer les procédures d'intégration des données dans les bases...


### Web scrapping, collecte automatique et analyse de données 
[*Partie Ajoutée par CH - Revoir le titre en fonction du reste ? En attente de relecture par un collègue que j'ai sollicité*]

Depuis l’explosion quantitative des données numériques, il est devenu extrêmement intéressant d’apprendre à recueillir, comprendre et exploiter les informations issues du web. On constate ces dernières années, dans le domaine des sciences sociales, le développement de nouvelles techniques de collecte rapide et de traitement automatisé des données et en particulier des big data. 

Pour collecter ces données, il est d’usage de recourir à des technique informatiques : programmation simplifiée ([Web-Harvest](http://web-harvest.sourceforge.net/), [scrapy](https://scrapy.org/))logiciels ou langages de programmation pour la collecte et l’analyse ([R](https://www.r-project.org/), [Python](https://www.python.org/)), logiciels d’analyse statistiques ([SAS](https://www.python.org/)), [SPAD](https://ia-data-analytics.fr/logiciel-data-mining/) , [SPSS](https://www.ibm.com/fr-fr/analytics/spss-statistics-software)), aspirateurs de site ([HTTrack](https://www.httrack.com/), [Web Crawler](https://raptor-dmt.com/seo-tools/web-crawler/?gclid=EAIaIQobChMIlK7j9e3v6gIV1_hRCh3MwwSEEAAYASAAEgKkxPD_BwE)) logiciels de mise en forme de données ([Open Refine](https://openrefine.org/)) etc. 
Pour un tableau non exhaustif de quelques outils utilisés, la présentation « [Data analysis with R](https://hpecout.gitpages.huma-num.fr/R_presentation_EN/#/) » proposée par Hugues Pécout (CNRS) et traduite par Violaine Jurie (Univ. Paris Diderot) donne un exemple de l’analyse de donnée avec le logiciel R. En plus d’une présentation du logiciel R et de RStudio, elle contextualise R dans le paysage de l’analyse de donnée en le comparant à des logiciels propriétaires existants sur le marché ainsi qu’au langage python.

Ces outils sont depuis quelques années en plein essor car ils permettent d’automatiser la constitution des bases de données, de collecter des sommes de données importantes, de compiler des données pour créer ses propres indicateurs (impossible avec des techniques de collecte classiques) ou encore de nettoyer, structurer des données déjà existantes… 

Ces modes de collecte automatisées renvoient aussi aux notions de Crawling, Scrapping, data harvesting ….

Dans la pratique, des questions juridiques peuvent se poser au regard de l’exploitation des données récoltées en masses par ces moyens qui sont susceptibles d’être des données personnelles ou protégées par la propriété intellectuelle.


### Respecter le RGPD 
[CH *Voir aussi la partie Imaginer*]
 
Dès lors que l’on va collecter des données personnelles (données permettant l’identification directe ou indirecte d’une personne), il sera important de respecter des principes essentiels 
sur la durée de conservation des données,le droit à l’information et l’obligation de sécuriser les données. Il ne faut pas hésiter à se rapprocher du correspondant du Délégué à 
la protection des données (DPD) de votre délégation (pour le CNRS) ou du Délégué à la protection des données de votre établissement.

Dans ce cadre, *Le Règlement Général sur la Protection des Données* (RGPD) est une modernisation fondamentale des lois européennes sur la protection des données qui place l’idée de confidentialité
comme droit fondamental des personnes.

On trouvera dans la présentation de D. Guillot la définition des "données personnelles", des données "sensibles"
* [Prise en compte des données personnelles - Évolution de la règlementation](https://qer-2017.sciencesconf.org/data/program/2017_ANF_tracabilite_guillot_1.pdf)
Patrick Guillot, DPO Univ. Grenoble Alpes
ANF "Traçabilité des activités de recherche et gestion des connaissances", Réseau Qualité en Recherche, Grenoble, 16-18 octobre 2017 


### Utiliser des Cahiers de laboratoire

L’ensemble des données produites par la recherche doit ainsi être répertorié et enregistré dans l’objectif d’une réutilisation potentielle. Nous disposons pour ce faire d’un certain nombre de supports comme les cahiers de laboratoire. 
Le cahier de laboratoire est un outil non obligatoire mais fortement recommandé pour toute structure générant des données donnant lieu à des connaissances diffusables et valorisables.
Il constitue un véritable outil scientifique et ce, dès le commencement d’un projet.

[Cahier de laboratoire et gestion des données de la recherche](<http://renatis.cnrs.fr/IMG/pdf/DIALOGIST_9_2020_Rivet.pdf)
Alain Rivet, CNRS-CERMAV
Atelier DAILOGU’IST « Rendre FAIR les données, mais quelles données préserver ? », 9 juillet 2020

Les plaquettes "Le cahier de laboratoire national : Pourquoi l’utiliser ?" et "Le cahier de laboratoire national : 
Comment l’utiliser ?" (<https://www.curie.asso.fr/-Cahier-de-laboratoire-national-.html>) présentent des recommandations sur la bonne gestion de ce dernier. 

Les cahiers de laboratoire électroniques présentent plusieurs avantages qui intéressent les unités de recherche : 
-	le partage de l’information avec un rattachement des données brutes ; 
-	une recherche d’informations facilitée ;
-	une datation assurée des expériences par l’horodatage.

Le site [datacc.org](datacc.org) consacre la mise en œuvre d’un service d’accompagnement sur la gestion des données en physique et en chimie, dans le cadre d’un projet CollEx-Persée. 
Le site fournit également des contenus nourris sur les cahiers de laboratoire électroniques (<https://www.datacc.org/bonnes-pratiques/utiliser-un-cahier-de-laboratoire-numerique/>), issus d’une expérimentation menée avec des chimistes de Lyon 1 et de Grenoble.

Diverses expérimentations ont également été réalisées :

   * [Expérimentation du cahier de laboratoire électronique à l’Inserm] (<https://qer-2017.sciencesconf.org/data/program/2017_ANF_tracabilite_dupre.pdf>  
Paul-Guy Dupré, INSERM    
ANF "Traçabilité des activités de recherche et gestion des connaissances", Réseau Qualité en Recherche, Grenoble, 16-18 octobre 2017 

   * [Utilisation du cahier de laboratoire électronique BIOVIA au sein de l'Institut de Biologie Structurale] (<https://qer-2017.sciencesconf.org/data/program/2017_ANF_tracabilite_laguri.pdf>)
Cédric Laguri, IBS
ANF "Traçabilité des activités de recherche et gestion des connaissances", Réseau Qualité en Recherche, Grenoble, 16-18 octobre 2017 
   

### Tablettes et carnets de terrain

 Les données et documents produits directement sur le terrain témoignent de l’activité de recherche dans diverses disciplines, notamment en sciences humaines et sociales, en sciences de la terre... 
 Il s’agit aussi bien de carnets issus d’entretiens de sociologues, d’ethnologues, de carnets de prélèvements en géochimie, géologie, de carnets de fouilles en archéologie, de notes, de photographies prises sur le terrain… 
 Certains sites peuvent aujourd’hui être des terrains de guerre et seuls les documents produits lors d’une mission restent utilisables pour la recherche actuelle et future. Il est de ce fait essentiel que ces données soient répertoriées et archivées. 
 L’utilisation des tablettes pour consigner les relevés et métadonnées observées permet de profiter des avantages de ces appareils nomades. 

[AR -> pb réf liens incorrects]
   * AKOUETTE, Ata Franck, 2018. Collecte de données terrain avec un smartphone : Prise en main de Kobotolbox et de Kobocollect. *FOSS4G-fr 2018*, Marne-la-vallée. 2018. <https://github.com/OSGeo-fr/FOSS4G-fr-2018/blob/master/ateliers/Atelier-kobo.pdf>

   * BORDÈRES, Serge, 2018. Pi 4x4 : Conception d'une tablette de terrain pour la recherche. In : *Atelier "carnets de terrain électroniques"* [en ligne]. Montpellier. 2018.<http://rbdd.cnrs.fr/IMG/pdf/borderes\_atelier2018.pdf?441/72f4dc952e0ac74b725a41d43bc28bb07ecdef38>

   * Retour d’expérience sur le montage d’une tablette PI4*4 (M. Rouan) (<http://rbdd.cnrs.fr/IMG/pdf/pi\_4\_4.pdf?578/687ae458554248a59a0359cc7f13b28fc2a602d5)>

   * Outils nomades : définir ses besoins. Tour d’horizon des applis embarquées et retour d’expérience (C. Plumejeaud, S.Ladet), 2019 
     * <http://rbdd.cnrs.fr/IMG/pdf/anf_rbbd_2019_outils_mobiles_cours-besoins_2juin2019.pdf?577/a94b104f974483c5dbda6c4939ef0db9422bd1dd>

    * [Carnet de terrain électronique, Retour d’expérience sur la création d’une boite à outils] (http://rbdd.cnrs.fr/IMG/pdf/carnet_terrain-rmll.pdf?134/71c86482f00d5bdfd436925df796213c837058d5)
* [vidéo] (http://video.rmll.info/videos/carnet-de-terrain-electronique/)
Marie-Claude Quidoz, Centre d’Écologie Fonctionnelle et Évolutive
15èmes Rencontres Mondiales du Logiciel Libre, juillet 2014 
        Ce retour d'expérience détaille l'étude et le développement d'un ensemble d'outils interopérables avec le système d'information du laboratoire CEFE et qui permet aux intervenants sur le terrain de collecter les données.

       
       * Présentation de la solution ODK (MC. Quidoz) (<http://rbdd.cnrs.fr/IMG/pdf/deployer\_avec\_odk.pdf?569/9c7a0e508a744bc2ddd757184ad6427b9505b214)>

       *  Comment faire un formulaire avec XLSForm ? (PY. Arnould) (<http://rbdd.cnrs.fr/IMG/pdf/xlsform\_anf2019-2.pdf?575/d45519bbb0360384f14bcf6f4c072313eb7c60a5)>

       *  Mobile Atlas Creator (A. Cheylan) (<http://rbdd.cnrs.fr/IMG/pdf/mobac.pdf?572/f49254091f49630e2b2088ec36964cbd931e0278)>

### Les systèmes d'acquisition : capteurs

Diverses communautés scientifiques sont intéressées par les problématiques inhérentes aux systèmes d'acquisitions et aux instruments associés. 

Dans le milieu "Ocean-Atmosphere" cette problématique occupe une place importante, à tel point 
que depuis plusieurs décennies METEO-FRANCE et l'INSU depuis 1966, l'IFREMER depuis 2002, l'IRD et le CNES depuis 2004, le SHOM depuis 2005, 
organise un Atelier dédié aux rencontres portant sur l'expériementation et l'instrumentation. Cet Atelier Expérimentation et Instrumentation (AEI)
[http://www.aei-ocean-atmosphere.org/](http://www.aei-ocean-atmosphere.org/) permet de réunir la communauté scientifique spécialisée dans la recherche instrumentale
et traiter divers thèmes d'actualité <http://www.aei-ocean-atmosphere.org/Editions-Precedentes>

L'AEI traite de manière privilégiée les aspects de mesure et de méthodologie, sans exclure pour autant l'exploitation scientifique des résultats. il a lieu alternativement à Paris, Toulon, Lille et Brest, généralement en début d'année. 

L'AEI permet aux équipes de recherche d'exposer leurs résultats dans un colloque francophone. 
Cet Atelier est un lieu de rencontre pour les participants, issus des différents organismes et groupes industriels, afin de favoriser les synergies et coopérations. 



## Structurer et organiser les données


### La gestion des collections 


  * "Stockez et retrouvez vos échantillons avec Collec-Science, un logiciel web permettant de gérer les échantillons collectés lors des campagnes d’acquisition"
       * <http://rbdd.cnrs.fr/spip.php?article304>
       * contact : Christine Plumejeaud christine.plumejeaud-perreau@univ-lr.fr).  
       * Pour plus d’informations : https://www.collec-science.org

Collec-Science est un logiciel web qui a été créé pour suivre les échantillons collectés lors des campagnes d’acquisition, et permet de répondre, entre autres, à ces questions :
    -   où est stocké l’échantillon ?
    -   d’où vient-il, quelle est sa généalogie (protocole de collecte, métadonnées associées à l’échantillon et ceux de ces ancêtres) ?
    -   quelles transformations ou opérations a-t-il subi ?
    -   sous quelle forme est-il conservé, existe-t-il un risque à le manipuler ?

Fruit d’une collaboration initiale entre Irstea (centre de Bordeaux), le laboratoire Epoc à Bordeaux, le LIENSs à La Rochelle, il a été enrichi avec la participation de nombreux autres laboratoires, dont les laboratoires Chrono-environnement à Besançon, Edytem à l’Université Savoie - Mont Blanc, etc. Il a été choisi par le Réseau des Zones Ateliers pour assurer le suivi des échantillons.

### Les catalogues de métadonnées

 Pour la gestion des jeux de données et pour  en permettre leur catalogage, leur affichage et diffusion,  le logiciel "GeoNetwork" 
 <https://geonetwork-opensource.org/> qui utilise  le standard CSW de l'OGC (cf plus haut) et permet de décrire des jeux de données avec des fiches utilisants des metadonnées 
 issues de la norme ISO 19115, ISO 19139  et du Dublin Core.
 
 Pour exemple De nombreux OSU (Observatoire des Sciences de l'Univers) mettent en oeuvre ces catalogues bien utiles pour inventorier les jeux de données disponibles dans les Instituts
 - <https://sig.oreme.org/geonetwork>
 - <http://portail.indigeo.fr/geonetwork>
 - <https://dataset.osupytheas.fr/geonetwork>
 



#### les standards de l'OGC en sciences de l'environnement 
[CH- *Attention - deux fois le même titre*]

Les gestionnaires de données environnementales mettent en place des chaînes de collecte de données provenant de capteurs de terrains, ou de modèles numériques. 
Ils se préoccupent de l’utilisation de normes interopérables dans les protocoles d'échange et dans les formats de données

A cet effet l'[OGC (Open Geospatial Consortium)](https://www.ogc.org/standards) publie différents standards d’interopérabilité, dont *SWE* "Sensor Web Enablement", 
qui permet de présenter des données de capteur de manière standardisée et interopérable [à détailler]


Ces protocoles standards de l'OGC ont été présentés lors du Séminaire du réseau SIST2015 à l'OSU Pytheas de Marseille <https://sist15.sciencesconf.org/program>

   * Présentation des différents protocoles interopérables proposés par l'OGC
       * <https://sist15.sciencesconf.org/data/program/ogc.pdf>
           * François André OMP Toulouse

   * Présentation du standard "SWE" Sensor Web Enablement et "SOS" Sensor Observation Service
       * <https://sist15.sciencesconf.org/program> (mettre le lien exact vers le fichier)
           * christoph Stasch co-auteur du logiciel "52North" présente les standards SWE et SOS et leur implémentation dans le logiciel "52North"
           <https://nuage.osupytheas.fr/s/iMx5S9orQ9zyoxk>
           

En ce sens, le projet Européen "*Seadatanet*" vise à élaborer et mettre en place un portail Européen d'accès aux données marines. 
Il se base sur de nombreux standards rendants les données FAIR

Seadatanet est un exemple d'envergure Européenne pour la mise en place de standards d'interopérabilité. Il repose sur de nombreux vocabulaires controlés
   * Présentation du projet *SeaDataNet, interopérabilité à l'échelle pan-européenne *
       * <https://sist15.sciencesconf.org/program> (mettre le lien exact vers le fichier)
           * Michèle Fichaut, Systèmes d'Informations Scientifiques pour la MER                                                                                    


   * Portail Web d'accès aux données de l'observatoire AMMA-CATCH et mise en oeuvre des standards d'échange des données OGC  
       * <https://sist15.sciencesconf.org/program> (mettre le lien exact vers le fichier)
           * Véronique CHAFFARD, Institut de Recherche pour le Développement, Laboratoire d'étude des transferts en hydrologie et environnement



   *  De la définition au déploiement de standards d'interopérabilité : retour d'expérience de la Direction des Systèmes d'Information (DSI) du BRGM
       * <https://sist15.sciencesconf.org/program> (mettre le lien exact vers le fichier)
           *  Grellet Sylvain, BRGM - Stéphane Loigerot, BRGM        


   *  Le projet Dat@OSU de gestion et valorisation des données de la recherche   
       *  <https://sist16.sciencesconf.org/data/pages/13\_B\_Debray.pdf>                                      
           *  - Bernard Debray, Univers, Transport, Interfaces, Nanostructures, Atmosphère et environnement, Molécules                                                                                         


Le protocole SOS (Sensor observation service) de l'OGC permet de présenter de manière standardisée les données issues de capteurs de terrain. 
Certains logiciels comme *52North* et *istSOS* permettent de gérer les donnéees de capteurs dans une BD et de fournir les données de capteurs de manière standardisée via le protocole SOS. 



   * Présentation du logiciel *istSOS*
       * <https://sist15.sciencesconf.org/program> (mettre le lien exact vers le fichier)
       * Massimiliano Canata 

 

## Développer les procédures d'intégration des données dans les bases de données                       
[ML: manque du texte explicatif et introductif ici]

*  Intégrer les données dans sa base métier (MC. Quidoz) (<http://rbdd.cnrs.fr/IMG/pdf/integrer\_donnees.pdf?570/2006217c4509e4d59e6cbf44a291f997e7500153)>
*  UUID avec PostgreSQL : Pourquoi ? Comment ? (N. Raidelet) (<http://rbdd.cnrs.fr/IMG/pdf/uuid\_postgres.pdf?405/e6315023727441ee71c5d63415dd28285bc24952)>
*  Les + et les - de la solution ODK (MC. Quidoz \& PY. Arnould \& les stagiaires) (<http://rbdd.cnrs.fr/IMG/pdf/bilan\_odk.pdf?579/014d2664a47a1b155dde7a4ce7cf84db388194fa)>
*  Référence sur ODK (MC. Quidoz) (<http://rbdd.cnrs.fr/IMG/pdf/bibliographie.pdf?571/bbecc7bd883e4751efb6ccba6f99517ded5a6305)>
    
*  Outils nomades : validation des données (C. Plumejeaud) (<http://rbdd.cnrs.fr/IMG/pdf/anf\_rbbd\_2019\_outils\_mobiles\_tp\_qualite.pdf?573/e1425561fd10c6bd1dd92fdee22871bc427f9873)>
*  Retour terrain : la délicate question de l’intégration des données (PY. Arnould) (<http://rbdd.cnrs.fr/IMG/pdf/anf2019\_seshat.pdf?576/575888582b8771a01200c5a6a5e751f0964e0c33)>



## Environnements de stockage - Sauvegarder les données**
[est ce qu'on met ici de paragraphe ici ou dans la phase dédiée ? ML.. on peut le mettre ici mais il faut savoir quoi développer a ce niveau]

prévoir, estimer le stockage nécessaire à la collecte de données : travailler en amont avec une équipe informatique en mode projet (gestion de projet)
 

   * Architectures de stockage traditionnels (ANF 2016)
   * <https://indico.mathrice.fr/event/5/contribution/2/material/slides/0.pdf>